# ===== EcodiaOS Collation =====
# Generated: 2025-10-11T19:06:57
# Root: D:\EcodiaOS\systems
# Systems: simula
# Extensions: .py
# Ignored dirs: .git, .hg, .idea, .svn, .venv, .vscode, __pycache__, node_modules, venv

# ===== DIRECTORY: D:\EcodiaOS\systems\simula =====

# ===== FILE: D:\EcodiaOS\systems\simula\__init__.py =====

# ===== FILE: D:\EcodiaOS\systems\simula\schema.py =====
# systems/simula/code_sim/specs/schema.py
# --- CONSOLIDATED AND UPGRADED TO PYDANTIC ---
from __future__ import annotations

from typing import Any, List, Optional, Sequence, Dict

from pydantic import BaseModel, ConfigDict, Field

# =========================
# Leaf specs
# =========================

class Constraints(BaseModel):
    model_config = ConfigDict(extra="ignore")
    python: str = ">=3.10"
    allowed_new_packages: List[str] = Field(default_factory=list)

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "Constraints":
        d = d or {}
        return Constraints(
            python=str(d.get("python", ">=3.10")),
            allowed_new_packages=list(d.get("allowed_new_packages") or []),
        )

class UnitTestsSpec(BaseModel):
    model_config = ConfigDict(extra="ignore")
    paths: List[str] = Field(default_factory=list)
    patterns: List[str] = Field(default_factory=list)

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "UnitTestsSpec":
        d = d or {}
        return UnitTestsSpec(
            paths=list(d.get("paths") or []),
            patterns=list(d.get("patterns") or []),
        )

class ContractsSpec(BaseModel):
    model_config = ConfigDict(extra="ignore")
    must_export: List[str] = Field(default_factory=list)
    must_register: List[str] = Field(default_factory=list)

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "ContractsSpec":
        d = d or {}
        return ContractsSpec(
            must_export=list(d.get("must_export") or []),
            must_register=list(d.get("must_register") or []),
        )

class DocsSpec(BaseModel):
    model_config = ConfigDict(extra="ignore")
    files_must_change: List[str] = Field(default_factory=list)

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "DocsSpec":
        d = d or {}
        return DocsSpec(files_must_change=list(d.get("files_must_change") or []))

class PerfSpec(BaseModel):
    model_config = ConfigDict(extra="ignore")
    pytest_duration_seconds: str | float = "<=30"

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "PerfSpec":
        d = d or {}
        return PerfSpec(pytest_duration_seconds=d.get("pytest_duration_seconds", "<=30"))

class AcceptanceSpec(BaseModel):
    model_config = ConfigDict(extra="ignore")
    unit_tests: UnitTestsSpec = Field(default_factory=UnitTestsSpec)
    contracts: ContractsSpec = Field(default_factory=ContractsSpec)
    docs: DocsSpec = Field(default_factory=DocsSpec)
    perf: PerfSpec = Field(default_factory=PerfSpec)

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "AcceptanceSpec":
        d = d or {}
        return AcceptanceSpec(
            unit_tests=UnitTestsSpec.from_dict(d.get("unit_tests")),
            contracts=ContractsSpec.from_dict(d.get("contracts")),
            docs=DocsSpec.from_dict(d.get("docs")),
            perf=PerfSpec.from_dict(d.get("perf")),
        )

class RuntimeSpec(BaseModel):
    model_config = ConfigDict(extra="ignore")
    import_modules: List[str] = Field(default_factory=list)
    commands: List[List[str]] = Field(default_factory=list)

    @staticmethod
    def from_dict(d: dict[str, Any] | None) -> "RuntimeSpec":
        d = d or {}
        return RuntimeSpec(
            import_modules=list(d.get("import_modules") or []),
            commands=[list(x) for x in (d.get("commands") or [])],
        )

# =========================
# Objective / Step
# =========================
# NOTE: The definition for Objective/Step seems to be missing from the provided file.
# Assuming it exists or is not relevant to the current error.

# =========================
# Codegen API Schemas (Moved from api/endpoints/simula/jobs_codegen.py)
# =========================

class SimulaCodegenTarget(BaseModel):
    model_config = ConfigDict(extra="ignore")
    path: str = Field(..., description="Repo-relative path")
    signature: Optional[str] = Field(default=None, description="Optional symbol within the file (e.g., ClassName or func_name)")

class SimulaCodegenIn(BaseModel):
    model_config = ConfigDict(extra="ignore")
    spec: str = Field(..., min_length=10)
    targets: list[SimulaCodegenTarget] = Field(default_factory=list)
    budget_ms: Optional[int] = None
    # +++ FIX: Add the session_id to allow stateful, multi-turn interactions +++
    session_id: Optional[str] = Field(default=None, description="An ID to track a multi-turn session. If not provided, a new one will be generated.")


class SimulaCodegenOut(BaseModel):
    job_id: str
    status: str
    message: Optional[str] = None

# Aliases for cross-system compatibility
CodegenRequest = SimulaCodegenIn
CodegenResponse = SimulaCodegenOut
# ===== FILE: D:\EcodiaOS\systems\simula\agent\__init__.py =====
# systems/simula/agent/__init__.py
# This file makes this directory a Python package, allowing imports.
from __future__ import annotations
# ===== FILE: D:\EcodiaOS\systems\simula\agent\autoheal.py =====
# systems/simula/agent/autoheal.py
# --- PROJECT SENTINEL UPGRADE ---
from __future__ import annotations

from typing import Any

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config


async def _git_diff(sess) -> str:
    """Captures the git diff from within a sandbox session."""
    out = await sess._run_tool(["git", "diff", "--unified=2", "--no-color"])
    return out.get("stdout") or ""


async def auto_heal_after_static(changed_paths: list[str]) -> dict[str, Any]:
    """
    A best-effort, sandboxed auto-healing and diagnostics tool.
    It runs formatters/fixers and returns a proposed diff, along with mypy diagnostics.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        # Run fixers. These tools modify files in place inside the sandbox.
        await sess._run_tool([sess.python_exe, "-m", "ruff", "check", *changed_paths, "--fix"])
        await sess._run_tool([sess.python_exe, "-m", "black", *changed_paths])

        # Capture the changes made by the fixers as a diff.
        diff = await _git_diff(sess)

        # Run mypy to get type-checking diagnostics to inform the LLM.
        # This does not block or change the diff.
        mypy_result = await sess.run_mypy(changed_paths)

    if diff.strip():
        return {"status": "proposed", "diff": diff, "diagnostics": {"mypy": mypy_result}}

    return {"status": "noop", "diagnostics": {"mypy": mypy_result}}

# ===== FILE: D:\EcodiaOS\systems\simula\agent\context_formatters.py =====
# systems/simula/agent/context_formatters.py

from __future__ import annotations
from typing import Dict, Any, List

def format_dossier_for_llm(dossier: Dict[str, Any]) -> str:
    """
    Transforms the rich JSON output from the Qora dossier service into a clean,
    hierarchical, and LLM-friendly markdown format. This provides deep,
    synthesized context for the Deliberative Core.
    """
    if not dossier:
        return "No dossier available for the target."

    chunks = []

    # --- Target ---
    target = dossier.get("target", {})
    if target.get("fqn"):
        chunks.append(f"# Dossier for `{target['fqn']}`")
        if target.get("path"):
            chunks.append(f"**Location:** `{target['path']}`")
        
        if docstring := target.get("docstring"):
            chunks.append(f"## Docstring\n```\n{docstring}\n```")
            
        if source := target.get("source_code"):
            chunks.append(f"## Source Code\n```python\n{source}\n```")

    analysis = dossier.get("analysis", {})
    
    # --- Structural Neighbors ---
    struct = analysis.get("structural_neighbors", {})
    if struct:
        chunks.append("## Structural Analysis")
        def format_neighbor_list(title: str, neighbors: List[Dict]) -> str:
            if not neighbors: return ""
            md = f"### {title}\n"
            for n in neighbors[:5]: # Limit for brevity
                md += f"- `{n.get('fqn', n.get('name', 'N/A'))}`\n"
            if len(neighbors) > 5:
                md += f"- ...and {len(neighbors) - 5} more.\n"
            return md

        chunks.append(format_neighbor_list("Callers (Code that calls this)", struct.get("callers", [])))
        chunks.append(format_neighbor_list("Callees (Code that this calls)", struct.get("callees", [])))
        chunks.append(format_neighbor_list("Siblings (Defined in the same file)", struct.get("siblings", [])))

    # --- Semantic Neighbors ---
    semantic = analysis.get("semantic_neighbors", [])
    if semantic:
        chunks.append("## Semantically Similar Code")
        chunks.append("The following code snippets from the repository have a similar purpose or meaning:")
        for n in semantic[:3]: # Limit for brevity
            chunks.append(f"- **`{n.get('fqn', n.get('name', 'N/A'))}`** (Similarity: {n.get('score', 0.0):.2f})\n  - {n.get('docstring', 'No docstring.')}")

    # --- Test Coverage ---
    tests = analysis.get("related_tests", [])
    if tests:
        chunks.append("## Test Coverage")
        chunks.append("This code appears to be tested by the following files:")
        for t in tests:
            chunks.append(f"- `{t.get('test_path')}`")
            
    # --- Historical Context ---
    history = analysis.get("historical_context", {})
    conflicts = history.get("conflicts_and_solutions", [])
    if conflicts:
        chunks.append("## Historical Conflicts")
        chunks.append("This code has been associated with the following past problems:")
        for c in conflicts[:2]: # Limit for brevity
            chunks.append(f"- **Conflict:** {c.get('description')}")

    return "\n\n".join(filter(None, chunks))
# ===== FILE: D:\EcodiaOS\systems\simula\agent\deliberation.py =====
# systems/synapse/deliberation.py

from __future__ import annotations

import asyncio
import logging
from typing import Any, Dict

from pydantic import BaseModel, Field

from core.prompting.orchestrator import build_prompt
from core.utils.llm_gateway_client import call_llm_service, extract_json_flex
from systems.synapse.schemas import ArmScore

from .scl_context import WorkingContext
from .scl_utils import (
    _sha16_for,
    _coerce_plan_steps,
    register_dynamic_arm_in_graph,
)

# NEW
from .runlog import RunLogger

log = logging.getLogger(__name__)


class DeliberationResult(BaseModel):
    """Standardized output for the deliberation process."""
    status: str = "rejected"  # "approved", "rejected"
    reason: str = "No deliberation occurred."
    initial_plan: Dict[str, Any] = Field(default_factory=dict)
    critique: Dict[str, Any] = Field(default_factory=dict)
    final_plan: Dict[str, Any] = Field(default_factory=dict)


class DeliberationRoom:
    """Orchestrates a multi-agent deliberation process to create a robust plan for Simula."""

    async def deliberate(self, working_context: WorkingContext, runlog: RunLogger | None) -> DeliberationResult:
        """Runs the full deliberation cycle: Plan -> Critique -> Judge."""
        # ---------------- PLAN ----------------
        try:
            initial_plan_obj = await self._invoke_planner(working_context, runlog=runlog)
            if not isinstance(initial_plan_obj, dict) or not initial_plan_obj.get("plan"):
                return DeliberationResult(
                    status="rejected",
                    reason="Planner failed to generate a valid initial plan structure."
                )
        except Exception as e:
            log.error("[Deliberation] Planner agent failed: %r", e, exc_info=True)
            return DeliberationResult(status="rejected", reason=f"Planner agent crashed: {e}")

        # --------------- CRITIQUE --------------
        try:
            critique_obj = await self._invoke_red_team(working_context, initial_plan_obj, runlog=runlog)
            if not isinstance(critique_obj, dict):
                critique_obj = {"summary": "Red-Team returned invalid format.", "findings": []}
        except Exception as e:
            log.error("[Deliberation] Red-Team agent failed: %r", e, exc_info=True)
            critique_obj = {"summary": f"Red-Team agent crashed: {e}", "findings": []}

        # ---------------- JUDGE ----------------
        try:
            judgement_obj = await self._invoke_judge(
                working_context, initial_plan_obj, critique_obj, runlog=runlog
            )
            if not isinstance(judgement_obj, dict):
                return DeliberationResult(
                    status="rejected",
                    reason="Judge agent returned an invalid format.",
                    initial_plan=initial_plan_obj,
                    critique=critique_obj,
                )

            decision = (judgement_obj.get("decision") or "reject").lower()
            reason = judgement_obj.get("reasoning") or "No reasoning provided."

            if decision == "approve":
                final_plan = self._normalize_and_finalize_plan(
                    initial_plan_obj, working_context.strategy_arm, runlog=runlog
                )
                return DeliberationResult(
                    status="approved", reason=reason,
                    initial_plan=initial_plan_obj, critique=critique_obj, final_plan=final_plan
                )

           
            elif decision == "revise":
                revised_plan_obj = judgement_obj.get("revised_plan", {})
                
                # Check if the revised plan is valid (either a dict with a 'plan' key or a non-empty list)
                is_valid_plan = (isinstance(revised_plan_obj, dict) and revised_plan_obj.get("plan")) or \
                                (isinstance(revised_plan_obj, list) and len(revised_plan_obj) > 0)

                if not is_valid_plan:
                    return DeliberationResult(
                        status="rejected",
                        reason="Judge's revised plan was invalid or empty.",
                        initial_plan=initial_plan_obj, critique=critique_obj
                    )
                
                # Now, pass the valid revised plan object for normalization
                final_plan = self._normalize_and_finalize_plan(
                    revised_plan_obj, working_context.strategy_arm, runlog=runlog
                )
                
                return DeliberationResult(
                    status="approved", reason=reason,
                    initial_plan=initial_plan_obj, critique=critique_obj, final_plan=final_plan
                )

            # default: REJECT
            return DeliberationResult(
                status="rejected", reason=reason,
                initial_plan=initial_plan_obj, critique=critique_obj
            )

        except Exception as e:
            log.error("[Deliberation] Judge agent failed: %r", e, exc_info=True)
            return DeliberationResult(
                status="rejected", reason=f"Judge agent crashed: {e}",
                initial_plan=initial_plan_obj, critique=critique_obj
            )

    # ----------------------------- helpers -----------------------------

    def _prepare_context(self, context: WorkingContext) -> Dict[str, Any]:
        """
        Flatten + sanitize for Jinja. Ensure 'strategy_arm' is ALWAYS present
        as a dict so templates can do {{ strategy_arm.arm_id }} safely.
        """
        out: Dict[str, Any] = {
            "goal": context.goal,
            "target_fqname": context.target_fqname,
            "history_summary": context.history_summary,
            "blackboard_insights": dict(getattr(context, "blackboard_insights", {}) or {}),
            "file_cards": [fc.model_dump(mode="json") for fc in (context.file_cards or [])],
            "tool_hints": [th.model_dump(mode="json") for th in (context.tool_hints or [])],
            "extras": getattr(context, "extras", {}) or {},
            "context_vars": getattr(context, "context_vars", {}) or {},
        }
        # Harden strategy_arm
        sa: Dict[str, Any] = {"arm_id": "unknown", "reason": "not provided", "score": 0.0}
        try:
            if isinstance(context.strategy_arm, ArmScore):
                sa = context.strategy_arm.model_dump(mode="json")
            elif isinstance(context.strategy_arm, dict):
                sa = {"arm_id": context.strategy_arm.get("arm_id", "unknown"), **context.strategy_arm}
            elif isinstance(context.strategy_arm, str):
                sa = {"arm_id": context.strategy_arm}
        except Exception:
            pass
        out["strategy_arm"] = sa
        out["strategy_arm_id"] = sa.get("arm_id", "unknown")
        return out

    async def _invoke_planner(self, context: WorkingContext, runlog: RunLogger | None) -> Dict[str, Any]:
        prompt_context = self._prepare_context(context)
        prompt = await build_prompt(
            scope="simula.deliberation.planner",
            context=prompt_context,
            summary="Generate a plan based on the given strategy."
        )
        resp = await call_llm_service(
            prompt,
            agent_name="Simula.Planner",
            scope="simula.deliberation.planner",
            timeout=45.0,
        )
        text = getattr(resp, "text", "")
        plan = extract_json_flex(text) or {}
        if not isinstance(plan, dict):
            plan = {}

        # NEW: log llm
        if runlog:
            # preview tries to show the last message content if available
            preview = None
            try:
                # RenderedPrompt may have messages or text; we defensively handle both
                preview = getattr(prompt, "text", None)
                if not preview and hasattr(prompt, "messages"):
                    # concatenate last message content as preview
                    msgs = getattr(prompt, "messages", [])
                    preview = msgs[-1]["content"] if msgs else None
            except Exception:
                pass

            runlog.log_llm(
                phase="planner",
                scope="simula.deliberation.planner",
                agent="Simula.Planner",
                prompt_preview=preview,
                prompt_struct=getattr(prompt, "model_dump", lambda **_: None)() if hasattr(prompt, "model_dump") else None,
                completion_text=text,
                extra={"plan_keys": list(plan.keys()) if isinstance(plan, dict) else []},
            )
        return plan

    async def _invoke_red_team(self, context: WorkingContext, plan_to_critique: Dict[str, Any], runlog: RunLogger | None) -> Dict[str, Any]:
        prompt_context = self._prepare_context(context)
        prompt_context["plan_to_critique"] = plan_to_critique
        prompt = await build_prompt(
            scope="simula.deliberation.red_team",
            context=prompt_context,
            summary="Find all flaws in the proposed plan."
        )
        resp = await call_llm_service(
            prompt,
            agent_name="Simula.RedTeam",
            scope="simula.deliberation.red_team",
            timeout=45.0,
        )
        text = getattr(resp, "text", "")
        critique = extract_json_flex(text) or {}
        if not isinstance(critique, dict):
            critique = {}

        # NEW: log llm
        if runlog:
            preview = None
            try:
                preview = getattr(prompt, "text", None)
                if not preview and hasattr(prompt, "messages"):
                    msgs = getattr(prompt, "messages", [])
                    preview = msgs[-1]["content"] if msgs else None
            except Exception:
                pass

            runlog.log_llm(
                phase="red_team",
                scope="simula.deliberation.red_team",
                agent="Simula.RedTeam",
                prompt_preview=preview,
                prompt_struct=getattr(prompt, "model_dump", lambda **_: None)() if hasattr(prompt, "model_dump") else None,
                completion_text=text,
                extra={"findings_len": len(critique.get("findings", [])) if isinstance(critique, dict) else 0},
            )
        return critique

    async def _invoke_judge(
        self,
        context: WorkingContext,
        plan: Dict[str, Any],
        critique: Dict[str, Any],
        runlog: RunLogger | None,
    ) -> Dict[str, Any]:
        prompt_context = self._prepare_context(context)
        prompt_context["initial_plan"] = plan
        prompt_context["critique"] = critique
        prompt = await build_prompt(
            scope="simula.deliberation.judge",
            context=prompt_context,
            summary="Decide whether to approve, revise, or reject the plan."
        )
        resp = await call_llm_service(
            prompt,
            agent_name="Simula.Judge",
            scope="simula.deliberation.judge",
            timeout=90.0,  # judge often needs more time
        )
        text = getattr(resp, "text", "")
        judgement = extract_json_flex(text) or {}
        if not isinstance(judgement, dict):
            judgement = {}

        # NEW: log llm
        if runlog:
            preview = None
            try:
                preview = getattr(prompt, "text", None)
                if not preview and hasattr(prompt, "messages"):
                    msgs = getattr(prompt, "messages", [])
                    preview = msgs[-1]["content"] if msgs else None
            except Exception:
                pass

            runlog.log_llm(
                phase="judge",
                scope="simula.deliberation.judge",
                agent="Simula.Judge",
                prompt_preview=preview,
                prompt_struct=getattr(prompt, "model_dump", lambda **_: None)() if hasattr(prompt, "model_dump") else None,
                completion_text=text,
                extra={"decision": judgement.get("decision")},
            )
        return judgement

    def _normalize_and_finalize_plan(self, plan_obj: Dict[str, Any], champion_arm: ArmScore, runlog: RunLogger | None) -> Dict[str, Any]:
        """
        Ensures the final plan has a canonical structure.

        IMPORTANT:
        - Do NOT overwrite the base/champion arm with the dynamic plan handle.
        - Provide both IDs explicitly (base + dynamic).
        """
        plan_steps = plan_obj if isinstance(plan_obj, list) else plan_obj.get("plan", [])
        
        normalized = {
            "interim_thought": (plan_obj.get("interim_thought", "No thought provided.") if isinstance(plan_obj, dict) else "No thought provided.").strip(),
            "scratchpad": (plan_obj.get("scratchpad", "No scratchpad provided.") if isinstance(plan_obj, dict) else "No scratchpad provided.").strip(),
            "plan": _coerce_plan_steps(plan_steps),
            "final_synthesis_prompt": (plan_obj.get("final_synthesis_prompt", "Summarize the results.") if isinstance(plan_obj, dict) else "Summarize the results.").strip(),
        }

        # Preserve the selected champion arm (base policy arm actually chosen)
        try:
            base_arm_id = champion_arm.arm_id
        except Exception:
            base_arm_id = "unknown"

        # Deterministic dynamic plan ID derived from (base arm + normalized plan)
        hash_seed = {
            "base_arm": base_arm_id,
            "plan": {
                "interim_thought": normalized["interim_thought"],
                "scratchpad": normalized["scratchpad"],
                "plan": normalized["plan"],
                "final_synthesis_prompt": normalized["final_synthesis_prompt"],
            },
        }
        dyn_hash = _sha16_for(hash_seed)
        dyn_arm_id = f"dyn::{dyn_hash}"

        # Provide both fields explicitly
        normalized["champion_arm_id"] = base_arm_id          # base arm (kept intact)
        normalized["dynamic_plan_arm_id"] = dyn_arm_id      # synthetic plan-handle

        log.info(
            "[Deliberation] Finalized plan with base_arm=%s dynamic_arm=%s steps=%d",
            base_arm_id, dyn_arm_id, len(normalized["plan"])
        )

        # Register in graph asynchronously (non-blocking).
        asyncio.create_task(register_dynamic_arm_in_graph(dyn_arm_id, "simula_planful"))

        # NEW: log plan structure for summary
        if runlog:
            runlog.log_llm(
                phase="plan_finalize",
                scope="simula.deliberation.finalize",
                agent="Simula.DeliberationRoom",
                prompt_preview="(normalized plan summary)",
                prompt_struct=None,
                completion_text=f"base_arm={base_arm_id}, dynamic_arm={dyn_arm_id}, steps={len(normalized['plan'])}",
                extra={"step_kinds": [s.get('action_type') for s in normalized['plan'] if isinstance(s, dict)]},
            )

        return normalized
# ===== FILE: D:\EcodiaOS\systems\simula\agent\dispatcher.py =====
# systems/simula/agent/dispatcher.py
# --- UNIFIED TOOL DISPATCHER FOR SIMULA ---

from __future__ import annotations
import logging
from typing import Any, Dict

from systems.simula.nscs.agent_tools import get_tracked_tools

log = logging.getLogger(__name__)

# --- Load all tools (via @track_tool in agent_tools) ---
TOOL_MAP: dict[str, Any] = get_tracked_tools()
_SIMULA_PREFIX = "simula.agent."

def _resolve_tool_from_arm_id(arm_id: str) -> str | None:
    """
    Accepts arm IDs like 'simula.agent.run_tests.v1' and returns the tool key ('run_tests').
    """
    # Direct match
    if arm_id in TOOL_MAP:
        return arm_id
    # Remove prefix and extract base name
    if arm_id.startswith(_SIMULA_PREFIX):
        tail = arm_id[len(_SIMULA_PREFIX):]
        return tail.split(".", 1)[0]
    return None

async def dispatch_tool(arm_id: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Dispatches a tool by resolving the arm ID and calling the registered function.
    """
    tool_key = _resolve_tool_from_arm_id(arm_id)

    if not tool_key or tool_key not in TOOL_MAP:
        log.error("Unknown tool for arm_id '%s'. Registry has: %s", arm_id, list(TOOL_MAP))
        return {"status": "error", "reason": f"Tool '{tool_key}' not found for arm '{arm_id}'."}

    tool_fn = TOOL_MAP[tool_key]
    try:
        return await tool_fn(**(params or {}))
    except Exception as e:
        log.exception("Tool '%s' crashed during execution", tool_key)
        return {"status": "error", "reason": f"Tool '{tool_key}' crashed: {e!r}"}

# ===== FILE: D:\EcodiaOS\systems\simula\agent\events.py =====
# Minimal shared topics/constants for Simula’s step loop.

from __future__ import annotations


def llm_tool_response_topic(request_id: str) -> str:
    return f"llm_tool_response:{request_id}"

# ===== FILE: D:\EcodiaOS\systems\simula\agent\runlog.py =====
from __future__ import annotations

import json
import os
import time
import hashlib
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional


def _ts() -> str:
    return datetime.utcnow().isoformat(timespec="milliseconds") + "Z"


def _sha16(obj: Any) -> str:
    data = json.dumps(obj, sort_keys=True, ensure_ascii=False).encode("utf-8")
    return hashlib.sha1(data).hexdigest()[:16]


SENSITIVE_KEYS = {
    "api_key", "apikey", "x-api-key", "authorization", "auth", "password", "token", "bearer", "secret"
}


def _redact(obj: Any) -> Any:
    """
    Shallow+recursive redaction of known-sensitive keys.
    Keeps shapes intact for debug, nukes obvious secrets.
    """
    try:
        if isinstance(obj, dict):
            out = {}
            for k, v in obj.items():
                lk = k.lower()
                if lk in SENSITIVE_KEYS or lk.endswith("_key") or lk.endswith("_token"):
                    out[k] = "REDACTED"
                else:
                    out[k] = _redact(v)
            return out
        if isinstance(obj, list):
            return [_redact(x) for x in obj]
        return obj
    except Exception:
        return obj


@dataclass
class RunHeader:
    kind: str
    session_id: str
    run_id: str
    started_at: str
    goal: str
    target_fqname: Optional[str]


class RunLogger:
    """
    Simple JSONL run logger + Markdown summary.

    Directory layout:
      runs/
        simula/
          YYYY-MM-DD/
            <session_id>/
              <run_id>/
                run.jsonl
                summary.md
                meta.json
    """

    def __init__(self, session_id: str, goal: str, target_fqname: Optional[str] = None):
        date_str = datetime.utcnow().strftime("%Y-%m-%d")
        run_id = datetime.utcnow().strftime("%Y%m%dT%H%M%S")
        root = Path(os.getenv("SIMULA_RUNS_DIR", "runs")) / "simula" / date_str / session_id / run_id
        root.mkdir(parents=True, exist_ok=True)

        self.session_id = session_id
        self.goal = goal
        self.target_fqname = target_fqname
        self.run_id = run_id
        self.root = root
        self.jsonl_path = root / "run.jsonl"
        self.summary_path = root / "summary.md"
        self.meta_path = root / "meta.json"

        self._summary_chunks: list[str] = []
        self._t0 = time.time()

        header = RunHeader(
            kind="run_header",
            session_id=session_id,
            run_id=run_id,
            started_at=_ts(),
            goal=goal,
            target_fqname=target_fqname,
        )
        self._write_jsonl(asdict(header))
        self._summary_chunks.append(f"# Simula Codegen Run\n\n- **Session:** `{session_id}`\n- **Run:** `{run_id}`\n- **Goal:** {goal}\n- **Target:** `{target_fqname or '—'}`\n- **Started:** {header.started_at}\n")

        # meta snapshot
        meta = {
            "env": {k: v for k, v in os.environ.items() if k.startswith(("APP_", "SIMULA_", "SYNAPSE_", "OPENAI_", "GOOGLE_", "GEMINI_"))},
            "cwd": os.getcwd(),
        }
        with self.meta_path.open("w", encoding="utf-8") as f:
            json.dump(_redact(meta), f, indent=2, ensure_ascii=False)

    # --- core write helpers ---

    def _write_jsonl(self, obj: Dict[str, Any]) -> None:
        obj = _redact(obj)
        with self.jsonl_path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(obj, ensure_ascii=False))
            f.write("\n")

    def _append_summary(self, md: str) -> None:
        self._summary_chunks.append(md)

    # --- public event apis ---

    def log_synapse_selection(self, task_ctx: Dict[str, Any], selection: Dict[str, Any], error: str | None = None) -> None:
        evt = {
            "kind": "synapse.select_or_plan",
            "t": _ts(),
            "task_ctx": task_ctx,
            "selection": selection if not error else None,
            "error": error,
        }
        self._write_jsonl(evt)
        if error:
            self._append_summary(f"\n## Strategy Selection (ERROR)\n```\n{error}\n```\n")
        else:
            arm_id = selection.get("champion_arm", {}).get("arm_id", "unknown")
            ep = selection.get("episode_id")
            self._append_summary(f"\n## Strategy Selection\n- **Episode:** `{ep}`\n- **Champion Arm:** `{arm_id}`\n")

    def log_llm(self, *, phase: str, scope: str, agent: str, prompt_preview: str | None, prompt_struct: Dict[str, Any] | None, completion_text: str | None, extra: Dict[str, Any] | None = None) -> None:
        evt = {
            "kind": "llm.call",
            "t": _ts(),
            "phase": phase,
            "scope": scope,
            "agent": agent,
            "prompt": {
                "preview": prompt_preview,
                "struct": prompt_struct,
            },
            "completion_text": completion_text,
            "extra": extra or {},
        }
        self._write_jsonl(evt)

        # summary (trim long)
        pv = (prompt_preview or "").strip()
        if len(pv) > 800:
            pv = pv[:800] + " …"
        ct = (completion_text or "").strip()
        if len(ct) > 800:
            ct = ct[:800] + " …"

        self._append_summary(f"\n### LLM · {phase} · `{scope}`\n<details><summary>Prompt (preview)</summary>\n\n```\n{pv}\n```\n</details>\n\n<details><summary>Completion</summary>\n\n```\n{ct}\n```\n</details>\n")

    def log_tool_call(self, *, index: int, tool_name: str, parameters: Dict[str, Any], outcome: Dict[str, Any]) -> None:
        evt = {
            "kind": "tool.call",
            "t": _ts(),
            "index": index,
            "tool_name": tool_name,
            "parameters": parameters,
            "outcome": outcome,
        }
        self._write_jsonl(evt)

        ok = (outcome.get("status", "") or "").lower()
        self._append_summary(f"\n### Tool {index}: `{tool_name}` — **{ok or 'unknown'}**\n<details><summary>Parameters</summary>\n\n```json\n{json.dumps(_redact(parameters), indent=2, ensure_ascii=False)}\n```\n</details>\n<details><summary>Outcome</summary>\n\n```json\n{json.dumps(_redact(outcome), indent=2, ensure_ascii=False)}\n```\n</details>\n")

    def log_http(self, *, method: str, url: str, status: int | None, request: Dict[str, Any] | None = None, response: Dict[str, Any] | None = None, error: str | None = None) -> None:
        evt = {
            "kind": "http.call",
            "t": _ts(),
            "method": method,
            "url": url,
            "status": status,
            "request": request,
            "response": response,
            "error": error,
        }
        self._write_jsonl(evt)
        st = status if status is not None else "—"
        self._append_summary(f"\n### HTTP {method} {url} → {st}\n")

    def log_outcome(self, *, status: str, episode_id: Optional[str], utility_score: Any, notes: Dict[str, Any]) -> None:
        evt = {
            "kind": "run.outcome",
            "t": _ts(),
            "status": status,
            "episode_id": episode_id,
            "utility_score": utility_score,
            "notes": notes,
            "elapsed_s": round(time.time() - self._t0, 3),
        }
        self._write_jsonl(evt)
        self._append_summary(f"\n## Outcome\n- **Status:** **{status}**\n- **Episode:** `{episode_id or '—'}`\n- **Utility:** `{utility_score}`\n- **Elapsed:** `{evt['elapsed_s']}s`\n")

    def save(self) -> None:
        with self.summary_path.open("w", encoding="utf-8") as f:
            f.write("\n".join(self._summary_chunks))

    # convenience: compute a stable id for any blob you want to cross-reference
    def stable_id(self, obj: Any) -> str:
        return _sha16(_redact(obj))

# ===== FILE: D:\EcodiaOS\systems\simula\agent\runner.py =====
# systems/simula/agent/runner.py
from __future__ import annotations

import asyncio
import hashlib
import json
import logging
from typing import Any, Dict, List, Optional, Tuple

from core.services.synapse import SynapseClient
from systems.synapse.schemas import TaskContext, SelectArmResponse, ArmScore

# Planner & LLM
from core.prompting.orchestrator import build_prompt
from core.utils.llm_gateway_client import call_llm_service, extract_json_flex

# Optional analytics/registration for dynamic arm nodes
from core.utils.neo.cypher_query import cypher_query

# Dispatches tools by arm id (handles simula.agent.<tool>.<variant> → tool)
from .dispatcher import dispatch_tool

# +++ FIX: Import the memory tools to resolve the 'not defined' error +++
from systems.simula.nscs.agent_tools import memory_read, memory_write

log = logging.getLogger(__name__)


# ------------------------------- ID / JSON helpers -------------------------------

def _canonical_json(obj: Any) -> str:
    """Creates a stable, sorted JSON string from an object."""
    try:
        if hasattr(obj, "model_dump"):
            obj = obj.model_dump(mode="json")
        return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
    except Exception:
        return json.dumps({"_repr": str(obj)})


def _sha16_for(obj: Any) -> str:
    """Generates a short SHA256 hash for an object."""
    return hashlib.sha256(_canonical_json(obj).encode("utf-8")).hexdigest()[:16]


# ------------------------------- target parsing -------------------------------

def _path_from_fqname(target_fqname: Optional[str]) -> Optional[str]:
    """'path/to/file.py::my_func' -> 'path/to/file.py'"""
    if not target_fqname:
        return None
    return target_fqname.split("::", 1)[0] or None


def _func_from_fqname(target_fqname: Optional[str]) -> Optional[str]:
    """'path/to/file.py::my_func' -> 'my_func'"""
    if not target_fqname or "::" not in target_fqname:
        return None
    return target_fqname.split("::", 1)[1] or None


# ------------------------------- plan shaping -------------------------------

def _coerce_plan_steps(raw_plan: Any) -> List[Dict[str, Any]]:
    """Normalizes a raw LLM plan into a canonical list of step dictionaries."""
    steps: List[Dict[str, Any]] = []
    if not isinstance(raw_plan, list):
        return [{"action_type": "respond", "tool_name": None, "parameters": {}}]

    for s in raw_plan:
        if not isinstance(s, dict):
            continue
        action = str(s.get("action_type") or "").lower()
        if action not in ("tool_call", "respond"):
            action = "tool_call" # Default to tool_call if ambiguous
        tool_name = s.get("tool_name") if action == "tool_call" else None
        params = s.get("parameters")
        if not isinstance(params, dict):
            params = {}
        steps.append({"action_type": action, "tool_name": tool_name, "parameters": params})

    if not any(step.get("action_type") == "respond" for step in steps):
        steps.append({"action_type": "respond", "tool_name": None, "parameters": {}})
    return steps


def _needs_module_function(tool_name: Optional[str]) -> bool:
    """Identifies tools that require 'module' and/or 'function' context."""
    return tool_name in {
        "run_fuzz_smoke",
        "generate_property_test",
        "debug_with_runtime_trace",
    }


def _inject_target_defaults(params: Dict[str, Any], *, target_fqname: Optional[str]) -> Dict[str, Any]:
    """Fills common missing parameters like 'module' or 'paths' from the target FQN."""
    if not target_fqname:
        return params
        
    mod = _path_from_fqname(target_fqname)
    fn = _func_from_fqname(target_fqname)

    if mod and "paths" not in params:
        params["paths"] = [mod]
    if mod and "module" not in params:
        params["module"] = mod
    if fn and "function" not in params:
        params["function"] = fn
    return params


# ------------------------------- planning -------------------------------

async def _make_simula_plan(
    *,
    champion: ArmScore,
    goal: str,
    target_fqname: Optional[str],
    dossier: Dict[str, Any],
    turn_history: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """Invokes the Simula unified planner with full context to generate a multi-step plan."""
    context = {
        "goal": goal,
        "retrieval_query": goal,
        "target_fqname": target_fqname,
        "dossier": dossier or {},
        "turn_history": turn_history,
        "selected_arm_id": champion.arm_id,
        "policy_hints": champion.policy_graph_meta or {},
    }
    prompt = await build_prompt(
        scope="simula.main.planning",
        context=context,
        summary="Simula planning: Analyze context and generate a precise tool sequence.",
    )
    llm_resp = await call_llm_service(prompt, agent_name="Simula.Planner", scope="simula.main.planning")

    plan_obj = extract_json_flex(getattr(llm_resp, "text", "")) or {}
    if not isinstance(plan_obj, dict):
        plan_obj = {}

    normalized = {
        "interim_thought": (plan_obj.get("interim_thought") or "No thought provided.").strip(),
        "scratchpad": (plan_obj.get("scratchpad") or "No scratchpad provided.").strip(),
        "plan": _coerce_plan_steps(plan_obj.get("plan")),
        "final_synthesis_prompt": (plan_obj.get("final_synthesis_prompt") or "Summarize the results.").strip(),
    }

    dyn_hash = _sha16_for({"arm": champion.arm_id, "norm": normalized})
    dyn_arm_id = f"dyn::{dyn_hash}"
    normalized["champion_arm_id"] = dyn_arm_id

    try:
        await cypher_query(
            """
            MERGE (a:PolicyArm {id: $id})
            ON CREATE SET a.created_ts = timestamp(), a.dynamic = true, a.mode = 'simula_planful'
            ON MATCH  SET a.dynamic = true
            """,
            {"id": dyn_arm_id},
        )
    except Exception as e:
        log.warning("[SimulaRunner] Graph registration for dynamic arm failed: %r", e)

    return normalized


# ------------------------------- execution -------------------------------

async def _execute_plan(plan_steps: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Executes plan steps, batching adjacent tool calls for concurrency."""
    results: Dict[str, Any] = {}
    i = 0
    while i < len(plan_steps):
        step = plan_steps[i]
        action = (step.get("action_type") or "").lower()

        if action != "tool_call":
            if action == "respond":
                results[f"step_{i}_respond"] = {"status": "queued_for_synthesis"}
            i += 1
            continue

        batch: List[Tuple[int, str, Dict[str, Any]]] = []
        while i < len(plan_steps) and (plan_steps[i].get("action_type") or "").lower() == "tool_call":
            s = plan_steps[i]
            tool_name = s.get("tool_name")
            if not tool_name:
                results[f"step_{i}_error"] = {"status": "error", "reason": "Missing tool_name for tool_call action."}
                i += 1
                continue
            
            params = s.get("parameters") or {}
            batch.append((i, tool_name, params))
            i += 1

        async def run_one(idx: int, name: str, p: Dict[str, Any]) -> Tuple[int, str, Dict[str, Any]]:
            arm_id = f"simula.agent.{name}"
            try:
                outcome = await dispatch_tool(arm_id, p)
            except Exception as e:
                log.exception("Tool execution crashed for '%s'", arm_id)
                outcome = {"status": "error", "reason": f"Tool execution crashed: {e!r}"}
            return idx, name, outcome

        if batch:
            batch_results = await asyncio.gather(*(run_one(idx, name, p) for idx, name, p in batch))
            for original_index, name, outcome in batch_results:
                results[f"step_{original_index}_{name}"] = outcome

    return results


# ------------------------------- scoring utilities -------------------------------

def _extract_final_diff_from_results(results: Dict[str, Any]) -> str:
    """Heuristically extracts the most likely unified diff from tool execution results."""
    if not isinstance(results, dict): return ""
    candidates: List[str] = []
    for _, outcome in results.items():
        if not isinstance(outcome, dict): continue
        
        # Check inside a 'result' dictionary
        res = outcome.get("result")
        if isinstance(res, dict):
            for k in ("diff", "unified_diff", "patch"):
                v = res.get(k)
                if isinstance(v, str) and "--- a/" in v:
                    candidates.append(v)
        
        # Check at the top level of the outcome
        for k in ("diff", "unified_diff", "patch"):
            v2 = outcome.get(k)
            if isinstance(v2, str) and "--- a/" in v2:
                candidates.append(v2)
                
    if not candidates: return ""
    return max(candidates, key=len)


def _extract_verification_results(results: Dict[str, Any]) -> Dict[str, Any]:
    """Extracts summaries from test, lint, or analysis tools."""
    if not isinstance(results, dict): return {}
    
    for _, outcome in results.items():
        if not isinstance(outcome, dict): continue
        res = outcome.get("result")
        if isinstance(res, dict):
            for k in ("test_results", "verification_results", "gauntlet", "analysis", "review", "diagnostics"):
                v = res.get(k)
                if isinstance(v, (dict, list, str, int, float, bool)):
                    return {"source": k, "payload": v}
                    
    return {"summary": {k: v.get("status", str(type(v))) for k, v in results.items()}}

async def _run_utility_scorer(
    *,
    goal: str,
    dossier: Dict[str, Any],
    plan: Dict[str, Any],
    execution_results: Dict[str, Any],
    final_diff: str,
    verification_results: Dict[str, Any],
) -> Dict[str, Any]:
    """Runs the Simula Utility Scorer with full turn context for intelligent evaluation."""
    context = {
        "goal": goal,
        "initial_context": {"dossier": dossier, "dossier_provided": bool(dossier)},
        "agent_plan": plan,
        "tool_outcomes": execution_results,
        "final_diff": final_diff,
        "verification_summary": verification_results,
    }
    try:
        prompt = await build_prompt(
            scope="simula.utility_scorer",
            context=context,
            summary="Holistically score the quality and strategy of the code evolution turn.",
        )
        resp = await call_llm_service(prompt, agent_name="Simula.UtilityScorer", scope="simula.utility_scorer")
        data = extract_json_flex(getattr(resp, "text", ""))

        if isinstance(data, dict) and "utility_score" in data:
            try:
                score = float(data.get("utility_score", 0.5))
            except (ValueError, TypeError):
                score = 0.5
            data["utility_score"] = max(0.0, min(1.0, score))
            return data
        return {"utility_score": 0.5, "reasoning": "Scorer returned invalid format or failed to parse."}
    except Exception as e:
        log.error("[SimulaRunner] Utility Scorer failed: %r", e, exc_info=True)
        return {"utility_score": 0.5, "reasoning": f"Utility Scorer crashed: {e}"}

# ------------------------------- public runner -------------------------------

class SimulaRunner:
    def __init__(self) -> None:
        self.synapse_client = SynapseClient()

    async def run(self, *, goal: str, dossier: dict, target_fqname: Optional[str], session_id: str) -> Dict[str, Any]:
        """
        Single-turn Simula runner with dynamic planning and integrated session memory.
        """
        # 1. Read previous turn history for this session
        history_key = f"simula_session_{session_id}_history"
        turn_history_result = await memory_read(key=history_key)
        turn_history = turn_history_result.get("value") or []
        log.info(f"[SimulaRunner] Loaded {len(turn_history)} previous turns from session '{session_id}'.")

        # 2. Build TaskContext with History and Dossier
        task_ctx = TaskContext(
            task_key="simula.agent.turn",
            goal=goal,
            risk_level="high",
            budget="constrained",
            metadata={
                "target_fqname": target_fqname,
                "dossier": dossier or {},
                "turn_history": turn_history,
            },
        )

        # 3. Get Decision from Synapse (bandit/base pick)
        try:
            selection: SelectArmResponse = await self.synapse_client.select_or_plan(task_ctx, candidates=[])
            champion = selection.champion_arm
            episode_id = selection.episode_id
            base_arm_id = champion.arm_id
            log.info("[SimulaRunner] Synapse chose base arm: %s (ep: %s)", base_arm_id, episode_id)
        except Exception as e:
            log.error("Synapse client failed: %r", e, exc_info=True)
            return {"status": "error", "reason": "Failed to get a decision from Synapse."}

        # 4. Determine plan source or generate one dynamically
        plan_dict: Optional[Dict[str, Any]] = None
        if isinstance(getattr(champion, "content", None), dict):
            c: Dict[str, Any] = champion.content
            if bool(isinstance(c.get("plan"), list) and ("final_synthesis_prompt" in c)):
                plan_dict = {
                    "interim_thought": (c.get("interim_thought") or c.get("scratchpad") or "").strip(),
                    "scratchpad": c.get("scratchpad") or "",
                    "plan": _coerce_plan_steps(c.get("plan")),
                    "final_synthesis_prompt": c.get("final_synthesis_prompt") or "",
                    "champion_arm_id": base_arm_id,
                }

        if plan_dict is None:
            plan_dict = await _make_simula_plan(
                champion=champion,
                goal=goal,
                target_fqname=target_fqname,
                dossier=task_ctx.metadata.get("dossier", {}),
                turn_history=task_ctx.metadata.get("turn_history", []),
            )

        # 5. Normalize and Inject Parameters
        plan_steps = plan_dict.get("plan") or []
        for s in plan_steps:
            if (s.get("action_type") or "").lower() == "tool_call":
                name = s.get("tool_name")
                params = s.get("parameters") or {}
                if _needs_module_function(name):
                    params = _inject_target_defaults(params, target_fqname=target_fqname)
                
                if name in ("write_file", "write_code") and not params.get("path"):
                    path_from_target = _path_from_fqname(target_fqname)
                    if path_from_target:
                        params["path"] = path_from_target
                        log.info(f"Injected missing 'path' parameter for '{name}' tool: {path_from_target}")
                
                s["parameters"] = params

        # 6. Execute Plan
        execution_results = await _execute_plan(plan_steps)
        is_success = any((v.get("status") or "").lower() in {"success", "ok", "proposed"} for v in execution_results.values() if isinstance(v, dict))

        # 7. Score Utility
        final_diff = _extract_final_diff_from_results(execution_results)
        verification_results = _extract_verification_results(execution_results)
        scorer = await _run_utility_scorer(
            goal=goal,
            dossier=dossier,
            plan=plan_dict,
            execution_results=execution_results,
            final_diff=final_diff,
            verification_results=verification_results,
        )
        
        # 8. Create Turn Summary for Memory
        current_turn_summary = {
            "turn_number": len(turn_history) + 1,
            "goal_for_turn": goal,
            "plan_thought": plan_dict.get("interim_thought"),
            "plan_actions": plan_dict.get("plan"),
            "execution_outcomes": {k: v.get("status", "error") for k, v in execution_results.items()},
            "utility_score": scorer.get("utility_score"),
            "utility_reasoning": scorer.get("reasoning"),
        }
        
        # 9. Write History Back to Memory
        new_history = turn_history + [current_turn_summary]
        await memory_write(key=history_key, value=new_history)

        # 10. Log Telemetry to Synapse
        chosen_arm_id_for_logging = plan_dict.get("champion_arm_id") or base_arm_id
        metrics: Dict[str, Any] = {
            "chosen_arm_id": chosen_arm_id_for_logging,
            "success": 1.0 if is_success else 0.0,
            "status": "ok" if is_success else "error",
            "utility_score": scorer.get("utility_score"),
            # Add other detailed metrics for analysis
            "details": {
                "tool_plan": plan_steps,
                "tool_results": {k: v.get("result") for k, v in execution_results.items() if isinstance(v, dict) and "result" in v},
                "tool_reasons": {k: v.get("reason") for k, v in execution_results.items() if isinstance(v, dict) and "reason" in v},
                "utility_reasoning": scorer.get("reasoning"),
                "final_diff_present": bool(final_diff),
            }
        }

        try:
            await self.synapse_client.log_outcome(
                episode_id=episode_id,
                task_key=task_ctx.task_key,
                metrics=metrics,
            )
        except Exception as e:
            log.error("Failed to log outcome to Synapse: %r", e, exc_info=True)

        # 11. Return Enriched Response
        return {
            "status": "ok" if is_success else "error",
            "episode_id": episode_id,
            "session_id": session_id,
            "selected_arm_id": chosen_arm_id_for_logging,
            "interim_thought": plan_dict.get("interim_thought"),
            "scratchpad": plan_dict.get("scratchpad"),
            "final_synthesis_prompt": plan_dict.get("final_synthesis_prompt"),
            "plan": plan_steps,
            "tool_execution": execution_results,
            "utility": scorer,
        }
# ===== FILE: D:\EcodiaOS\systems\simula\agent\scl_context.py =====
# systems/simula/agent/scl_context.py
from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field

from systems.synapse.schemas import ArmScore
from systems.simula.nscs.agent_tools import memory_read, memory_write

log = logging.getLogger(__name__)

class FileCard(BaseModel):
    path: str
    summary: str
    relevance_score: float = 0.0
    entities: List[str] = Field(default_factory=list)

class ToolHint(BaseModel):
    signature: str
    description: str
    relevance_score: float = 0.0

class WorkingContext(BaseModel):
    """A compact, token-budgeted context object for the deliberation room."""
    goal: str
    target_fqname: Optional[str]
    strategy_arm: ArmScore
    file_cards: List[FileCard] = Field(default_factory=list)
    tool_hints: List[ToolHint] = Field(default_factory=list)
    history_summary: str
    blackboard_insights: Dict[str, Any] = Field(default_factory=dict)

    # NEW: safe scratchpads for templates / orchestrator
    context_vars: Dict[str, Any] = Field(default_factory=dict)
    extras: Dict[str, Any] = Field(default_factory=dict)

class Blackboard:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.history_key = f"simula_session_{session_id}_history"
        self.turn_history: List[Dict[str, Any]] = []
        self.insights: Dict[str, Any] = {}  # For ephemeral data like last_error

    async def load_from_memory(self):
        result = await memory_read(key=self.history_key)
        self.turn_history = (result.get("value") or []) if isinstance(result.get("value"), list) else []

    async def persist_to_memory(self):
        await memory_write(key=self.history_key, value=self.turn_history)

    def get_history_summary(self) -> str:
        if not self.turn_history:
            return "This is the first turn. No prior history."
        last_turn = self.turn_history[-1]
        score = last_turn.get('utility_score', 'N/A')
        reason = last_turn.get('utility_reasoning', 'No reasoning recorded.')
        return f"Last turn (Turn {len(self.turn_history)}) scored {score}. Reasoning: '{reason}'"

    async def record_turn(self, *, goal: str, plan: dict, execution_outcomes: dict, utility_score: float, utility_reasoning: str):
        turn_summary = {
            "turn_number": len(self.turn_history) + 1,
            "goal_for_turn": goal,
            "plan_thought": plan.get("interim_thought"),
            "plan_actions": plan.get("plan"),
            "execution_outcomes": execution_outcomes,
            "utility_score": utility_score,
            "utility_reasoning": utility_reasoning,
        }
        self.turn_history.append(turn_summary)

async def synthesize_context(
    *,
    strategy_arm: ArmScore,
    goal: str,
    target_fqname: Optional[str],
    dossier: Dict[str, Any],
    blackboard: Blackboard,
) -> WorkingContext:
    file_cards = []
    if dossier:
        for file_path, content in dossier.items():
            if isinstance(content, str) and content.strip():
                first_lines = content.strip().splitlines()[:5]
                summary = " ".join(first_lines)
                if len(summary) > 250:
                    summary = summary[:250] + "..."
            else:
                summary = f"Content of file {file_path} is empty or not a string."
            file_cards.append(FileCard(path=file_path, summary=summary, relevance_score=0.9))

    tool_hints = [
        ToolHint(signature="read_file(path: str)", description="Reads the content of a file.", relevance_score=0.8),
        ToolHint(signature="write_file(path: str, content: str)", description="Writes content to a file.", relevance_score=0.9),
        ToolHint(signature="run_tests_and_diagnose_failures(paths: list)", description="Runs tests and diagnoses failures.", relevance_score=0.95),
        ToolHint(signature="static_check(paths: list)", description="Runs linting and type checking.", relevance_score=0.85),
        ToolHint(signature="qora_semantic_search(query_text: str)", description="Finds relevant code examples.", relevance_score=0.7),
    ]

    wc = WorkingContext(
        goal=goal,
        target_fqname=target_fqname,
        strategy_arm=strategy_arm,
        file_cards=file_cards,
        tool_hints=tool_hints,
        history_summary=blackboard.get_history_summary(),
        blackboard_insights=blackboard.insights,
    )

    # Ensure templates ALWAYS have an object-like strategy_arm and convenient mirrors
    sa_dict = strategy_arm.model_dump(mode="json")
    wc.context_vars["strategy_arm"] = sa_dict
    wc.context_vars["strategy_arm_id"] = sa_dict.get("arm_id")
    wc.extras["strategy_arm"] = sa_dict
    wc.extras["strategy_arm_id"] = sa_dict.get("arm_id")

    return wc

# ===== FILE: D:\EcodiaOS\systems\simula\agent\scl_orchestrator.py =====
# systems/simula/agent/scl_orchestrator.py

from __future__ import annotations

import asyncio
import logging
import os
import time
from typing import Any, Dict, List, Optional
from uuid import uuid4

from core.services.synapse import SynapseClient
from systems.synapse.schemas import TaskContext, SelectArmResponse, ArmScore

# --- MDO-ARCH-LIVE: Import all core cognitive components ---
from systems.simula.memory.schemas import SynapticTrace
from systems.simula.memory.trace_db import TraceDBClient
from core.services.qora import QoraClient
from core.llm.embeddings_gemini import get_embedding as embed_text

# --- MDO-ARCH-LIVE: Your existing, battle-tested components ---
from .deliberation import DeliberationRoom
from .scl_context import WorkingContext, Blackboard, synthesize_context
from .dispatcher import dispatch_tool
from .runlog import RunLogger
from .scl_utils import _inject_target_defaults, _run_utility_scorer

log = logging.getLogger(__name__)


def _resolve_sandbox_paths(params: Dict[str, Any]) -> Dict[str, Any]:
    sandbox_root = os.getenv("SANDBOX_ROOT", "/app")
    if not isinstance(params, dict):
        return params
    def _fix(v: Any) -> Any:
        if isinstance(v, str):
            if v.startswith(sandbox_root) or v.startswith("/") or "://" in v: return v
            if ("/" in v) or v.endswith((".py", ".txt", ".md")): return os.path.join(sandbox_root, v.lstrip("/"))
            return v
        if isinstance(v, list): return [_fix(x) for x in v]
        if isinstance(v, dict): return {k: _fix(x) for k, x in v.items()}
        return v
    return {k: _fix(v) for k, v in params.items()}


class SCL_Orchestrator:
    """
    Manages the full cognitive cycle for the MDO, prioritizing reflexes (System 1)
    over deliberation (System 2).
    """

    def __init__(self, session_id: str, synapse_client: SynapseClient | None = None):
        self.session_id = session_id or uuid4().hex
        self.synapse = synapse_client or SynapseClient()
        self.blackboard = Blackboard(session_id=self.session_id)
        self.deliberation_room = DeliberationRoom()

        # MDO-ARCH-LIVE: Initialize clients for the MDO's cognitive functions
        self.trace_db = TraceDBClient()
        self.qora = QoraClient()

    async def _generate_triggering_state_vector(
        self, goal: str, target_fqname: str | None, error_context: str | None = None
    ) -> List[float]:
        """
        Creates a rich, multi-modal embedding representing the "smell" of a problem
        by leveraging the Qora dossier service.
        """
        text_blob = f"GOAL: {goal}\n\nERROR: {error_context or 'None'}"

        if target_fqname:
            dossier = await self.qora.get_dossier(target_fqname)
            target_code = dossier.get("target", {}).get("source_code", "")
            summary = dossier.get("summary", "")
            text_blob += f"\n\nTARGET CONTEXT: {summary}\n\nCODE:\n{target_code}"

        return await embed_text(text_blob, task_type="RETRIEVAL_QUERY")

    async def run(self, *, goal: str, dossier: dict, target_fqname: Optional[str], error_context: Optional[str] = None) -> Dict[str, Any]:
        """Executes a full, single-turn invocation of the MDO's cognitive cycle."""
        runlog = RunLogger(session_id=self.session_id, goal=goal, target_fqname=target_fqname)
        await self.blackboard.load_from_memory()
        log.info("[SCL] Loaded %d turns for session '%s'.", len(self.blackboard.turn_history), self.session_id)

        execution_results: Dict[str, Any] = {}
        final_plan: Dict[str, Any] | List[Dict[str, Any]] = {}
        plan_steps: List[Dict[str, Any]] = []
        episode_id: Optional[str] = None
        was_reflex = False
        state_vector: List[float] = []

        # --- MDO-ARCH-LIVE: REFLEX ARC (SYSTEM 1) ---
        try:
            state_vector = await self._generate_triggering_state_vector(goal, target_fqname, error_context)
            matching_trace = await self.trace_db.search(state_vector, min_confidence=0.8, similarity_threshold=0.95)
        except Exception as e:
            log.error(f"[SCL-S1] Reflex Arc failed during search: {e!r}", exc_info=True)
            matching_trace = None

        if matching_trace:
            was_reflex = True
            log.info(f"[SCL-S1] Reflex Arc Fired! Executing Trace: {matching_trace.trace_id}")
            runlog.log_note("reflex_arc_fired", {"trace_id": matching_trace.trace_id, "confidence": matching_trace.confidence_score})
            
            plan_steps = matching_trace.action_sequence
            final_plan = {"plan": plan_steps, "dynamic_plan_arm_id": f"reflex::{matching_trace.trace_id}"}
            
            log.info("[SCL] Phase 3: Executing %d reflexive steps.", len(plan_steps))
            execution_results = await self._execute_plan(plan_steps, runlog=runlog)
            episode_id = f"reflex_ep_{uuid4().hex}"
            
        else:
            log.info("[SCL-S2] No reflex found. Engaging Deliberative Core.")
            
            # --- MDO-ARCH-LIVE: DELIBERATIVE CORE (SYSTEM 2) ---
            task_ctx = TaskContext(
                task_key="simula.agent.turn", goal=goal, metadata={"target_fqname": target_fqname, "dossier_provided": bool(dossier)}
            )
            try:
                selection = await self.synapse.select_or_plan(task_ctx, candidates=[])
                episode_id = selection.episode_id
                working_context = await synthesize_context(strategy_arm=selection.champion_arm, goal=goal, target_fqname=target_fqname, dossier=dossier, blackboard=self.blackboard)
                deliberation_result = await self.deliberation_room.deliberate(working_context=working_context, runlog=runlog)
            except Exception as e:
                msg = f"Deliberative Core failed during planning: {e!r}"
                log.error("[SCL-S2] CRITICAL: %s", msg, exc_info=True)
                runlog.log_outcome(status="error", episode_id=episode_id, utility_score=None, notes={"reason": msg})
                runlog.save()
                return {"status": "error", "reason": msg}

            if deliberation_result.status != "approved":
                msg = f"Plan rejected: {deliberation_result.reason}"
                log.warning(f"[SCL-S2] {msg}")
                runlog.log_outcome(status="error", episode_id=episode_id, utility_score=None, notes={"deliberation": deliberation_result.model_dump()})
                runlog.save()
                return {"status": "error", "reason": msg, "deliberation": deliberation_result.model_dump()}

            final_plan = deliberation_result.final_plan
            plan_steps = final_plan.get("plan", []) if isinstance(final_plan, dict) else final_plan
            
            for step in plan_steps:
                if step.get("action_type") == "tool_call":
                    params = step.get("parameters", {}) or {}
                    params = _inject_target_defaults(params, tool_name=step.get("tool_name"), target_fqname=target_fqname)
                    params = _resolve_sandbox_paths(params)
                    step["parameters"] = params

            log.info("[SCL] Phase 3: Executing %d approved steps.", len(plan_steps))
            execution_results = await self._execute_plan(plan_steps, runlog=runlog)

        # --- MDO-ARCH-LIVE: PHASE 4: REFLECT AND LEARN ---
        is_success = any((v.get("status", "")).lower() in {"success", "ok", "passed", "proposed"} for v in execution_results.values() if isinstance(v, dict))
        log.info("[SCL] Phase 4: Reflecting on turn outcome for learning.")
        
        scorer_results = await _run_utility_scorer(goal=goal, dossier=dossier, plan=final_plan, execution_results=execution_results, final_diff="", verification_results={}, runlog=runlog)
        utility_score = scorer_results.get("utility_score", 0.0)

        if not was_reflex and is_success and state_vector:
            try:
                new_trace = SynapticTrace(triggering_state_vector=state_vector, action_sequence=plan_steps, outcome_utility=utility_score)
                await self.trace_db.save(new_trace)
                log.info(f"[SCL-S2] Synaptic Solidification: New trace created: {new_trace.trace_id}")
                runlog.log_note("synaptic_solidification", {"trace_id": new_trace.trace_id, "utility": utility_score})
            except Exception as e:
                log.error(f"[SCL-S2] Synaptic Solidification failed: {e!r}", exc_info=True)

        if was_reflex and matching_trace:
            try:
                arm_id_for_synapse = f"trace::{matching_trace.trace_id}"
                feedback = self.synapse.reward_arm if is_success else self.synapse.punish_arm
                await feedback(arm_id=arm_id_for_synapse, value=0.25)
                log.info(f"[SCL-S1] {'Rewarded' if is_success else 'Punished'} reflex: {matching_trace.trace_id}")
                runlog.log_note("reflex_feedback", {"action": "reward" if is_success else "punish", "trace_id": matching_trace.trace_id})
            except Exception as e:
                log.error(f"[SCL-S1] Reflex reinforcement failed: {e!r}", exc_info=True)

        await self.blackboard.record_turn(goal=goal, plan=final_plan, execution_outcomes={k: v.get("status", "error") for k, v in execution_results.items()}, utility_score=utility_score, utility_reasoning=scorer_results.get("reasoning", ""))
        await self.blackboard.persist_to_memory()
        
        metrics = {"chosen_arm_id": final_plan.get("dynamic_plan_arm_id", "dyn::unknown"), "success": 1.0 if is_success else 0.0, "utility_score": utility_score}
        if episode_id:
            await self.synapse.log_outcome(episode_id=episode_id, task_key="simula.agent.reflex" if was_reflex else "simula.agent.turn", metrics=metrics)
            log.info("[SCL] Logged outcome for episode '%s' with score %s.", episode_id, utility_score)

        runlog.log_outcome(status="ok" if is_success else "error", episode_id=episode_id, utility_score=utility_score, notes={"final_plan": final_plan, "tool_execution": execution_results})
        runlog.save()
        
        return {"status": "ok" if is_success else "error", "episode_id": episode_id, "tool_execution": execution_results, "was_reflex": was_reflex}

    async def _execute_plan(self, plan_steps: List[Dict[str, Any]], runlog: RunLogger | None = None) -> Dict[str, Any]:
        results: Dict[str, Any] = {}
        for i, step in enumerate(plan_steps):
            if not isinstance(step, dict) or step.get("action_type") != "tool_call": continue
            tool_name = step.get("tool_name")
            if not tool_name:
                outcome = {"status": "error", "reason": "Missing tool_name"}
                results[f"step_{i}_error"] = outcome
                if runlog: runlog.log_tool_call(index=i, tool_name="(missing)", parameters={}, outcome=outcome)
                continue
            params = step.get("parameters", {}) or {}
            try:
                outcome = await dispatch_tool(f"simula.agent.{tool_name}", params)
            except Exception as e:
                outcome = {"status": "error", "reason": f"Tool '{tool_name}' crashed: {e!r}"}
            results[f"step_{i}_{tool_name}"] = outcome
            if runlog: runlog.log_tool_call(index=i, tool_name=tool_name, parameters=params, outcome=outcome)
        return results
# ===== FILE: D:\EcodiaOS\systems\simula\agent\scl_utils.py =====
from __future__ import annotations

import asyncio
import hashlib
import json
import logging
from typing import Any, Dict, List, Optional

from core.prompting.orchestrator import build_prompt
from core.utils.llm_gateway_client import call_llm_service, extract_json_flex
from core.utils.neo.cypher_query import cypher_query
from systems.synapse.schemas import ArmScore

# NEW: run logger for structured traces
from .runlog import RunLogger

log = logging.getLogger(__name__)

# ------------------------------- ID / Hashing -------------------------------

def _canonical_json(obj: Any) -> str:
    """Creates a stable, sorted JSON string from an object."""
    try:
        if hasattr(obj, "model_dump"):
            obj = obj.model_dump(mode="json")
        return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
    except Exception:
        return json.dumps({"_repr": str(obj)})

def _sha16_for(obj: Any) -> str:
    """Generates a short SHA256 hash for an object."""
    return hashlib.sha256(_canonical_json(obj).encode("utf-8")).hexdigest()[:16]

# ------------------------------- FQN Parsing -------------------------------

def _path_from_fqname(target_fqname: Optional[str]) -> Optional[str]:
    """'path/to/file.py::my_func' -> 'path/to/file.py'"""
    return (target_fqname.split("::", 1)[0] or None) if target_fqname else None

def _func_from_fqname(target_fqname: Optional[str]) -> Optional[str]:
    """'path/to/file.py::my_func' -> 'my_func'"""
    if not target_fqname or "::" not in target_fqname:
        return None
    return target_fqname.split("::", 1)[1] or None

# ------------------------------- Plan Shaping & Injection -------------------------------

def _coerce_plan_steps(raw_plan: Any) -> List[Dict[str, Any]]:
    if not isinstance(raw_plan, list):
        return []
    steps: List[Dict[str, Any]] = []
    for s in raw_plan:
        if not isinstance(s, dict):
            continue
        kind = s.get("action_type")
        if kind == "tool_call":
            steps.append({
                "action_type": "tool_call",
                "tool_name": s.get("tool_name"),
                "parameters": s.get("parameters") if isinstance(s.get("parameters"), dict) else {},
            })
        elif kind == "respond":
            # keep simple response payload if provided
            steps.append({
                "action_type": "respond",
                "parameters": {
                    "content": s.get("content") or s.get("message") or s.get("text") or ""
                },
            })
    return steps

def _inject_target_defaults(params: Dict[str, Any], *, tool_name: str, target_fqname: Optional[str]) -> Dict[str, Any]:
    if not target_fqname:
        return params

    path = _path_from_fqname(target_fqname)
    func = _func_from_fqname(target_fqname)

    if tool_name in {"read_file", "write_file", "delete_file"}:
        if path and "path" not in params:
            params["path"] = path

    elif tool_name in {
        "run_tests", "run_tests_k", "run_tests_xdist",
        "run_tests_and_diagnose_failures",   # <— add this
        "run_repair_engine", "static_check", "format_patch"
    }:
        if path and "paths" not in params:
            params["paths"] = [path]

    elif tool_name in {"run_fuzz_smoke", "generate_property_test", "debug_with_runtime_trace"}:
        if path and "module" not in params:
            params["module"] = path.replace("/", ".").removesuffix(".py")
        if func and "function" not in params:
            params["function"] = func

    return params


async def register_dynamic_arm_in_graph(arm_id: str, mode: str):
    """Merges a dynamic arm node into the graph for analytics."""
    try:
        await cypher_query(
            """
            MERGE (a:PolicyArm {id: $id})
            ON CREATE SET a.created_ts = timestamp(), a.dynamic = true, a.mode = $mode
            ON MATCH SET a.dynamic = true
            """,
            {"id": arm_id, "mode": mode},
        )
    except Exception as e:
        log.warning("[SCL] Graph registration for dynamic arm '%s' failed: %r", arm_id, e)

# ------------------------------- Scoring & Result Extraction -------------------------------

def _extract_final_diff_from_results(results: Dict[str, Any]) -> str:
    """Heuristically extracts the most likely unified diff from tool execution results."""
    candidates = []
    if not isinstance(results, dict):
        return ""
    for outcome in results.values():
        if not isinstance(outcome, dict):
            continue
        if res := outcome.get("result"):
            if isinstance(res, dict):
                for k in ("diff", "patch"):
                    if isinstance(v := res.get(k), str) and "--- a/" in v:
                        candidates.append(v)
    return max(candidates, key=len) if candidates else ""

def _extract_verification_results(results: Dict[str, Any]) -> Dict[str, Any]:
    """Extracts summaries from test, lint, or analysis tools."""
    if not isinstance(results, dict):
        return {}
    for outcome in results.values():
        if isinstance(outcome, dict) and isinstance(res := outcome.get("result"), dict):
            for k in ("test_results", "verification_results", "diagnostics", "review"):
                if v := res.get(k):
                    return {"source": k, "payload": v}
    return {}

async def _run_utility_scorer(
    *,
    goal: str,
    dossier: Dict[str, Any],
    plan: Dict[str, Any],
    execution_results: Dict[str, Any],
    final_diff: str,
    verification_results: Dict[str, Any],
    runlog: "RunLogger | None" = None,  # NEW: optional structured logger
) -> Dict[str, Any]:
    """
    Runs the Simula Utility Scorer with full turn context for intelligent evaluation.
    Now logs the scorer LLM prompt & completion to the run file when a RunLogger is provided.
    """
    context = {
        "goal": goal,
        "initial_context": {"dossier_provided": bool(dossier)},
        "agent_plan": plan,
        "tool_outcomes": execution_results,
        "final_diff": final_diff,
        "verification_summary": verification_results,
    }
    try:
        prompt = await build_prompt(
            scope="simula.utility_scorer",
            context=context,
            summary="Score the code evolution turn.",
        )
        resp = await call_llm_service(
            prompt,
            agent_name="Simula.UtilityScorer",
            scope="simula.utility_scorer",
            timeout=45.0,
        )
        text = getattr(resp, "text", "")
        data = extract_json_flex(text)

        # --- NEW: runlog entry for scorer LLM ---
        if runlog:
            preview = None
            try:
                preview = getattr(prompt, "text", None)
                if not preview and hasattr(prompt, "messages"):
                    msgs = getattr(prompt, "messages", [])
                    preview = msgs[-1]["content"] if msgs else None
            except Exception:
                pass

            runlog.log_llm(
                phase="utility_scorer",
                scope="simula.utility_scorer",
                agent="Simula.UtilityScorer",
                prompt_preview=preview,
                prompt_struct=getattr(prompt, "model_dump", lambda **_: None)() if hasattr(prompt, "model_dump") else None,
                completion_text=text,
                extra={"parsed_keys": list(data.keys()) if isinstance(data, dict) else []},
            )
        # --- END runlog ---

        if isinstance(data, dict) and "utility_score" in data:
            score = float(data.get("utility_score", 0.5))
            data["utility_score"] = max(0.0, min(1.0, score))
            return data
        return {"utility_score": 0.5, "reasoning": "Scorer returned invalid format."}
    except Exception as e:
        log.error("[SCL] Utility Scorer failed: %r", e, exc_info=True)
        if runlog:
            runlog.log_llm(
                phase="utility_scorer",
                scope="simula.utility_scorer",
                agent="Simula.UtilityScorer",
                prompt_preview="(exception)",
                prompt_struct=None,
                completion_text=f"(error) {e!r}",
                extra={"crashed": True},
            )
        return {"utility_score": 0.5, "reasoning": f"Utility Scorer crashed: {e}"}

# ===== FILE: D:\EcodiaOS\systems\simula\agent\strategies\apply_refactor_smart.py =====
# systems/simula/agent/strategies/apply_refactor_smart.py
from __future__ import annotations

import re

from systems.simula.agent import tools as _t

_HUNK_RE = re.compile(r"^diff --git a/.+?$\n(?:.+\n)+?(?=^diff --git a/|\Z)", re.M)


def _split_unified_diff(diff: str) -> list[str]:
    hunks = _HUNK_RE.findall(diff or "")
    return hunks if hunks else ([diff] if diff else [])


async def apply_refactor_smart(
    diff: str,
    *,
    verify_paths: list[str] | None = None,
) -> dict[str, object]:
    """
    Apply a large diff in smaller hunks, running tests after each chunk.
    If a chunk fails, stop and report the failing hunk index.
    """
    chunks = _split_unified_diff(diff)
    if not chunks:
        return {"status": "error", "reason": "empty diff"}
    verify = verify_paths or ["tests"]
    applied_count = 0
    for i, chunk in enumerate(chunks):
        res = await _t.apply_refactor({"diff": chunk, "verify_paths": verify})
        if res.get("status") != "success":
            return {
                "status": "partial",
                "applied_chunks": applied_count,
                "failed_chunk": i,
                "logs": res.get("logs"),
            }
        applied_count += 1
    return {"status": "success", "applied_chunks": applied_count}

# ===== FILE: D:\EcodiaOS\systems\simula\artifacts\index.py =====
from __future__ import annotations

import json
import mimetypes
import os
import time
from collections.abc import Iterable
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from systems.simula.config import settings

# Try to use your existing artifacts package if it exposes compatible funcs
_BACKEND = os.getenv("SIMULA_ARTIFACTS_BACKEND", "auto")  # "auto" | "package" | "fs"

_pkg = None
if _BACKEND in ("auto", "package"):
    try:
        # adjust import to your real package/module path
        import artifacts as _pkg  # e.g. `from artifacts import api as _pkg`
    except Exception:
        _pkg = None

ARTIFACT_DIRS = [
    "artifacts",
    "artifacts/reports",
    "artifacts/proposals",
    ".simula",
    "spec_eval",
]
TEXT_EXTS = {
    ".json",
    ".md",
    ".txt",
    ".log",
    ".yaml",
    ".yml",
    ".toml",
    ".py",
    ".cfg",
    ".ini",
    ".csv",
    ".tsv",
    ".diff",
    ".patch",
    ".xml",
    ".html",
    ".css",
    ".js",
    ".sh",
}
MAX_INLINE_BYTES = 256 * 1024


@dataclass
class Artifact:
    path: str
    size: int
    mtime: float
    type: str
    rel_root: str

    def to_dict(self) -> dict[str, Any]:
        d = asdict(self)
        d["mtime_iso"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(self.mtime))
        return d


def _root() -> Path:
    return Path(settings.artifacts_root or (settings.repo_root or ".")).resolve()


# -------- package-backed paths (preferred if available) --------
def _pkg_list(kind: str | None, limit: int) -> dict[str, Any]:
    # Expect your package to offer something like: list(kind=None, limit=200) -> items
    items = _pkg.list(kind=kind, limit=limit)  # type: ignore[attr-defined]
    return {"count": len(items), "items": items, "root": _pkg.root()}  # adjust if needed


def _pkg_read(rel_path: str) -> dict[str, Any]:
    return _pkg.read(rel_path)  # type: ignore[attr-defined]


def _pkg_delete(paths: list[str]) -> dict[str, Any]:
    return _pkg.delete(paths)  # type: ignore[attr-defined]


# -------- filesystem fallback (safe if no package or forced) --------
def _is_textlike(p: Path) -> bool:
    if p.suffix.lower() in TEXT_EXTS:
        return True
    mt, _ = mimetypes.guess_type(str(p))
    return (mt or "").startswith("text/")


def _iter_candidate_files(base: Path) -> Iterable[Path]:
    for d in ARTIFACT_DIRS:
        dp = (base / d).resolve()
        if dp.is_file():
            yield dp
        elif dp.is_dir():
            for fp in dp.rglob("*"):
                if fp.is_file():
                    try:
                        fp.resolve().relative_to(base)  # containment
                    except Exception:
                        continue
                    yield fp


def _infer_type(rel: str, suffix: str) -> str:
    if "reports/" in rel or suffix == ".md":
        return "report"
    if "proposals/" in rel:
        return "proposal"
    if "spec_eval/" in rel and suffix == ".json":
        return "score"
    if rel.endswith("gates.json"):
        return "gates"
    if suffix in (".json", ".yaml", ".yml") and ".simula" in rel:
        return "cache"
    if suffix == ".log":
        return "log"
    return "other"


def _fs_list(kind: str | None, limit: int) -> dict[str, Any]:
    base = _root()
    rows: list[Artifact] = []
    for fp in _iter_candidate_files(base):
        rel = str(fp.relative_to(base))
        t = _infer_type(rel, fp.suffix.lower())
        if kind and t != kind:
            continue
        st = fp.stat()
        rows.append(
            Artifact(path=rel, size=st.st_size, mtime=st.st_mtime, type=t, rel_root=str(base)),
        )
    rows.sort(key=lambda a: (a.mtime, a.size), reverse=True)
    out = [a.to_dict() for a in rows[: max(1, min(1000, limit))]]
    return {"count": len(out), "items": out, "root": str(base)}


def _fs_read(rel_path: str) -> dict[str, Any]:
    base = _root()
    fp = (base / rel_path).resolve()
    fp.relative_to(base)  # containment
    if not fp.exists() or not fp.is_file():
        return {"status": "error", "reason": "not_found"}
    st = fp.stat()
    info = {
        "path": str(fp.relative_to(base)),
        "size": st.st_size,
        "mtime": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(st.st_mtime)),
        "textlike": _is_textlike(fp),
    }
    if st.st_size <= MAX_INLINE_BYTES and _is_textlike(fp):
        try:
            text = fp.read_text(encoding="utf-8", errors="replace")
            try:
                return {
                    "status": "success",
                    "info": info,
                    "content": text,
                    "json": json.loads(text),
                }
            except Exception:
                return {"status": "success", "info": info, "content": text}
        except Exception as e:
            return {"status": "error", "reason": f"read_failed: {e!r}", "info": info}
    return {"status": "success", "info": info}


def _fs_delete(paths: list[str]) -> dict[str, Any]:
    base = _root()
    deleted, failed = [], []
    for rp in paths:
        try:
            fp = (base / rp).resolve()
            fp.relative_to(base)
            if fp.exists() and fp.is_file():
                fp.unlink()
                deleted.append(rp)
            else:
                failed.append({"path": rp, "reason": "not_found"})
        except Exception as e:
            failed.append({"path": rp, "reason": str(e)})
    return {"deleted": deleted, "failed": failed}


# -------- public API (delegates) --------
def list_artifacts(kind: str | None = None, limit: int = 200) -> dict[str, Any]:
    if _pkg and _BACKEND in ("auto", "package"):
        try:
            return _pkg_list(kind, limit)
        except Exception:
            pass
    return _fs_list(kind, limit)


def read_artifact(rel_path: str) -> dict[str, Any]:
    if _pkg and _BACKEND in ("auto", "package"):
        try:
            return _pkg_read(rel_path)
        except Exception:
            pass
    return _fs_read(rel_path)


def delete_artifacts(rel_paths: list[str]) -> dict[str, Any]:
    if _pkg and _BACKEND in ("auto", "package"):
        try:
            return _pkg_delete(rel_paths)
        except Exception:
            pass
    return _fs_delete(rel_paths)

# ===== FILE: D:\EcodiaOS\systems\simula\artifacts\package.py =====
# systems/simula/artifacts/package.py
from __future__ import annotations

import hashlib
import json
import tarfile
import time
from dataclasses import dataclass
from pathlib import Path


@dataclass
class ArtifactBundle:
    path: str
    manifest_path: str
    sha256: str


def _sha256(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1 << 16), b""):
            h.update(chunk)
    return h.hexdigest()


def _collect(paths: list[str]) -> list[Path]:
    out: list[Path] = []
    for p in paths:
        pp = Path(p)
        if pp.is_file():
            out.append(pp)
        elif pp.is_dir():
            for q in pp.rglob("*"):
                if q.is_file():
                    out.append(q)
    return out


def create_artifact_bundle(
    *,
    proposal_id: str,
    evidence: dict[str, object],
    extra_paths: list[str] | None = None,
) -> ArtifactBundle:
    ts = int(time.time())
    root = Path("artifacts/bundles")
    root.mkdir(parents=True, exist_ok=True)
    tar_path = root / f"{proposal_id}_{ts}.tar.gz"
    manifest = {
        "proposal_id": proposal_id,
        "ts": ts,
        "evidence": evidence,
        "extra": extra_paths or [],
    }
    manifest_path = root / f"{proposal_id}_{ts}_manifest.json"
    manifest_path.write_text(json.dumps(manifest, indent=2), encoding="utf-8")

    with tarfile.open(tar_path, "w:gz") as tar:
        tar.add(manifest_path, arcname=manifest_path.name)
        for p in _collect(["artifacts/reports"] + (extra_paths or [])):
            try:
                tar.add(p, arcname=str(p))
            except Exception:
                pass

    return ArtifactBundle(
        path=str(tar_path),
        manifest_path=str(manifest_path),
        sha256=_sha256(tar_path),
    )

# ===== FILE: D:\EcodiaOS\systems\simula\build\detect.py =====
# systems/simula/build/detect.py
from __future__ import annotations

from pathlib import Path
from typing import Literal, Optional

BuildKind = Literal[
    "python", "node", "go", "java", "rust",
    "bazel", "cmake"
]

def detect_build_system(root: str = ".") -> Optional[BuildKind]:
    p = Path(root)
    if (p / "pyproject.toml").exists() or any(p.rglob("*.py")):
        return "python"
    if (p / "package.json").exists():
        return "node"
    if (p / "go.mod").exists() or any(p.rglob("*.go")):
        return "go"
    if (p / "pom.xml").exists() or (p / "build.gradle").exists() or any(p.rglob("*.java")):
        return "java"
    if (p / "Cargo.toml").exists() or any(p.rglob("*.rs")):
        return "rust"
    if (p / "WORKSPACE").exists() or (p / "WORKSPACE.bazel").exists():
        return "bazel"
    if any(p.rglob("CMakeLists.txt")):
        return "cmake"
    return None

# ===== FILE: D:\EcodiaOS\systems\simula\build\run.py =====
# systems/simula/build/run.py
from __future__ import annotations

from typing import Dict, List
from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config
from .detect import detect_build_system

async def run_build_and_tests(paths: List[str] | None = None, *, timeout_sec: int = 2400) -> Dict[str, object]:
    kind = detect_build_system(".")
    paths = paths or ["."]
    async with DockerSandbox(seed_config()).session() as sess:
        if kind == "python":
            out = await sess._run_tool(["bash", "-lc", "pytest -q --maxfail=1 || true"], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0 and "failed" not in (out.get("stdout") or "")
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        if kind == "node":
            out = await sess._run_tool(["bash", "-lc", "npm test --silent || npx jest -w 4 --ci --silent || true"], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0 and "failed" not in (out.get("stdout") or "")
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        if kind == "go":
            out = await sess._run_tool(["bash", "-lc", "go test ./... -count=1 || true"], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0 and "FAIL" not in (out.get("stdout") or "")
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        if kind == "java":
            cmd = "mvn -q -DskipITs test || gradle -q test || true"
            out = await sess._run_tool(["bash", "-lc", cmd], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0 and "FAIL" not in (out.get("stdout") or "")
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        if kind == "rust":
            out = await sess._run_tool(["bash", "-lc", "cargo test --quiet || true"], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0 and "FAILED" not in (out.get("stdout") or "")
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        if kind == "bazel":
            out = await sess._run_tool(["bash", "-lc", "bazel test //... --test_output=errors || true"], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        if kind == "cmake":
            out = await sess._run_tool(["bash", "-lc", "cmake -S . -B build && cmake --build build && ctest --test-dir build -j 4 || true"], timeout=timeout_sec)
            ok = out.get("returncode", 0) == 0
            return {"status": "success" if ok else "failed", "kind": kind, "logs": out}
        return {"status": "success", "kind": "generic", "note": "no build system detected"}

# ===== FILE: D:\EcodiaOS\systems\simula\ci\pipelines.py =====
# systems/simula/ci/pipelines.py
from __future__ import annotations

import textwrap


def github_actions_yaml(*, use_xdist: bool = True) -> str:
    return (
        textwrap.dedent(f"""
    name: Simula Hygiene
    on: [pull_request]
    jobs:
      hygiene:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v4
          - uses: actions/setup-python@v5
            with: {{ python-version: '3.11' }}
          - run: pip install -U pip pytest mypy ruff
          - run: pytest {'-n auto' if use_xdist else ''} -q --maxfail=1 || true
          - run: ruff check . || true
          - run: mypy --hide-error-context --pretty . || true
    """).strip()
        + "\n"
    )


def gitlab_ci_yaml(*, use_xdist: bool = True) -> str:
    return (
        textwrap.dedent(f"""
    stages: [hygiene]
    hygiene:
      stage: hygiene
      image: python:3.11
      script:
        - pip install -U pip pytest mypy ruff
        - pytest {'-n auto' if use_xdist else ''} -q --maxfail=1 || true
        - ruff check . || true
        - mypy --hide-error-context --pretty . || true
    """).strip()
        + "\n"
    )


def render_ci(provider: str = "github", *, use_xdist: bool = True) -> str:
    return (
        github_actions_yaml(use_xdist=use_xdist)
        if provider.lower().startswith("gh")
        else gitlab_ci_yaml(use_xdist=use_xdist)
    )

# ===== FILE: D:\EcodiaOS\systems\simula\client\llm.py =====

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\diffutils.py =====
# systems/simula/code_sim/diffutils.py
from __future__ import annotations

import re

_HUNK_HEADER = re.compile(r"^@@\s+-\d+(?:,\d+)?\s+\+\d+(?:,\d+)?\s+@@")
_PLUS_FILE = re.compile(r"^\+\+\+\s+b/(.+)$")
_MINUS_FILE = re.compile(r"^---\s+a/(.+)$")


def _is_ws_only_change_line(line: str) -> bool:
    # "+    " / "-\t" etc — purely whitespace payload
    return (line.startswith("+") or line.startswith("-")) and (line[1:].strip() == "")


def drop_whitespace_only_hunks(diff_text: str) -> str:
    """
    Best-effort: remove hunks where *every* +/- change line is whitespace-only.
    We keep headers and context intact. If detection is ambiguous, keep the hunk.
    """
    if not diff_text:
        return diff_text

    out: list[str] = []
    buf: list[str] = []
    in_hunk = False
    hunk_has_non_ws_change = False

    def _flush_hunk():
        nonlocal buf, in_hunk, hunk_has_non_ws_change, out
        if not buf:
            return
        if in_hunk:
            if hunk_has_non_ws_change:
                out.extend(buf)  # keep the hunk
            # else: drop the entire hunk
        else:
            out.extend(buf)
        buf = []
        in_hunk = False
        hunk_has_non_ws_change = False

    for ln in diff_text.splitlines():
        if _HUNK_HEADER.match(ln):
            _flush_hunk()
            in_hunk = True
            hunk_has_non_ws_change = False
            buf.append(ln)
            continue

        if in_hunk:
            # Track whether we see any non-whitespace +/- line
            if (ln.startswith("+") or ln.startswith("-")) and not _is_ws_only_change_line(ln):
                hunk_has_non_ws_change = True
            buf.append(ln)
        else:
            buf.append(ln)

    _flush_hunk()
    return "\n".join(out) + ("\n" if diff_text.endswith("\n") else "")

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\eval_types.py =====
# systems/simula/code_sim/eval_types.py
"""
Defines the canonical data structures for evaluation results (EvalResult)
and the logic for aggregating those results into a single score (RewardAggregator).
This is the V2 reward system, which integrates with telemetry and is designed
to be configurable and extensible.
"""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import asdict, dataclass, field
from typing import Any

# Attempt to import telemetry; it's a soft dependency for logging rewards.
try:
    from systems.simula.code_sim.telemetry import telemetry
except ImportError:
    # Create a mock telemetry object if it's not available in the context.
    class MockTelemetry:
        def reward(self, *args, **kwargs):
            pass  # No-op

    telemetry = MockTelemetry()

# =========================
# Evaluation Data Structure
# =========================


@dataclass
class EvalResult:
    """A unified, typed container for all evaluator outputs."""

    # Primary pass ratios / scores, scaled to [0,1]
    unit_pass_ratio: float = 0.0
    integration_pass_ratio: float = 0.0
    static_score: float = 0.0
    contracts_score: float = 0.0
    perf_score: float = 0.0
    coverage_delta_score: float = 0.0
    security_score: float = 0.0

    # Optional penalty to be subtracted from the final score
    policy_penalty: float = 0.0  # [0,1] amount to subtract

    # Configurable thresholds for what constitutes a "pass" for hard gates.
    gate_thresholds: dict[str, float] = field(
        default_factory=lambda: {
            "unit": 0.99,  # Require all or nearly all unit tests to pass
            "contracts": 0.99,  # Require all contracts to be met
            "security": 0.99,  # Require no high-severity security issues
        },
    )

    def as_dict(self) -> dict:
        """Returns the evaluation result as a dictionary."""
        return asdict(self)

    @property
    def hard_gates_ok(self) -> bool:
        """
        Computes whether the results pass the non-negotiable quality gates.
        Treats missing metrics as 0.0 for this calculation.
        """
        return (
            self.unit_pass_ratio >= self.gate_thresholds.get("unit", 1.0)
            and self.contracts_score >= self.gate_thresholds.get("contracts", 1.0)
            and self.security_score >= self.gate_thresholds.get("security", 1.0)
        )


# =========================
# Reward Aggregation Logic
# =========================

DEFAULT_WEIGHTS: dict[str, float] = {
    "unit": 0.40,
    "integration": 0.15,
    "static": 0.10,
    "contracts": 0.15,
    "perf": 0.10,
    "coverage": 0.05,
    "security": 0.05,
}

# Optional calibration functions can be defined to reshape metric scores.
# Example: lambda x: 1 / (1 + exp(-10 * (x - 0.85)))
CalibFn = Callable[[float], float]
CALIBRATORS: dict[str, CalibFn] = {}


class RewardAggregator:
    """
    Calculates a single [0,1] reward score from a complex EvalResult object.
    Enforces hard gates, applies configurable weights, and handles penalties.
    """

    def __init__(self, cfg: dict[str, Any] | None = None):
        cfg = cfg or {}
        w = cfg.get("weights", {})
        self.weights: dict[str, float] = {**DEFAULT_WEIGHTS, **w}

        # Normalize weights to ensure they sum to 1.0
        total_weight = sum(self.weights.values())
        if total_weight <= 0:
            raise ValueError("Total reward weights must be > 0")
        for k in self.weights:
            self.weights[k] /= total_weight

    def _calibrate(self, name: str, value: float) -> float:
        """Applies a calibration function to a metric if one is defined."""
        calibration_fn = CALIBRATORS.get(name)
        value = max(0.0, min(1.0, float(value)))  # Clamp input
        if not calibration_fn:
            return value
        try:
            # Apply and re-clamp the output
            return max(0.0, min(1.0, float(calibration_fn(value))))
        except Exception:
            return value

    def score(self, eval_result: EvalResult) -> float:
        """
        Computes the final reward score. Returns 0.0 if hard gates fail.
        Otherwise, returns the weighted, calibrated, and penalized score.
        """
        # 1. Check hard gates first
        if not eval_result.hard_gates_ok:
            telemetry.reward(0.0, reason="hard_gates_fail", meta=self.explain(eval_result))
            return 0.0

        # 2. Calculate the weighted sum of calibrated metrics
        weighted_sum = sum(
            self.weights.get(metric, 0.0)
            * self._calibrate(
                metric,
                getattr(
                    eval_result,
                    f"{metric}_score",
                    getattr(eval_result, f"{metric}_pass_ratio", 0.0),
                ),
            )
            for metric in self.weights
        )

        # 3. Apply penalties
        penalized_score = max(0.0, weighted_sum - eval_result.policy_penalty)

        final_score = max(0.0, min(1.0, penalized_score))  # Final clamp
        telemetry.reward(final_score, reason="aggregate_score")
        return final_score

    def explain(self, eval_result: EvalResult) -> dict[str, float]:
        """Returns a dictionary showing the contribution of each metric to the score."""
        contributions = {
            metric: self.weights.get(metric, 0.0)
            * self._calibrate(
                metric,
                getattr(
                    eval_result,
                    f"{metric}_score",
                    getattr(eval_result, f"{metric}_pass_ratio", 0.0),
                ),
            )
            for metric in self.weights
        }
        contributions["penalty"] = -eval_result.policy_penalty
        return contributions

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\loop.py =====
# systems/simula/code_sim/loop.py
"""
Simula Code Evolution - Utilities Module

This module previously contained the main SimulaEngine orchestrator. Its core logic
has been refactored into the `execute_planned_code_evolution` tool, which is
now available to the AgentOrchestrator.

This file is preserved to provide essential, stateless utility classes and
functions that support the new tool and other parts of the system, such as:
- Artifact storage and management (`ArtifactStore`)
- Standardized JSON logging (`JsonLogFormatter`)
"""

from __future__ import annotations

import datetime as dt
import json
import logging
import sys
from dataclasses import dataclass
from pathlib import Path

# --- Simula subsystems ---
# Note: The main loop dependencies are now in agent/tools.py

try:
    import yaml
except ImportError as e:
    raise RuntimeError("PyYAML is required for Simula's utility functions.") from e

# =========================
# Utilities & Logging
# =========================


class JsonLogFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        payload = {
            "ts": dt.datetime.utcnow().isoformat(timespec="milliseconds") + "Z",
            "lvl": record.levelname,
            "msg": record.getMessage(),
            "logger": record.name,
        }
        if record.exc_info:
            payload["exc"] = self.formatException(record.exc_info)
        extra = getattr(record, "extra", None)
        if isinstance(extra, dict):
            payload.update(extra)
        return json.dumps(payload, ensure_ascii=False)


def setup_logging(verbose: bool, run_dir: Path) -> None:
    run_dir.mkdir(parents=True, exist_ok=True)
    log = logging.getLogger("simula")  # Get simula-namespaced logger
    log.handlers.clear()
    log.setLevel(logging.DEBUG if verbose else logging.INFO)

    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(JsonLogFormatter())
    log.addHandler(ch)

    fh = logging.FileHandler(run_dir / "simula.log", encoding="utf-8")
    fh.setFormatter(JsonLogFormatter())
    log.addHandler(fh)


def sha1(s: str) -> str:
    import hashlib as _h

    return _h.sha1(s.encode("utf-8")).hexdigest()

# --- DELETED SECTION ---
# The SandboxCfg, OrchestratorCfg, and SimulaConfig dataclasses
# were here. They are now obsolete and have been removed.
# The new Pydantic-based settings in systems/simula/config/__init__.py
# are the single source of truth.

# =========================
# Provenance / Artifacts
# =========================


class ArtifactStore:
    """
    Persists patches, evaluator outputs, and other artifacts for a given run.
    """

    def __init__(self, root_dir: Path, run_id: str):
        self.base = root_dir / "runs" / run_id
        self.base.mkdir(parents=True, exist_ok=True)
        (self.base / "candidates").mkdir(exist_ok=True)
        (self.base / "winners").mkdir(exist_ok=True)
        (self.base / "evaluator").mkdir(exist_ok=True)

    def write_text(self, rel: str, content: str) -> Path:
        p = self.base / rel
        p.parent.mkdir(parents=True, exist_ok=True)
        p.write_text(content, encoding="utf-8")
        return p

    def save_candidate(
        self,
        step_name: str,
        iter_idx: int,
        file_rel: str,
        patch: str,
        tag: str = "",
    ) -> Path:
        h = sha1(patch)[:10]
        safe_rel = (file_rel or "unknown").replace("/", "__")
        name = f"{step_name}_iter{iter_idx:02d}_{safe_rel}_{h}{('_' + tag) if tag else ''}.diff"
        return self.write_text(f"candidates/{name}", patch)
# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\portfolio.py =====
# systems/simula/code_sim/portfolio.py
# REFACTORED: Standardized on passing a 'step_dict' dictionary instead of a 'step' object.
from __future__ import annotations

from typing import Any

from systems.simula.code_sim.mutators.ast_refactor import AstMutator
from systems.simula.code_sim.mutators.prompt_patch import llm_unified_diff
from systems.simula.code_sim.telemetry import telemetry


async def _generate_single_candidate(step_dict: dict[str, Any], strategy: str) -> str | None:
    """Generates a single code modification candidate (diff) based on the chosen strategy."""
    if strategy == "llm_base":
        return await llm_unified_diff(step_dict, variant="base")
    if strategy == "llm_creative":
        return await llm_unified_diff(step_dict, variant="creative")
    if strategy == "ast_scaffold":
        # Pass the dictionary directly to the mutator
        return AstMutator(aggressive=False).mutate(step_dict=step_dict, mode="scaffold")
    # Add more strategies here
    return None


async def generate_candidate_portfolio(
    job_meta: dict,
    step_dict: dict[str, Any],
) -> list[dict[str, Any]]:
    """
    Generates a portfolio of candidate diffs using various strategies.
    This function NO LONGER evaluates, scores, or ranks candidates. That is
    the sole responsibility of Synapse.
    """
    strategies = ["llm_base", "llm_creative", "ast_scaffold"]
    candidate_diffs: list[str] = []

    # --- Generate diffs for all strategies ---
    for strategy in strategies:
        diff = await _generate_single_candidate(step_dict, strategy)
        if diff:
            candidate_diffs.append(diff)
            telemetry.log_event(
                "candidate_generated",
                {
                    "job_id": job_meta.get("job_id"),
                    "step": step_dict.get("name", "unknown"), # Access name from dict
                    "strategy": strategy,
                    "diff_size": len(diff.splitlines()),
                },
            )

    # Package the raw diffs into the content payload for Synapse
    portfolio = []
    for diff_text in set(candidate_diffs):  # Use set to de-duplicate
        portfolio.append(
            {
                "type": "unified_diff",
                "diff": diff_text,
                # In the future, add more metadata here like the source strategy
            },
        )

    return portfolio
# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\telemetry.py =====
# systems/simula/code_sim/telemetry.py
"""
Drop-in, zero-dependency (stdlib-only) telemetry for Simula.
"""

from __future__ import annotations

import contextvars
import datetime as _dt
import inspect
import json
import os
import sys
import time
import uuid
from collections.abc import Callable
from dataclasses import dataclass, field
from typing import Any
from functools import wraps # +++ Import wraps for proper decorator behavior +++

# ---------------- Core state ----------------
_current_job: contextvars.ContextVar[str | None] = contextvars.ContextVar(
    "simula_job",
    default=None,
)

# +++ NEW: The central registry for all decorated tools. +++
# This dictionary will be populated when agent_tools.py is imported.
_TOOL_REGISTRY: dict[str, Callable[..., Any]] = {}


def _now_iso() -> str:
    return _dt.datetime.now(tz=_dt.UTC).isoformat()


def _redact(obj: Any) -> Any:
    try:
        s = json.dumps(obj)
        if len(s) > 50_000:
            return {"_redacted": True, "reason": "payload_too_large", "approx_bytes": len(s)}
        return obj
    except Exception:
        return str(obj)


@dataclass
class Telemetry:
    # ... (class implementation is unchanged) ...
    enabled: bool = False
    sink: str = "both"  # stdout|file|both
    trace_dir: str = "/app/.simula/traces"
    sample: float = 1.0
    redact: bool = True
    _job_start_ts: dict[str, float] = field(default_factory=dict)

    # -------- lifecycle --------
    @classmethod
    def from_env(cls) -> Telemetry:
        enabled = os.getenv("SIMULA_TRACE", "0") not in ("0", "false", "False", "off", None)
        sink = os.getenv("SIMULA_TRACE_SINK", "both")
        trace_dir = os.getenv("SIMULA_TRACE_DIR", "/app/.simula/traces")
        sample = float(os.getenv("SIMULA_TRACE_SAMPLE", "1.0"))
        redact = os.getenv("SIMULA_TRACE_REDACT", "1") not in ("0", "false", "False", "off")
        t = cls(enabled=enabled, sink=sink, trace_dir=trace_dir, sample=sample, redact=redact)
        if enabled:
            t._ensure_dirs()
        return t

    def enable_if_env(self) -> None:
        if self.enabled:
            self._ensure_dirs()

    # -------- writing --------
    def _ensure_dirs(self) -> None:
        day = _dt.datetime.now().strftime("%Y-%m-%d")
        day_dir = os.path.join(self.trace_dir, day)
        os.makedirs(day_dir, exist_ok=True)

    def _job_file(self, job_id: str) -> str:
        day = _dt.datetime.now().strftime("%Y-%m-%d")
        day_dir = os.path.join(self.trace_dir, day)
        os.makedirs(day_dir, exist_ok=True)
        return os.path.join(day_dir, f"{job_id}.jsonl")

    def _write(self, job_id: str, event: dict[str, Any]) -> None:
        if not self.enabled:
            return
        try:
            event.setdefault("ts", _now_iso())
            line = json.dumps(event, ensure_ascii=False)
            if self.sink in ("stdout", "both"):
                print(f"SIMULA.TRACE {job_id} {line}")
            if self.sink in ("file", "both"):
                with open(self._job_file(job_id), "a", encoding="utf-8") as f:
                    f.write(line + "\n")
        except Exception as e:
            print(f"[telemetry] write error: {e}", file=sys.stderr)

    # -------- public API --------
    def start_job(
        self,
        job_id: str | None = None,
        job_meta: dict[str, Any] | None = None,
    ) -> str:
        if job_id is None:
            job_id = uuid.uuid4().hex[:12]
        _current_job.set(job_id)
        self._job_start_ts[job_id] = time.perf_counter()
        self._write(job_id, {"type": "job_start", "job": job_meta or {}})
        return job_id

    def end_job(self, status: str = "ok", extra: dict[str, Any] | None = None) -> None:
        job_id = _current_job.get() or "unknown"
        dur = None
        if job_id in self._job_start_ts:
            dur = (time.perf_counter() - self._job_start_ts.pop(job_id)) * 1000.0
        self._write(
            job_id,
            {"type": "job_end", "status": status, "duration_ms": dur, "extra": extra or {}},
        )

    def llm_call(
        self,
        model: str,
        tokens_in: int,
        tokens_out: int,
        meta: dict[str, Any] | None = None,
    ) -> None:
        job_id = _current_job.get() or "unknown"
        self._write(
            job_id,
            {
                "type": "llm_call",
                "model": model,
                "tokens_in": tokens_in,
                "tokens_out": tokens_out,
                "meta": meta or {},
            },
        )

    def reward(self, value: float, reason: str = "", meta: dict[str, Any] | None = None) -> None:
        job_id = _current_job.get() or "unknown"
        self._write(
            job_id,
            {"type": "reward", "value": value, "reason": reason, "meta": meta or {}},
        )

    def log_event(self, event_type: str, payload: dict[str, Any] | None = None) -> None:
        """Logs a generic, structured event."""
        job_id = _current_job.get() or "unknown"
        self._write(
            job_id,
            {
                "type": "generic_event",
                "event_type": event_type,
                "payload": payload or {},
            },
        )

    def tool_event(
        self,
        phase: str,
        name: str,
        args: Any = None,
        result: Any = None,
        ok: bool | None = None,
        err: str | None = None,
        extra: dict[str, Any] | None = None,
        started_ms: float | None = None,
    ) -> None:
        job_id = _current_job.get() or "unknown"
        payload: dict[str, Any] = {
            "type": "tool_" + phase,
            "name": name,
            "ok": ok,
            "err": err,
            "extra": extra or {},
        }
        if started_ms is not None:
            payload["duration_ms"] = (time.perf_counter() - started_ms) * 1000.0
        if self.redact:
            if args is not None:
                payload["args"] = {"_redacted": True}
            if result is not None:
                payload["result"] = {"_redacted": True}
        else:
            if args is not None:
                payload["args"] = _redact(args)
            if result is not None:
                payload["result"] = _redact(result)
        self._write(job_id, payload)

    def graph_write(
        self,
        nodes: int = 0,
        rels: int = 0,
        labels: dict[str, int] | None = None,
    ) -> None:
        job_id = _current_job.get() or "unknown"
        self._write(
            job_id,
            {"type": "graph_write", "nodes": nodes, "rels": rels, "labels": labels or {}},
        )


telemetry = Telemetry.from_env()


# --------------- Context manager for jobs ---------------
class with_job_context:
    def __init__(self, job_id: str | None = None, job_meta: dict[str, Any] | None = None):
        self.job_id = job_id
        self.job_meta = job_meta or {}
        self._token = None

    def __enter__(self):
        jid = telemetry.start_job(self.job_id, self.job_meta)
        self.job_id = jid
        return jid

    def __exit__(self, exc_type, exc, tb):
        status = "ok" if exc is None else "error"
        extra = {"exc": repr(exc)} if exc else None
        telemetry.end_job(status=status, extra=extra)
        return False


# --------------- Decorator for tools ---------------

def track_tool(name: str | None = None) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """Wrap a sync or async tool to emit start/end events and register it for discovery."""

    def _decorator(fn: Callable[..., Any]) -> Callable[..., Any]:
        tool_name = name or getattr(fn, "__name__", "tool")

        # +++ FIX: Register the original, unwrapped function for introspection +++
        _TOOL_REGISTRY[tool_name] = fn

        if inspect.iscoroutinefunction(fn):
            @wraps(fn) # Use wraps to preserve function metadata
            async def _aw(*args, **kwargs):
                t0 = time.perf_counter()
                telemetry.tool_event(
                    "start",
                    tool_name,
                    args={"argc": len(args), "keys": list(kwargs.keys())},
                )
                try:
                    res = await fn(*args, **kwargs)
                    telemetry.tool_event("end", tool_name, result=res, ok=True, started_ms=t0)
                    return res
                except Exception as e:
                    telemetry.tool_event("end", tool_name, ok=False, err=repr(e), started_ms=t0)
                    raise
            return _aw
        else:
            @wraps(fn) # Use wraps for sync functions too
            def _sw(*args, **kwargs):
                t0 = time.perf_counter()
                telemetry.tool_event(
                    "start",
                    tool_name,
                    args={"argc": len(args), "keys": list(kwargs.keys())},
                )
                try:
                    res = fn(*args, **kwargs)
                    telemetry.tool_event("end", tool_name, result=res, ok=True, started_ms=t0)
                    return res
                except Exception as e:
                    telemetry.tool_event("end", tool_name, ok=False, err=repr(e), started_ms=t0)
                    raise
            return _sw
    return _decorator
# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\archive\pareto.py =====
from __future__ import annotations

import json
from pathlib import Path

ARCHIVE = Path("/app/_simula/archive/pareto.jsonl")
ARCHIVE.parent.mkdir(parents=True, exist_ok=True)


def _write_jsonl(obj: dict):
    with open(ARCHIVE, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")


def _read_jsonl() -> list[dict]:
    if not ARCHIVE.exists():
        return []
    return [json.loads(l) for l in ARCHIVE.read_text(encoding="utf-8").splitlines() if l.strip()]


def _dominates(a: dict, b: dict) -> bool:
    # maximize: tests_ok, static, coverage, contracts; minimize: diff_size
    return (
        (a["tests_ok"] >= b["tests_ok"])
        and (a["static"] >= b["static"])
        and (a["coverage"] >= b["coverage"])
        and (a["contracts"] >= b["contracts"])
        and (a["diff_size"] <= b["diff_size"])
        and (
            (a["tests_ok"] > b["tests_ok"])
            or (a["static"] > b["static"])
            or (a["coverage"] > b["coverage"])
            or (a["contracts"] > b["contracts"])
            or (a["diff_size"] < b["diff_size"])
        )
    )


def add_candidate(record: dict):
    """
    record = {
      "path": str, "diff": str, "tests_ok": int(0/1),
      "static": float, "coverage": float, "contracts": float, "diff_size": int,
      "notes": str
    }
    """
    _write_jsonl(record)


def top_k_similar(path: str, k: int = 3) -> list[dict]:
    """Return best Pareto-ish items for this path."""
    rows = [r for r in _read_jsonl() if r.get("path") == path]
    if not rows:
        return []
    # Fast Pareto filter
    pareto = []
    for r in rows:
        if any(_dominates(o, r) for o in rows):  # dominated
            continue
        pareto.append(r)
    # sort by (tests_ok desc, static desc, coverage desc, -diff_size)
    pareto.sort(
        key=lambda r: (r["tests_ok"], r["static"], r["coverage"], -r["diff_size"]),
        reverse=True,
    )
    return pareto[:k]

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\cache\patch_cache.py =====
# systems/simula/code_sim/cache/patch_cache.py
from __future__ import annotations

import hashlib
import json
import os
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any

_lock = threading.Lock()


def _repo_root() -> Path:
    try:
        from systems.simula.config import settings  # type: ignore

        root = getattr(settings, "repo_root", None)
        if root:
            return Path(root).resolve()
    except Exception:
        pass
    for env in ("SIMULA_WORKSPACE_ROOT", "SIMULA_REPO_ROOT", "PROJECT_ROOT"):
        p = os.getenv(env)
        if p:
            return Path(p).resolve()
    return Path(".").resolve()


def _cache_path() -> Path:
    root = _repo_root()
    p = root / ".simula" / "cache" / "hygiene.json"
    p.parent.mkdir(parents=True, exist_ok=True)
    return p


def _load() -> dict[str, Any]:
    p = _cache_path()
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _save(d: dict[str, Any]) -> None:
    p = _cache_path()
    p.write_text(json.dumps(d, indent=2), encoding="utf-8")


def _key(diff_text: str) -> str:
    h = hashlib.sha256()
    h.update(diff_text.encode("utf-8", errors="ignore"))
    return h.hexdigest()


@dataclass
class CacheEntry:
    static_ok: bool
    tests_ok: bool
    delta_cov_pct: float
    payload: dict[str, Any]


def get(diff_text: str) -> CacheEntry | None:
    k = _key(diff_text or "")
    with _lock:
        store = _load()
        rec = store.get(k)
        if not rec:
            return None
        try:
            return CacheEntry(
                static_ok=bool(rec.get("static_ok")),
                tests_ok=bool(rec.get("tests_ok")),
                delta_cov_pct=float(rec.get("delta_cov_pct", 0.0)),
                payload=dict(rec.get("payload") or {}),  # repo_rev lives here if present
            )
        except Exception:
            return None


def put(
    diff_text: str,
    *,
    static_ok: bool,
    tests_ok: bool,
    delta_cov_pct: float,
    payload: dict[str, Any],
) -> None:
    k = _key(diff_text or "")
    entry = {
        "static_ok": bool(static_ok),
        "tests_ok": bool(tests_ok),
        "delta_cov_pct": float(delta_cov_pct),
        "payload": payload or {},
    }
    with _lock:
        store = _load()
        store[k] = entry
        _save(store)

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\diagnostics\error_parser.py =====
# systems/simula/code_sim/diagnostics/error_parser.py
from __future__ import annotations

import re
from dataclasses import dataclass


@dataclass
class Failure:
    file: str
    line: int
    test: str | None
    errtype: str | None
    message: str | None


_TEST_LINE = re.compile(r"^(.+?):(\d+): (?:in )?(.+)$")
_FAIL_HEADER = re.compile(r"^=+ FAILURES =+$|^_+ (.+?) _+$")
_ERR_TYPE = re.compile(r"^E\s+([A-Za-z_][A-Za-z0-9_\.]*):\s*(.*)$")
_STACK_PATH = re.compile(r"^(.+?):(\d+): in (.+)$")


def parse_pytest_output(stdout: str) -> list[Failure]:
    """
    Extract failing test locations from pytest output. Robust to -q and verbose formats.
    """
    if not stdout:
        return []
    lines = stdout.splitlines()
    failures: list[Failure] = []
    cur_test: str | None = None
    cur_errtype: str | None = None
    cur_msg: str | None = None
    cur_file: str | None = None
    cur_line: int | None = None

    def _flush():
        nonlocal cur_test, cur_errtype, cur_msg, cur_file, cur_line
        if cur_file and cur_line:
            failures.append(Failure(cur_file, int(cur_line), cur_test, cur_errtype, cur_msg))
        cur_test = cur_errtype = cur_msg = cur_file = None
        cur_line = None

    for i, ln in enumerate(lines):
        if _FAIL_HEADER.match(ln):
            _flush()
            cur_test = None
            continue
        m = _STACK_PATH.match(ln)
        if m:
            cur_file, cur_line, _fn = m.group(1), int(m.group(2)), m.group(3)
            continue
        m = _TEST_LINE.match(ln)
        if m and not cur_file:
            cur_file, cur_line, cur_test = m.group(1), int(m.group(2)), m.group(3)
            continue
        m = _ERR_TYPE.match(ln)
        if m:
            cur_errtype, cur_msg = m.group(1), m.group(2)
            # flush at the end of a block or if next failure begins
            _flush()
    _flush()
    # Deduplicate by (file,line)
    seen = set()
    uniq: list[Failure] = []
    for f in failures:
        key = (f.file, f.line)
        if key in seen:
            continue
        seen.add(key)
        uniq.append(f)
    return uniq

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\diff\minimize.py =====
# systems/simula/code_sim/diff/minimize.py  (whitespace hunk minimizer)
from __future__ import annotations

import re
from collections.abc import Iterable

_HUNK = re.compile(r"(?ms)^diff --git a/.+?$\n(?:.+?\n)*?(?=(?:^diff --git a/)|\Z)")


def _is_whitespace_only(block: str) -> bool:
    plus = [l for l in block.splitlines() if l.startswith("+") and not l.startswith("+++")]
    minus = [l for l in block.splitlines() if l.startswith("-") and not l.startswith("---")]

    def _strip_payload(ls: Iterable[str]) -> str:
        return "".join(re.sub(r"\s+", "", l[1:]) for l in ls)

    return _strip_payload(plus) == _strip_payload(minus)


def drop_whitespace_only_hunks(diff_text: str) -> str:
    blocks = _HUNK.findall(diff_text or "")
    keep = [b for b in blocks if not _is_whitespace_only(b)]
    return "".join(keep) if keep else diff_text

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\__init__.py =====
# systems/simula/code_sim/evaluators/__init__.py
# --- PROJECT SENTINEL UPGRADE ---
from __future__ import annotations

from dataclasses import asdict, dataclass
from typing import Any, Dict

from . import contracts as _contracts
from . import perf as _perf
from . import runtime as _runtime
from . import static as _static
from . import tests as _tests


@dataclass
class EvalResult:
    """
    Canonical evaluator aggregate for the verification gauntlet.
    All scores are normalized to [0,1]. hard_gates_ok determines commit eligibility.
    """

    hard_gates_ok: bool
    raw: dict[str, Any]

    def as_dict(self) -> dict:
        """Returns the evaluation result as a dictionary."""
        return asdict(self)

    # --- Convenience properties for easy access to key metrics ---
    @property
    def unit_pass_ratio(self) -> float:
        return float(self.raw.get("tests", {}).get("unit", {}).get("ratio", 0.0))

    @property
    def static_score(self) -> float:
        s = self.raw.get("static", {})
        parts = [1.0 if s.get(k) else 0.0 for k in ["ruff_ok", "mypy_ok"]]
        return sum(parts) / len(parts) if parts else 0.0

    @property
    def security_score(self) -> float:
        return 1.0 if self.raw.get("static", {}).get("bandit_ok") else 0.0

    @property
    def contracts_score(self) -> float:
        c = self.raw.get("contracts", {})
        parts = [1.0 if c.get(k) else 0.0 for k in ["exports_ok", "registry_ok"]]
        return sum(parts) / len(parts) if parts else 0.0

    def summary(self) -> dict[str, Any]:
        """Provides a clean, flat summary of the evaluation for logging and observation."""
        return {
            "hard_gates_ok": self.hard_gates_ok,
            "unit_pass_ratio": self.unit_pass_ratio,
            "static_score": self.static_score,
            "security_score": self.security_score,
            "contracts_score": self.contracts_score,
            "raw_outputs": {
                "tests": self.raw.get("tests", {}).get("stdout", "N/A")[-1000:],
                "static": self.raw.get("static", {}).get("outputs", {}),
            },
        }


def run_evaluator_suite(objective: dict[str, Any], sandbox_session) -> EvalResult:
    """
    Executes the full evaluator ensemble inside the provided sandbox session.
    """
    tests = _tests.run(objective, sandbox_session)
    static = _static.run(objective, sandbox_session)
    contracts = _contracts.run(objective, sandbox_session)
    runtime = _runtime.run(objective, sandbox_session)
    perf = _perf.run(objective, sandbox_session)

    # Define the conditions for passing the hard gates
    unit_ok = bool(tests.get("ok"))
    contracts_ok = bool(contracts.get("exports_ok"))
    security_ok = bool(static.get("bandit_ok"))
    runtime_ok = bool(runtime.get("start_ok"))
    hard_ok = all([unit_ok, contracts_ok, security_ok, runtime_ok])

    raw = {
        "tests": tests,
        "static": static,
        "contracts": contracts,
        "runtime": runtime,
        "perf": perf,
    }
    return EvalResult(hard_gates_ok=hard_ok, raw=raw)

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\contracts.py =====
# simula/code_sim/evaluators/contracts.py
"""
Contracts evaluator: exports present, registry updated, docs touched.

Objective keys used
-------------------
objective.acceptance.contracts.must_export: ["path.py::func(a:int)->R", ...]
objective.acceptance.contracts.must_register: ["registry: contains tool 'NAME'", ...]
objective.acceptance.docs.files_must_change: ["docs/...", ...]

Public API
----------
run(step, sandbox_session) -> dict
    {
      "exports_ok": bool,
      "registry_ok": bool,
      "docs_ok": bool,
      "details": {"exports": [...], "registry": [...], "docs_required": [...]}
    }
"""

from __future__ import annotations

import re
from pathlib import Path


def _read(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return ""


def _approx_sig_present(src: str, func_sig: str) -> bool:
    # Compare by name + arg count (ignore types/whitespace)
    head = func_sig.strip()
    name = head.split("(", 1)[0].strip()
    try:
        params = head.split("(", 1)[1].rsplit(")", 1)[0]
    except Exception:
        return False
    param_names = [p.split(":")[0].split("=")[0].strip() for p in params.split(",") if p.strip()]
    pat = re.compile(rf"def\s+{re.escape(name)}\s*\((.*?)\)\s*:", re.DOTALL)
    for m in pat.finditer(src):
        got = [a.split("=")[0].split(":")[0].strip() for a in m.group(1).split(",") if a.strip()]
        if len(got) == len(param_names):
            return True
    return False


def _contains_tool_registration(src: str, tool_name: str) -> bool:
    return bool(re.search(rf"(register|add)_tool\([^)]*{re.escape(tool_name)}[^)]*\)", src))


def _git_changed(sess) -> list[str]:
    rc, out = sess.run(["git", "diff", "--name-only"], timeout=300)
    return out.strip().splitlines() if rc == 0 else []


def run(objective: dict, sandbox_session) -> dict[str, object]:
    """
    FIX: Changed function signature from 'step' to 'objective' to match the caller.
    """
    acc = objective.get("acceptance", {})
    exports = acc.get("contracts", {}).get("must_export", []) or []
    registers = acc.get("contracts", {}).get("must_register", []) or []
    docs_required = acc.get("docs", {}).get("files_must_change", []) or []

    exports_ok = True
    export_details: list[str] = []
    for spec in exports:
        try:
            file_part, sig = spec.split("::", 1)
        except ValueError:
            exports_ok = False
            export_details.append(f"BAD_SPEC {spec!r}")
            continue
        # This path resolution might need adjustment if sandbox root differs from repo root
        src = _read(Path(file_part))
        present = _approx_sig_present(src, sig)
        export_details.append(f"{'OK' if present else 'MISS'} {file_part} :: {sig}")
        exports_ok &= present

    registry_ok = True
    registry_details: list[str] = []
    for item in registers:
        m = re.search(r"tool\s+'([^']+)'", item)
        if not m:
            registry_ok = False
            registry_details.append(f"BAD_SPEC {item!r}")
            continue
        tool = m.group(1)
        reg_path = Path("systems/synk/core/tools/registry.py")
        src = _read(reg_path)
        ok = _contains_tool_registration(src, tool) or (tool in src)
        registry_details.append(f"{'OK' if ok else 'MISS'} registry contains {tool}")
        registry_ok &= ok

    docs_ok = True
    if docs_required:
        changed = set(_git_changed(sandbox_session))
        need = set(docs_required)
        docs_ok = need.issubset(changed)

    return {
        "exports_ok": exports_ok,
        "registry_ok": registry_ok,
        "docs_ok": docs_ok,
        "details": {
            "exports": export_details,
            "registry": registry_details,
            "docs_required": docs_required,
        },
    }

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\coverage_delta.py =====
# systems/simula/code_sim/evaluators/coverage_delta.py
from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path

from core.utils.diff import changed_paths_from_unified_diff


@dataclass
class DeltaCoverage:
    changed_files: list[str]
    changed_lines: dict[str, set[int]]
    covered_changed_lines: dict[str, set[int]]
    pct_changed_covered: float

    def summary(self) -> dict[str, object]:
        total_changed = sum(len(v) for v in self.changed_lines.values()) or 0
        total_cov = sum(len(v) for v in self.covered_changed_lines.values()) or 0
        pct = 100.0 * (total_cov / total_changed) if total_changed else 0.0
        return {
            "changed_files": self.changed_files,
            "changed_lines_total": total_changed,
            "covered_changed_lines_total": total_cov,
            "pct_changed_covered": round(pct, 2),
        }


_HUNK_HEADER = re.compile(r"^@@ -\d+(?:,\d+)? \+(\d+)(?:,(\d+))? @@", re.M)


def _changed_lines_from_unified_diff(diff: str) -> dict[str, set[int]]:
    """
    Extract changed line numbers per file from a unified diff (for the 'b/' side).
    """
    changed: dict[str, set[int]] = {}
    current_file: str | None = None
    for line in diff.splitlines():
        if line.startswith("+++ b/"):
            current_file = line[6:].strip()
            changed.setdefault(current_file, set())
            continue
        if current_file is None:
            # Wait for file header first
            continue
        m = _HUNK_HEADER.match(line)
        if m:
            start = int(m.group(1))
            int(m.group(2) or "1")
            cur = start
            continue  # move to next lines; adds come next
        if line.startswith("+") and not line.startswith("+++"):
            # added line in new file; record then increment counter
            try:
                changed[current_file].add(cur)
                cur += 1
            except Exception:
                # cur not initialized yet (malformed diff) — ignore
                pass
        elif line.startswith("-") and not line.startswith("---"):
            # removed line in old file; new-file line number does not advance
            pass
        else:
            # context line advances both sides
            try:
                cur += 1
            except Exception:
                pass
    return changed


def load_coverage_json(path: str = "coverage.json") -> dict[str, object]:
    p = Path(path)
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}


def compute_delta_coverage(
    diff_text: str,
    coverage_json_path: str = "coverage.json",
) -> DeltaCoverage:
    """
    Compute coverage over *changed lines only* using coverage.py JSON.
    """
    changed_files = [p for p in changed_paths_from_unified_diff(diff_text) if p.endswith(".py")]
    changed_lines = _changed_lines_from_unified_diff(diff_text)

    cov = load_coverage_json(coverage_json_path)
    files = (cov.get("files") or {}) if isinstance(cov, dict) else {}
    covered_changed: dict[str, set[int]] = {f: set() for f in changed_files}

    for f in changed_files:
        rec = files.get(str(Path(f).resolve()))
        if not rec:
            # coverage.py sometimes stores paths as relative; try both
            rec = files.get(f)
        if not rec:
            continue
        executed = set(rec.get("executed_lines") or [])
        for ln in changed_lines.get(f, set()):
            if ln in executed:
                covered_changed.setdefault(f, set()).add(ln)

    total_changed = sum(len(v) for v in changed_lines.values()) or 0
    total_cov = sum(len(v) for v in covered_changed.values()) or 0
    pct = (100.0 * total_cov / total_changed) if total_changed else 0.0

    return DeltaCoverage(
        changed_files=changed_files,
        changed_lines=changed_lines,
        covered_changed_lines=covered_changed,
        pct_changed_covered=pct,
    )

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\impact.py =====
# systems/simula/code_sim/evaluators/impact.py
from __future__ import annotations

import ast
from collections.abc import Iterable
from dataclasses import dataclass
from pathlib import Path

from core.utils.diff import changed_paths_from_unified_diff


@dataclass
class ImpactReport:
    changed: list[str]
    candidate_tests: list[str]
    k_expr: str  # pytest -k expression focusing on impacted scopes


def _iter_tests(root: Path) -> Iterable[Path]:
    for p in root.rglob("test_*.py"):
        yield p
    for p in root.rglob("*_test.py"):
        yield p


def _module_name_from_path(p: Path) -> str:
    # turn "pkg/foo/bar.py" → "pkg.foo.bar"
    rel = p.as_posix().rstrip(".py")
    if rel.endswith(".py"):
        rel = rel[:-3]
    return rel.replace("/", ".").lstrip(".")


def _collect_imports(p: Path) -> set[str]:
    out: set[str] = set()
    try:
        tree = ast.parse(p.read_text(encoding="utf-8"))
    except Exception:
        return out
    for n in ast.walk(tree):
        if isinstance(n, ast.Import):
            for a in n.names:
                out.add(a.name)
        elif isinstance(n, ast.ImportFrom):
            if n.module:
                out.add(n.module)
    return out


def _likely_test_for_module(mod_name: str, tests_root: Path) -> list[str]:
    # Heuristics:
    # 1) direct file mapping: tests/test_<leaf>.py
    # 2) any test file importing the module or its parent package
    candidates: set[str] = set()

    leaf = mod_name.split(".")[-1]
    for pattern in [f"test_{leaf}.py", f"{leaf}_test.py"]:
        for p in tests_root.rglob(pattern):
            candidates.add(p.as_posix())

    # import-based matching
    wanted = {mod_name}
    # include parent packages (pkg.foo.bar -> pkg.foo, pkg)
    parts = mod_name.split(".")
    for i in range(len(parts) - 1, 0, -1):
        wanted.add(".".join(parts[:i]))

    for p in _iter_tests(tests_root):
        imps = _collect_imports(p)
        if any(w in imps for w in wanted):
            candidates.add(p.as_posix())

    return sorted(candidates)


def _nodeids_from_files(files: list[str]) -> list[str]:
    # Pytest nodeids can just be file paths; -k uses substrings, so we return base filenames too
    ids: set[str] = set()
    for f in files:
        ids.add(f)
        ids.add(Path(f).stem)  # helps -k match
    return sorted(ids)


def compute_impact(diff_text: str, *, workspace_root: str = ".") -> ImpactReport:
    """
    Map a unified diff to an impact-focused test selection.
    Returns test file candidates and a `-k` expression for pytest.
    """
    changed = [p for p in changed_paths_from_unified_diff(diff_text) if p.endswith(".py")]
    if not changed:
        return ImpactReport(changed=[], candidate_tests=[], k_expr="")

    root = Path(workspace_root).resolve()
    tests_root = root / "tests"
    mods = []
    for c in changed:
        p = (root / c).resolve()
        if not p.exists():
            # infer module name from path anyway
            mods.append(_module_name_from_path(Path(c)))
        else:
            mods.append(_module_name_from_path(p.relative_to(root)))

    test_files: set[str] = set()
    if tests_root.exists():
        for m in mods:
            for t in _likely_test_for_module(m, tests_root):
                test_files.add(t)

    # fallbacks: if nothing matched, run whole tests dir
    if not test_files and tests_root.exists():
        for p in _iter_tests(tests_root):
            test_files.add(p.as_posix())

    nodeids = _nodeids_from_files(sorted(test_files))
    # Pytest -k expression prefers OR of stems to keep it short
    # cap to avoid CLI explosion
    stems = [Path(n).stem for n in nodeids if n.endswith(".py")]
    stems = stems[:24] if len(stems) > 24 else stems
    k_expr = " or ".join(sorted(set(stems)))

    return ImpactReport(changed=changed, candidate_tests=sorted(test_files), k_expr=k_expr)

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\perf.py =====
# simula/code_sim/evaluators/perf.py
"""
Performance evaluator: enforce per-objective pytest runtime budgets.

Objective keys used
-------------------
objective.acceptance.perf.pytest_duration_seconds: "<=30"  (string or number)

Public API
----------
run(step, sandbox_session) -> dict
    {
      "duration_s": float,
      "rc": int,
      "score": float,   # 1.0 if within budget; linearly decays below 0
      "stdout": str,
      "selected": ["tests/..."],
    }
"""

from __future__ import annotations

import glob
import time
from collections.abc import Sequence
from pathlib import Path
from typing import Any

# ------------------------------- helpers ------------------------------------


def _is_mapping(x) -> bool:
    return isinstance(x, dict)


def _get(obj: Any, key: str, default=None):
    """Dict-or-attr getter."""
    if _is_mapping(obj):
        return obj.get(key, default)
    return getattr(obj, key, default)


def _get_path(obj: Any, path: Sequence[str], default=None):
    """Nested getter by path: ['acceptance','unit_tests','patterns']"""
    cur = obj
    for i, k in enumerate(path):
        if cur is None:
            return default
        cur = _get(cur, k, None if i < len(path) - 1 else default)
    return cur if cur is not None else default


def _extract_tests(step_or_objective: Any) -> list[str]:
    """
    Resolution order (works for dicts or objects):
      1) step.tests
      2) (step.objective or objective).acceptance.tests
      3) (..).acceptance.unit_tests.patterns
      4) (..).acceptance.unit_tests.paths
      -> default ['tests'] if nothing provided
    """
    tests = _get(step_or_objective, "tests", None)
    if not tests:
        carrier = _get(step_or_objective, "objective", None) or step_or_objective
        acc = _get(carrier, "acceptance", {}) or {}
        tests = (
            _get(acc, "tests", None)
            or _get_path(acc, ["unit_tests", "patterns"], None)
            or _get_path(acc, ["unit_tests", "paths"], None)
        )
    if isinstance(tests, str | Path | bytes):
        if isinstance(tests, bytes):
            return [tests.decode(errors="replace")]
        return [str(tests)]
    if not tests:
        # Sensible fallback if not specified anywhere
        return ["tests"]
    return [str(t) for t in tests]


def _expand_tests(patterns: list[str]) -> list[str]:
    """
    Expand globs so pytest has concrete inputs. If a pattern does not match,
    keep the token (pytest can still collect from a directory name).
    """
    out: list[str] = []
    for pat in patterns:
        matches = sorted(glob.glob(pat, recursive=True))
        if matches:
            out.extend(matches)
        else:
            out.append(pat)
    # Deduplicate while preserving order
    seen = set()
    uniq: list[str] = []
    for p in out:
        if p not in seen:
            uniq.append(p)
            seen.add(p)
    return uniq or ["tests"]


def _budget_seconds(objective: Any) -> float:
    """
    FIX: Parameter renamed to 'objective' for clarity.
    Reads acceptance.perf.pytest_duration_seconds.
    """
    perf = _get_path(objective, ["acceptance", "perf"], {}) or {}
    raw = _get(perf, "pytest_duration_seconds", "<=30")
    if isinstance(raw, int | float):
        return float(raw)
    try:
        s = str(raw).strip().lstrip("<=")
        return float(s)
    except Exception:
        return 30.0


def run(objective: dict, sandbox_session) -> dict[str, Any]:
    """
    FIX: Changed function signature from 'step' to 'objective' to match the caller.
    """
    tests = _expand_tests(_extract_tests(objective))
    budget = _budget_seconds(objective)

    cmd = ["pytest", "-q", "--disable-warnings", "--maxfail=1", *tests]

    t0 = time.time()
    rc, out = sandbox_session.run(cmd, timeout=max(60, int(budget * 5)))
    dur = time.time() - t0

    out_str = out.decode("utf-8", errors="replace") if isinstance(out, bytes) else str(out)
    score = 1.0 if dur <= budget else max(0.0, 1.0 - (dur - budget) / max(budget, 1.0))

    return {
        "duration_s": dur,
        "rc": int(rc),
        "score": round(float(score), 4),
        "stdout": out_str[-10000:],
        "selected": tests,
    }

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\risk.py =====
# systems/simula/code_sim/evaluators/risk.py
from __future__ import annotations

from dataclasses import dataclass


@dataclass
class HygieneStatus:
    static_ok: bool
    tests_ok: bool
    changed_count: int


def risk_score(hygiene: HygieneStatus, *, prior_bug_rate: float = 0.08) -> float:
    """
    Heuristic risk score in [0,1], higher means riskier.
    - penalize when static/tests fail
    - more changed files → higher risk
    - combine with prior bug rate
    """
    score = prior_bug_rate
    if not hygiene.static_ok:
        score += 0.3
    if not hygiene.tests_ok:
        score += 0.4
    score += min(0.3, hygiene.changed_count * 0.03)
    return max(0.0, min(1.0, score))


def summarize(hygiene_status: dict[str, object]) -> dict[str, object]:
    hs = HygieneStatus(
        static_ok=(hygiene_status.get("static") == "success"),
        tests_ok=(hygiene_status.get("tests") == "success"),
        changed_count=int(hygiene_status.get("changed_count") or 1),
    )
    return {"risk": risk_score(hs), "hygiene": hs.__dict__}

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\runtime.py =====
# systems/simula/code_sim/evaluators/runtime.py
from __future__ import annotations


def run(objective: dict, sandbox_session) -> dict:
    """
    FIX: Changed function signature from 'step' to 'objective' to match the caller.
    """
    runtime = objective.get("runtime", {})
    imports: list[str] = runtime.get("import_modules", ["systems", "systems.synk", "systems.axon"])
    cmds: list[list[str]] = runtime.get("commands", [])

    import_ok = True
    import_logs: list[str] = []
    for mod in imports:
        rc, out = sandbox_session.run(
            ["python", "-c", f"import importlib; importlib.import_module('{mod}'); print('OK')"],
            timeout=120,
        )
        ok = (rc == 0) and ("OK" in out)
        import_ok &= ok
        import_logs.append(f"{'OK' if ok else 'FAIL'} import {mod}")

    cmd_ok = True
    cmd_logs: list[str] = []
    for cmd in cmds:
        rc, out = sandbox_session.run(cmd, timeout=300)
        ok = rc == 0
        cmd_ok &= ok
        cmd_logs.append(f"{'OK' if ok else 'FAIL'} {' '.join(cmd)}")

    return {
        "start_ok": import_ok and cmd_ok,
        "health_ok": import_ok,
        "details": {"imports": import_logs, "commands": cmd_logs},
    }

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\security.py =====
# systems/simula/code_sim/evaluators/security.py
from __future__ import annotations

import re
from dataclasses import dataclass


@dataclass
class SecurityGateResult:
    ok: bool
    findings: list[str]

    def summary(self) -> dict[str, object]:
        return {"ok": self.ok, "findings": self.findings}


SECRET_RX = re.compile(
    r'(api[_-]?key|secret|token)\s*[:=]\s*[\'"][A-Za-z0-9_\-]{16,}[\'"]|Bearer\s+[A-Za-z0-9._\-]{20,}',
    re.I,
)
CREDENTIAL_FILE_HINT = re.compile(r"(id_rsa|aws_credentials|netrc|\.pypirc|\.npmrc)", re.I)
LICENSE_BLOCKLIST = {"AGPL-3.0", "SSPL-1.0"}  # extend per org policy


def scan_diff_for_secrets(diff_text: str) -> SecurityGateResult:
    findings: list[str] = []
    for line in diff_text.splitlines():
        if line.startswith("+") and SECRET_RX.search(line):
            findings.append(f"Potential secret in line: {line[:200]}")
    return SecurityGateResult(ok=(len(findings) == 0), findings=findings)


def scan_diff_for_disallowed_licenses(diff_text: str) -> SecurityGateResult:
    findings: list[str] = []
    for lic in LICENSE_BLOCKLIST:
        if lic in diff_text:
            findings.append(f"Disallowed license reference detected: {lic}")
    return SecurityGateResult(ok=(len(findings) == 0), findings=findings)


def scan_diff_for_credential_files(diff_text: str) -> SecurityGateResult:
    findings: list[str] = []
    for line in diff_text.splitlines():
        if line.startswith("+++ b/") and CREDENTIAL_FILE_HINT.search(line):
            findings.append(f"Suspicious file added/modified: {line[6:]}")
    return SecurityGateResult(ok=(len(findings) == 0), findings=findings)

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\spec_miner.py =====
# systems/simula/code_sim/evaluators/spec_miner.py
from __future__ import annotations

from dataclasses import dataclass

from systems.simula.code_sim.diagnostics.error_parser import parse_pytest_output


@dataclass
class AcceptanceHint:
    file: str
    line: int
    test: str | None
    errtype: str | None
    message: str | None
    suggestion: str


def _suggestion(errtype: str | None, msg: str | None) -> str:
    t = (errtype or "").lower()
    (msg or "").lower()
    if "typeerror" in t:
        return "Add input-type guard or coerce types; update acceptance spec for type contracts."
    if "assertionerror" in t:
        return "Document invariant as explicit acceptance; adjust function behavior or tests accordingly."
    if "keyerror" in t or "indexerror" in t:
        return "Guard missing keys/indices or return safe default."
    return "Add acceptance clause for this edge case and implement guard."


def derive_acceptance(proposal_tests_stdout: str) -> dict[str, list[dict[str, str | int]]]:
    fails = parse_pytest_output(proposal_tests_stdout)
    hints: list[AcceptanceHint] = []
    for f in fails:
        hints.append(
            AcceptanceHint(
                file=f.file,
                line=f.line,
                test=f.test,
                errtype=f.errtype,
                message=f.message,
                suggestion=_suggestion(f.errtype, f.message),
            ),
        )
    return {
        "acceptance_hints": [
            {
                "file": h.file,
                "line": h.line,
                "test": h.test or "",
                "errtype": h.errtype or "",
                "message": h.message or "",
                "suggestion": h.suggestion,
            }
            for h in hints
        ],
    }

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\static.py =====
# simula/code_sim/evaluators/static.py
"""
Static suite: ruff (lint), mypy (types), bandit (security).

Public API
----------
run(objective, sandbox_session) -> dict
    {
      "ruff_ok": bool, "mypy_ok": bool, "bandit_ok": bool,
      "outputs": {"ruff": str, "mypy": str, "bandit": str}
    }
"""

from __future__ import annotations


def _run(sess, args, timeout):
    rc, out = sess.run(args, timeout=timeout)
    return (rc == 0), out


def run(objective, sandbox_session) -> dict:
    r_ok, r_out = _run(sandbox_session, ["ruff", "check", "."], timeout=1200)
    m_ok, m_out = _run(
        sandbox_session,
        ["mypy", "--hide-error-context", "--pretty", "."],
        timeout=1800,
    )
    b_ok, b_out = _run(sandbox_session, ["bandit", "-q", "-r", "."], timeout=1800)
    # Treat any High severity finding as a failure even if rc=0 (some bandit configs do that)
    if "SEVERITY: High" in b_out:
        b_ok = False 
    return {
        "ruff_ok": r_ok,
        "mypy_ok": m_ok,
        "bandit_ok": b_ok,
        "outputs": {"ruff": r_out[-10000:], "mypy": m_out[-10000:], "bandit": b_out[-10000:]},
    }
# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\evaluators\tests.py =====
# simula/code_sim/evaluators/tests.py
"""
Tests evaluator: discover + run unit/integration suites with structured output.

Public API
----------
run(step_or_objective, sandbox_session) -> dict
    {
      "ok": bool,                 # True iff all selected tests passed
      "unit": {"passed": int, "failed": int, "errors": int, "skipped": int, "ratio": float},
      "integration": {"..."}      # reserved (mirrors unit); may be empty
      "coverage_delta": float,    # heuristic bump if all pass
      "per_file_coverage": {str: float},
      "duration_s": float,
      "rc": int,
      "stdout": str,              # trimmed output
      "selected": ["tests/..."],  # what we actually ran
    }
"""

from __future__ import annotations

import glob
import os
import re
import time
import xml.etree.ElementTree as ET
from collections.abc import Sequence
from pathlib import Path
from typing import Any

COV_XML = Path("/app/coverage.xml")

# ------------------------------- helpers ------------------------------------


def _is_mapping(x) -> bool:
    return isinstance(x, dict)


def _get(obj: Any, key: str, default=None):
    """Dict-or-attr getter."""
    if _is_mapping(obj):
        return obj.get(key, default)
    return getattr(obj, key, default)


def _get_path(obj: Any, path: Sequence[str], default=None):
    """Nested dict-or-attr getter by path: ['acceptance','unit_tests','patterns']"""
    cur = obj
    for i, k in enumerate(path):
        if cur is None:
            return default
        cur = _get(cur, k, None if i < len(path) - 1 else default)
    return cur if cur is not None else default


def _extract_tests(objective: dict[str, Any]) -> list[str]:
    """
    Resolution order (works for dicts):
      1) objective.tests (if it's a step dict)
      2) objective.acceptance.tests
      3) objective.acceptance.unit_tests.patterns
      4) objective.acceptance.unit_tests.paths
    """
    tests = objective.get("tests") # Handles case where a step_dict is passed
    if not tests:
        acc = objective.get("acceptance", {}) or {}
        tests = (
            acc.get("tests")
            or _get_path(acc, ["unit_tests", "patterns"])
            or _get_path(acc, ["unit_tests", "paths"])
        )
    # normalize to list[str]
    if isinstance(tests, str | Path):
        tests = [str(tests)]
    elif not isinstance(tests, list):
        tests = list(tests) if tests else []
    return [str(t) for t in tests if t]


def _expand_test_selection(patterns: list[str]) -> list[str]:
    """
    Expand globs so pytest receives concrete paths. If nothing expands, keep the
    original token (pytest can still collect from a directory name).
    """
    selected: list[str] = []
    for pat in patterns:
        matches = sorted(glob.glob(pat, recursive=True))
        if matches:
            selected.extend(matches)
        else:
            selected.append(pat)

    # Sensible fallback if selection still empty
    if not selected:
        for candidate in ("tests", "test", "src/tests"):
            if os.path.exists(candidate):
                selected.append(candidate)
                break
        if not selected:
            selected = ["tests"]
    return selected


def _coverage_per_file() -> dict[str, float]:
    if not COV_XML.exists():
        return {}
    try:
        root = ET.fromstring(COV_XML.read_text(encoding="utf-8"))
        out: dict[str, float] = {}
        for cls in root.findall(".//class"):
            fname = cls.attrib.get("filename", "")
            lines = cls.findall("./lines/line")
            if not lines:
                continue
            total = len(lines)
            hit = sum(1 for l in lines if l.attrib.get("hits", "0") != "0")
            out[fname] = (hit / total) if total else 0.0
        return out
    except Exception:
        return {}


_SUMMARY = re.compile(
    r"(?:(?P<passed>\d+)\s+passed)|"
    r"(?:(?P<failed>\d+)\s+failed)|"
    r"(?:(?P<errors>\d+)\s+errors?)|"
    r"(?:(?P<skipped>\d+)\s+skipped)",
    re.IGNORECASE,
)


def _parse_counts(txt: str) -> dict[str, int]:
    d = {"passed": 0, "failed": 0, "errors": 0, "skipped": 0}
    for m in _SUMMARY.finditer(txt or ""):
        for k in d:
            v = m.group(k)
            if v:
                d[k] += int(v)
    return d


def _ratio(passed: int, total: int) -> float:
    return 1.0 if total == 0 else max(0.0, min(1.0, passed / total))


# --------------------------------- API --------------------------------------


def run(objective: dict[str, Any], sandbox_session) -> dict[str, Any]:
    """
    Execute pytest inside the provided sandbox session.
    - Accepts an `objective` or `step` dictionary.
    - Selects tests per resolution order above.
    - Produces coverage.xml and parses per-file coverage.
    """
    tests = _expand_test_selection(_extract_tests(objective))

    # Pytest invocation:
    # - quiet
    # - stop at first failure (fast signal for iter loops)
    # - disable warnings clutter
    # - coverage xml written to /app/coverage.xml (COV_XML)
    cmd = [
        "pytest",
        "-q",
        "--maxfail=1",
        "--disable-warnings",
        "--cov=.",
        f"--cov-report=xml:{COV_XML}",
        *tests,
    ]

    t0 = time.time()
    # NOTE: sandbox_session.run is assumed to be synchronous here.
    # If your sandbox API is async, wrap with anyio/run_sync or adjust call sites.
    rc, out = sandbox_session.run(cmd, timeout=1800)
    dur = time.time() - t0

    # Normalize output to str
    if isinstance(out, bytes | bytearray):
        try:
            out = out.decode("utf-8", errors="replace")
        except Exception:
            out = str(out)

    counts = _parse_counts(out)
    total = counts["passed"] + counts["failed"] + counts["errors"]
    ratio = _ratio(counts["passed"], total)
    ok = (rc == 0) and (ratio == 1.0)

    # Simple heuristic coverage bump if everything green and non-trivial
    cov_delta = 0.05 if ok and total > 0 else 0.0
    per_file = _coverage_per_file()

    return {
        "ok": ok,
        "unit": {**counts, "ratio": ratio},
        "integration": {},
        "coverage_delta": cov_delta,
        "per_file_coverage": per_file,
        "duration_s": dur,
        "rc": rc,
        "stdout": (out or "")[-20000:],  # trim to keep artifacts small
        "selected": tests,
    }

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\fuzz\hypo_driver.py =====
# systems/simula/code_sim/fuzz/hypo_driver.py
from __future__ import annotations

import os
import tempfile

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config

_TEMPLATE = """
import importlib, inspect, builtins, pytest
try:
    import hypothesis, hypothesis.strategies as st
except Exception:
    hypothesis = None

MOD_PATH = {mod_path!r}
FUNC_NAME = {func_name!r}

@pytest.mark.skipif(hypothesis is None, reason="hypothesis not installed")
def test_fuzz_smoke():
    m = importlib.import_module(MOD_PATH)
    fn = getattr(m, FUNC_NAME)
    sig = inspect.signature(fn)
    # Heuristic: support up to 2 positional args with common primitives
    @hypothesis.given(st.one_of(st.none(), st.text(), st.integers(), st.floats(allow_nan=False)),
                      st.one_of(st.none(), st.text(), st.integers(), st.floats(allow_nan=False)))
    def _prop(a, b):
        params = list(sig.parameters.values())
        args = []
        if len(params) >= 1 and params[0].kind in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
            args.append(a)
        if len(params) >= 2 and params[1].kind in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
            args.append(b)
        try:
            fn(*args[:len(params)])
        except Exception:
            # property: should not catastrophically fail for arbitrary inputs
            pytest.fail("fuzz-triggered exception")
    _prop()
"""


async def run_hypothesis_smoke(
    mod_path: str,
    func_name: str,
    *,
    timeout_sec: int = 600,
) -> tuple[bool, dict]:
    tf = tempfile.NamedTemporaryFile("w", delete=False, suffix="_fuzz_test.py")
    tf.write(_TEMPLATE.format(mod_path=mod_path, func_name=func_name))
    tf.flush()
    tf.close()
    async with DockerSandbox(seed_config()).session() as sess:
        cmd = ["bash", "-lc", f"pytest -q {tf.name} || true"]
        out = await sess._run_tool(cmd, timeout=timeout_sec)
        ok = out.get("returncode", 0) == 0 and "failed" not in (out.get("stdout") or "")
        try:
            os.unlink(tf.name)
        except Exception:
            pass
        return ok, out

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\impact\index.py =====
# systems/simula/code_sim/impact/index.py
from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path

from .py_callgraph import build_callgraph

_INDEX_PATH = Path(".simula/impact_index.json")


@dataclass
class ImpactIndex:
    callgraph: dict[str, list[str]]
    symbol_tests: dict[str, list[str]]


def load_index() -> ImpactIndex:
    if _INDEX_PATH.exists():
        try:
            d = json.loads(_INDEX_PATH.read_text(encoding="utf-8"))
            return ImpactIndex(
                callgraph=d.get("callgraph") or {},
                symbol_tests=d.get("symbol_tests") or {},
            )
        except Exception:
            pass
    return ImpactIndex(callgraph={}, symbol_tests={})


def save_index(ix: ImpactIndex) -> None:
    _INDEX_PATH.parent.mkdir(parents=True, exist_ok=True)
    _INDEX_PATH.write_text(
        json.dumps({"callgraph": ix.callgraph, "symbol_tests": ix.symbol_tests}, indent=2),
        encoding="utf-8",
    )


def update_callgraph(root: str = ".") -> ImpactIndex:
    ix = load_index()
    cg = build_callgraph(root)
    ix.callgraph = {k: sorted(list(v)) for k, v in cg.items()}
    save_index(ix)
    return ix


def record_symbol_tests(symbol: str, tests: list[str]) -> None:
    ix = load_index()
    st = set(ix.symbol_tests.get(symbol) or [])
    st.update(tests or [])
    ix.symbol_tests[symbol] = sorted(st)
    save_index(ix)


def k_expr_for_changed(paths: list[str]) -> str:
    ix = load_index()
    stems: set[str] = set()
    for p in paths:
        sym = Path(p).stem
        for t in ix.symbol_tests.get(sym, []):
            stems.add(Path(t).stem)
    return " or ".join(sorted(stems))[:256]

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\impact\py_callgraph.py =====
# systems/simula/code_sim/impact/py_callgraph.py
from __future__ import annotations

import ast
from pathlib import Path


def build_callgraph(root: str = ".") -> dict[str, set[str]]:
    """
    Approximate callgraph mapping function name -> set(callees) within the project.
    """
    cg: dict[str, set[str]] = {}
    files = [p for p in Path(root).rglob("**/*.py") if "/tests/" not in str(p)]
    for p in files:
        try:
            tree = ast.parse(p.read_text(encoding="utf-8", errors="ignore"))
        except Exception:
            continue
        funcs: list[str] = []
        for n in ast.walk(tree):
            if isinstance(n, ast.FunctionDef):
                funcs.append(n.name)
                cg.setdefault(n.name, set())
        for n in ast.walk(tree):
            if isinstance(n, ast.Call) and isinstance(n.func, ast.Name):
                for f in funcs:
                    # naive: attribute calls ignored
                    cg.setdefault(f, set())
                # link all funcs in this file to called name
                for f in funcs:
                    cg[f].add(n.func.id)
    return cg

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\impact\scope_map.py =====
# systems/simula/code_sim/impact/scope_map.py
from __future__ import annotations

import ast
from pathlib import Path


def map_symbols_to_tests(root: str = ".") -> dict[str, set[str]]:
    """
    Heuristic map: if a test file imports module X, we map X to that test file.
    """
    rootp = Path(root)
    mod_to_tests: dict[str, set[str]] = {}
    tests: list[Path] = []
    for p in rootp.rglob("tests/**/*.py"):
        tests.append(p)
    for p in rootp.rglob("**/*.py"):
        if "/tests/" in str(p.as_posix()):
            continue
        try:
            ast.parse(p.read_text(encoding="utf-8", errors="ignore"))
        except Exception:
            continue
        mod = p.as_posix().replace("/", ".")[:-3]
        mod_to_tests.setdefault(mod, set())
        for tp in tests:
            try:
                ttree = ast.parse(tp.read_text(encoding="utf-8", errors="ignore"))
            except Exception:
                continue
            for n in ast.walk(ttree):
                if (
                    isinstance(n, ast.ImportFrom)
                    and n.module
                    and n.module in (mod, mod.rsplit(".", 1)[0])
                ):
                    mod_to_tests[mod].add(tp.as_posix())
                if isinstance(n, ast.Import):
                    for nm in n.names:
                        if nm.name in (mod, mod.rsplit(".", 1)[0]):
                            mod_to_tests[mod].add(tp.as_posix())
    return mod_to_tests

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\mutators\__init__.py =====
from .ast_refactor import AstMutator

MUTATORS = {
    "scaffold": AstMutator().mutate,
    "imports": AstMutator().mutate,
    "typing": AstMutator().mutate,
    "error_paths": AstMutator().mutate,
}

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\mutators\ast_refactor.py =====
# systems/simula/code_sim/mutators/ast_refactor.py
"""
AST-driven refactoring & scaffolding for Simula

Purpose
-------
Produce deterministic, structure-correct unified diffs that:
- Scaffold missing modules/functions/classes from step targets
- Repair imports (add/normalize) based on usage and constraints
- Tighten typing (add annotations, Optional, return types)
- Harden error paths (explicit exceptions, guard clauses, logging)
- Perform small, safe rewrites that unblock tests/static analysis

Key Principles
--------------
- No side effects: returns *diff text only*. Orchestrator applies/rolls back.
- Idempotent: running the same mutation twice yields the same file content.
- Conservative by default; "aggressive" toggled by Portfolio when needed.
- Stdlib-only. Python ≥3.11 assumed by Simula constraints.

Public API
----------
AstMutator.mutate(step, mode) -> Optional[str]
  modes: "scaffold", "imports", "typing", "error_paths"

Implementation Notes
--------------------
- Uses Python's `ast` for correctness; relies on `ast.unparse` for codegen.
- Preserves module headers and critical comments via a simple preamble keeper.
- Generates *unified diff* with standard a/<rel> and b/<rel> paths.

Limitations
-----------
- Does not attempt deep semantic edits; this is a structural un-blocker.
- Typing mode heuristics are intentionally conservative to avoid churn.
"""

from __future__ import annotations

import ast
import difflib
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any

# Repo root (stringly path for safety inside containers/CI)
REPO_ROOT = Path(os.environ.get("SIMULA_REPO_ROOT", Path.cwd())).resolve()


# =========================
# Small utilities
# =========================


def _read(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except FileNotFoundError:
        return ""


def _rel_for_diff(path: Path) -> str:
    try:
        rel = path.resolve().relative_to(REPO_ROOT).as_posix()
    except Exception:
        rel = path.name  # fallback
    return rel


def _unified_diff(old: str, new: str, rel_path: str) -> str:
    a = old.splitlines(True)
    b = new.splitlines(True)
    return "".join(
        difflib.unified_diff(a, b, fromfile=f"a/{rel_path}", tofile=f"b/{rel_path}", lineterm=""),
    )


def _strip_shebang_and_encoding(src: str) -> tuple[str, str]:
    """Return (preamble, body) where preamble keeps shebang/encoding/comment banner."""
    lines = src.splitlines(True)
    pre: list[str] = []
    i = 0
    for i, ln in enumerate(lines):
        if i == 0 and ln.startswith("#!"):
            pre.append(ln)
            continue
        if re.match(r"#\s*-\*-\s*coding:", ln):
            pre.append(ln)
            continue
        if ln.startswith("#") and i < 8:
            pre.append(ln)
            continue
        if ln.strip() == "" and i < 6:
            pre.append(ln)
            continue
        break
    else:
        i += 1
    body = "".join(lines[i:])
    return ("".join(pre), body)


def _ensure_module_docstring(tree: ast.Module, doc: str) -> None:
    if not (
        tree.body
        and isinstance(tree.body[0], ast.Expr)
        and isinstance(getattr(tree.body[0], "value", None), ast.Constant)
        and isinstance(tree.body[0].value.value, str)
    ):
        tree.body.insert(0, ast.Expr(value=ast.Constant(value=doc)))


def _parse_sig(signature: str) -> tuple[str, list[str]]:
    """Parse 'name(arg: T, x: int) -> R' into (name, [param names])."""
    head = signature.strip()
    name = head.split("(", 1)[0].strip()
    inside = head.split("(", 1)[1].rsplit(")", 1)[0] if "(" in head and ")" in head else ""
    params = [p.split(":")[0].split("=")[0].strip() for p in inside.split(",") if p.strip()]
    return name, params


def _build_func_def_from_sig(signature: str, doc: str) -> ast.FunctionDef:
    """
    Best-effort: synthesis a FunctionDef with typed args from a human signature.
    - NO NotImplementedError: we generate a non-throwing stub (docstring + pass)
    - Types are parsed literally; unknowns become `Any` (typing import added elsewhere)
    """
    name = signature.strip().split("(", 1)[0].strip()
    ret_ann = None
    if "->" in signature:
        ret_part = signature.split("->", 1)[1].strip()
        if ret_part:
            base = ret_part.replace("[", " ").replace("]", " ").split()[0]
            if base:
                ret_ann = ast.Name(id=base, ctx=ast.Load())

    args_blob = signature.split("(", 1)[1].rsplit(")", 1)[0] if "(" in signature else ""
    args = ast.arguments(
        posonlyargs=[],
        args=[],
        kwonlyargs=[],
        kw_defaults=[],
        defaults=[],
        vararg=None,
        kwarg=None,
    )
    for p in [s.strip() for s in args_blob.split(",") if s.strip()]:
        nm = p.split(":")[0].split("=")[0].strip()
        ann = None
        if ":" in p:
            typ = p.split(":", 1)[1].strip().split("=")[0].strip()
            base = typ.replace("[", " ").replace("]", " ").split()[0]
            if base:
                ann = ast.Name(id=base, ctx=ast.Load())
        args.args.append(ast.arg(arg=nm, annotation=ann))

    body = [
        ast.Expr(value=ast.Constant(value=doc)),  # docstring must be first
        ast.Pass(),
    ]
    fn = ast.FunctionDef(
        name=name,
        args=args,
        body=body,
        decorator_list=[],
        returns=ret_ann,
        type_comment=None,
    )
    ast.fix_missing_locations(fn)
    return fn


def _ensure_import(
    module: ast.Module,
    name: str,
    asname: str | None = None,
    from_: str | None = None,
) -> bool:
    """Ensure an import is present; return True if modified."""

    def has_import() -> bool:
        for node in module.body:
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name == name and (asname is None or alias.asname == asname):
                        return True
            if isinstance(node, ast.ImportFrom) and node.module == from_:
                for alias in node.names:
                    if alias.name == name and (asname is None or alias.asname == asname):
                        return True
        return False

    if has_import():
        return False

    imp = (
        ast.ImportFrom(module=from_, names=[ast.alias(name=name, asname=asname)], level=0)
        if from_
        else ast.Import(names=[ast.alias(name=name, asname=asname)])
    )
    # Insert after module docstring if present
    insert_at = (
        1
        if (
            module.body
            and isinstance(module.body[0], ast.Expr)
            and isinstance(getattr(module.body[0], "value", None), ast.Constant)
        )
        else 0
    )
    module.body.insert(insert_at, imp)
    ast.fix_missing_locations(module)
    return True


def _ensure_logger(module: ast.Module) -> None:
    modified = False
    modified |= _ensure_import(module, "logging")
    # ensure logger variable if missing
    for node in module.body:
        if isinstance(node, ast.Assign):
            for t in node.targets:
                if isinstance(t, ast.Name) and t.id == "logger":
                    return
    assign = ast.Assign(
        targets=[ast.Name(id="logger", ctx=ast.Store())],
        value=ast.Call(
            func=ast.Attribute(
                value=ast.Name(id="logging", ctx=ast.Load()),
                attr="getLogger",
                ctx=ast.Load(),
            ),
            args=[ast.Name(id="__name__", ctx=ast.Load())],
            keywords=[],
        ),
        type_comment=None,
    )
    # place after imports/docstring cluster
    idx = 0
    for i, n in enumerate(module.body[:6]):
        if isinstance(n, ast.Import | ast.ImportFrom) or (
            isinstance(n, ast.Expr) and isinstance(getattr(n, "value", None), ast.Constant)
        ):
            idx = i + 1
    module.body.insert(idx, assign)
    ast.fix_missing_locations(module)


def _module_has_function(module: ast.Module, name: str) -> bool:
    return any(isinstance(n, ast.FunctionDef) and n.name == name for n in module.body)


def _add_guard_raises(fn: ast.FunctionDef, exc: str = "ValueError") -> bool:
    """
    Insert a minimal guard on the first argument if no guard present.
    """
    if not fn.args.args:
        return False
    if any(isinstance(n, ast.Raise) for n in fn.body[:2]):
        return False
    # skip if a top guard already exists
    for n in fn.body[:3]:
        if isinstance(n, ast.If):
            return False
    first = fn.args.args[0]
    cond = ast.UnaryOp(op=ast.Not(), operand=ast.Name(id=first.arg, ctx=ast.Load()))
    msg = ast.Constant(value=f"Invalid '{first.arg}'")
    raise_stmt = ast.Raise(
        exc=ast.Call(func=ast.Name(id=exc, ctx=ast.Load()), args=[msg], keywords=[]),
        cause=None,
    )
    fn.body.insert(0, ast.If(test=cond, body=[raise_stmt], orelse=[]))
    ast.fix_missing_locations(fn)
    return True


def _ensure_return_annotations(fn: ast.FunctionDef) -> bool:
    if fn.returns is not None:
        return False
    # Simple heuristic:
    # - is_/has_/can_/should_ -> bool
    # - get/find/load/fetch   -> Optional[Any]
    # - otherwise             -> Any
    name = fn.name.lower()
    if name.startswith(("is_", "has_", "can_", "should_", "valid")):
        fn.returns = ast.Name(id="bool", ctx=ast.Load())
    elif name.startswith(("get", "find", "load", "fetch")):
        fn.returns = ast.Subscript(
            value=ast.Name(id="Optional", ctx=ast.Load()),
            slice=ast.Name(id="Any", ctx=ast.Load()),
            ctx=ast.Load(),
        )
    else:
        fn.returns = ast.Name(id="Any", ctx=ast.Load())
    ast.fix_missing_locations(fn)
    return True


def _ensure_arg_annotations(fn: ast.FunctionDef) -> bool:
    modified = False
    for a in fn.args.args:
        if a.annotation is None:
            a.annotation = ast.Name(id="Any", ctx=ast.Load())
            modified = True
    if modified:
        ast.fix_missing_locations(fn)
    return modified


# =========================
# Mutator
# =========================


@dataclass
class AstMutator:
    aggressive: bool = False

    def set_aggressive(self, v: bool) -> None:
        self.aggressive = bool(v)

    # ---- Public entrypoint ----

    def mutate(self, step_dict: dict[str, Any], mode: str = "scaffold") -> str | None:
        """
        Return a unified diff for the primary target file, or None if no-op.
        Modes:
          - scaffold: ensure module + target function exists with docstring & logger
          - imports: add missing imports for typing/logging/typing.Optional
          - typing: add Any/Optional/return annotations conservatively
          - error_paths: insert minimal guard raises & error logs
        """
        targets = step_dict.get("targets", [])
        primary_target = targets[0] if targets and isinstance(targets, list) else {}
        target_file = primary_target.get("file")
        export_sig = primary_target.get("export")        
        
        if not target_file:
            return None
        path = (REPO_ROOT / target_file).resolve()
        old_src = _read(path)
        preamble, body = _strip_shebang_and_encoding(old_src)
        if not body.strip():
            body = "\n"  # keep parseable when empty

        try:
            tree = ast.parse(body)
        except SyntaxError:
            tree = ast.parse("")  # start clean if broken

        changed = False

        if mode == "scaffold":
            step_name = step_dict.get("name", "simula")
            changed |= self._do_scaffold(
                tree,
                export_sig,
                step_name=step_name,
            )
            _ensure_logger(tree)  # always ensure logger on scaffold
        elif mode == "imports":
            changed |= self._do_imports(tree)
        elif mode == "typing":
            changed |= self._do_typing(tree)
        elif mode == "error_paths":
            changed |= self._do_error_paths(tree)
        else:
            return None

        if not changed:
            return None

        # Build new source (idempotent, preserve preamble)
        new_body = ast.unparse(tree)
        new_src = preamble + new_body + ("" if new_body.endswith("\n") else "\n")

        rel = _rel_for_diff(path)
        return _unified_diff(old_src, new_src, rel)

    # ---- Mode handlers ----

    def _do_scaffold(self, module: ast.Module, export_sig: str | None, step_name: str) -> bool:
        modified = False
        _ensure_module_docstring(module, f"Autogenerated by Simula step: {step_name}")
        if export_sig:
            fn_name, _ = _parse_sig(export_sig)
            if not _module_has_function(module, fn_name):
                module.body.append(
                    _build_func_def_from_sig(export_sig, f"{step_name}: autogenerated stub"),
                )
                modified = True
            # ensure logger usage inside function (info on entry) after docstring
            for node in module.body:
                if isinstance(node, ast.FunctionDef) and node.name == fn_name:
                    # compute insert index: after docstring if present
                    insert_at = (
                        1
                        if (
                            node.body
                            and isinstance(node.body[0], ast.Expr)
                            and isinstance(getattr(node.body[0], "value", None), ast.Constant)
                        )
                        else 0
                    )
                    has_info = any(
                        isinstance(n, ast.Expr)
                        and isinstance(getattr(n, "value", None), ast.Call)
                        and isinstance(getattr(n.value, "func", None), ast.Attribute)
                        and getattr(n.value.func, "attr", "") == "info"
                        for n in node.body[:2]
                    )
                    if not has_info:
                        call = ast.Expr(
                            value=ast.Call(
                                func=ast.Attribute(
                                    value=ast.Name(id="logger", ctx=ast.Load()),
                                    attr="info",
                                    ctx=ast.Load(),
                                ),
                                args=[ast.Constant(value=f"{fn_name}() called")],
                                keywords=[],
                            ),
                        )
                        node.body.insert(insert_at, call)
                        ast.fix_missing_locations(node)
                        modified = True
        return modified

    def _do_imports(self, module: ast.Module) -> bool:
        modified = False
        modified |= _ensure_import(module, "logging")
        # typing essentials if used elsewhere
        modified |= _ensure_import(module, "Any", from_="typing")
        modified |= _ensure_import(module, "Optional", from_="typing")
        _ensure_logger(module)
        return modified

    def _do_typing(self, module: ast.Module) -> bool:
        modified = False
        any_arg_or_ret = False
        for node in module.body:
            if isinstance(node, ast.FunctionDef):
                any_arg_or_ret |= _ensure_arg_annotations(node)
                any_arg_or_ret |= _ensure_return_annotations(node)
        if any_arg_or_ret:
            modified |= _ensure_import(module, "Any", from_="typing")
            modified |= _ensure_import(module, "Optional", from_="typing")
        return modified or any_arg_or_ret

    def _do_error_paths(self, module: ast.Module) -> bool:
        modified = False
        _ensure_logger(module)
        for node in module.body:
            if isinstance(node, ast.FunctionDef):
                modified |= _add_guard_raises(node, exc="ValueError")
                # if a Raise exists, prepend a logger.error for traceability
                for i, stmt in enumerate(list(node.body)):
                    if isinstance(stmt, ast.Raise):
                        prev = node.body[i - 1] if i > 0 else None
                        already_logged = (
                            isinstance(prev, ast.Expr)
                            and isinstance(getattr(prev, "value", None), ast.Call)
                            and isinstance(getattr(prev.value, "func", None), ast.Attribute)
                            and getattr(prev.value.func, "attr", "") in {"error", "exception"}
                        )
                        if not already_logged:
                            err = ast.Expr(
                                value=ast.Call(
                                    func=ast.Attribute(
                                        value=ast.Name(id="logger", ctx=ast.Load()),
                                        attr="error",
                                        ctx=ast.Load(),
                                    ),
                                    args=[ast.Constant(value=f"{node.name} raised")],
                                    keywords=[],
                                ),
                            )
                            node.body.insert(i, err)
                            ast.fix_missing_locations(node)
                            modified = True
                        break
        return modified

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\mutators\mutants.py =====
# systems/simula/code_sim/mutation/mutants.py
from __future__ import annotations

import ast
import difflib
from dataclasses import dataclass
from pathlib import Path


@dataclass
class Mutant:
    file: str
    before: str
    after: str
    label: str


_REWRS = [
    # Boolean negation
    (
        ast.UnaryOp,
        ast.Not,
        lambda n: ast.copy_location(ast.UnaryOp(op=ast.Not(), operand=n.operand), n),
    ),
    # Compare operators swap
    (ast.Gt, None, lambda n: ast.Lt()),
    (ast.Lt, None, lambda n: ast.Gt()),
    (ast.GtE, None, lambda n: ast.LtE()),
    (ast.LtE, None, lambda n: ast.GtE()),
    # True/False flip
    (
        ast.Constant,
        True,
        lambda n: ast.copy_location(ast.Constant(value=not n.value), n)
        if isinstance(n.value, bool)
        else n,
    ),
]


def _mutate(tree: ast.AST) -> list[tuple[str, ast.AST]]:
    out = []

    class Rewriter(ast.NodeTransformer):
        def visit(self, node):  # type: ignore
            for typ, mark, fn in _REWRS:
                try:
                    if (
                        typ is ast.Constant
                        and isinstance(node, ast.Constant)
                        and isinstance(node.value, bool)
                    ) or (
                        isinstance(node, typ)
                        and (mark is None or isinstance(getattr(node, "op", None), mark))
                    ):
                        new = fn(node)
                        if new is not node:
                            out.append((f"{typ.__name__}", new))
                except Exception:
                    pass
            return self.generic_visit(node)

    Rewriter().visit(tree)
    return [(lbl, t) for (lbl, t) in out]


def _unified(before: str, after: str, path: str) -> str:
    a = before.splitlines(keepends=True)
    b = after.splitlines(keepends=True)
    return "".join(difflib.unified_diff(a, b, fromfile=f"a/{path}", tofile=f"b/{path}", n=2))


def generate_mutants(py_file: str, *, max_per_file: int = 8) -> list[Mutant]:
    p = Path(py_file)
    try:
        text = p.read_text(encoding="utf-8")
    except Exception:
        return []
    try:
        tree = ast.parse(text)
    except Exception:
        return []
    muts = _mutate(tree)[:max_per_file]
    out: list[Mutant] = []
    for i, (lbl, _newnode) in enumerate(muts):
        # naive: replace first occurrence only by toggling booleans in text positions
        after = (
            text.replace(" True", " False").replace(" False", " True")
            if "Constant" in lbl
            else text
        )
        if after != text:
            out.append(Mutant(file=str(p), before=text, after=after, label=lbl))
            break
    return out

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\mutators\prompt_patch.py =====
from __future__ import annotations

import json
import os
import re
from pathlib import Path
from typing import Any

from httpx import HTTPStatusError

from core.prompting.orchestrator import build_prompt
from core.utils.net_api import ENDPOINTS, get_http_client
from core.utils.llm_gateway_client import call_llm_service
REPO_ROOT = Path(os.environ.get("SIMULA_REPO_ROOT", "/workspace")).resolve()


async def _read_snip(p: Path, n: int = 120) -> str:
    if not p.is_file():
        return ""
    try:
        lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
        if len(lines) > n:
            head = "\n".join(lines[: n // 2])
            tail = "\n".join(lines[-n // 2 :])
            return f"{head}\n...\n{tail}"
        return "\n".join(lines)
    except Exception:
        return ""


async def _targets_context(step_dict: dict[str, Any]) -> str:
    blocks = []
    # Access targets from the dictionary
    targets = step_dict.get("targets", [])
    for t in targets or []:
        rel = t.get("file") if isinstance(t, dict) else None
        if not rel:
            continue
        p = (REPO_ROOT / rel).resolve()
        snippet = await _read_snip(p)
        blocks.append(f"### {rel}\n```\n{snippet}\n```")
    return "\n".join(blocks)



def _strip_fences(text: str | None) -> str:
    if not text:
        return ""
    # capture from first '--- a/' up to a trailing ``` or end of string
    m = re.search(r"--- a/.*?(?=\n```|\Z)", text, re.DOTALL)
    return m.group(0).strip() if m else ""


def _coerce_primary_target_text(step_dict: dict[str, Any]) -> str:
    # Replicate primary_target() logic using dictionary access
    targets = step_dict.get("targets", [])
    primary_target = targets[0] if targets and isinstance(targets, list) else {}
    target_file = primary_target.get("file")
    export_sig = primary_target.get("export")
    
    if target_file and export_sig:
        return f"{target_file} :: {export_sig}"
    if target_file:
        return target_file
    return ""


async def llm_unified_diff(step_dict: dict[str, Any], variant: str = "base") -> str | None:
    """
    Generate a unified diff via the central PromptSpec orchestrator.
    Output should be raw text starting with '--- a/...'.
    """
    few_shot_example = (
        "--- a/example.py\n"
        "+++ b/example.py\n"
        "@@ -1,3 +1,3 @@\n"
        " def main():\n"
        '-    print("hello")\n'
        '+    print("hello, world")\n'
    )

    objective_dict = step_dict.get("objective", {})
    objective_text = objective_dict if isinstance(objective_dict, str) else json.dumps(objective_dict)
    primary_target_text = _coerce_primary_target_text(step_dict)
    context_str = await _targets_context(step_dict)

    prompt_response = await build_prompt(
        scope="simula.codegen.unified_diff",
        summary="Produce a valid unified diff for Simula code evolution",
        context={
            "vars": {
                "objective_text": objective_text,
                "primary_target_text": primary_target_text,
                "file_context": context_str,
                "few_shot_example": few_shot_example,
                "variant": variant,
            },
        },
    )

    try:
        llm_resp = await call_llm_service(
            prompt_response=prompt_response,
            agent_name="Simula",
            scope="simula.codegen.unified_diff",
        )
        raw_text = (getattr(llm_resp, "text", "") or "").strip()

        # Debug (optional)
        print("\n[DEBUG LLM_PATCH] --- RAW LLM Response ---")
        print(raw_text[:2000])
        print("---------------------------------------\n")

        cleaned = _strip_fences(raw_text) or raw_text
        return cleaned if cleaned.startswith("--- a/") else cleaned
    except Exception as e:
        print(f"[PROMPT_PATCH_ERROR] Unexpected error: {e}")
        return None

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\mutators\retrieval_edit.py =====
# systems/simula/code_sim/mutators/retrieval_edit.py
from __future__ import annotations

import difflib
import logging
import os
import re
from pathlib import Path
from typing import Literal

logger = logging.getLogger(__name__)

REPO_ROOT = Path(os.environ.get("SIMULA_REPO_ROOT", "/workspace")).resolve()


def _unidiff(old: str, new: str, rel: str) -> str:
    """Generate a unified diff between old and new text."""
    a = old.splitlines(True)
    b = new.splitlines(True)
    return "".join(difflib.unified_diff(a, b, fromfile=f"a/{rel}", tofile=f"b/{rel}", lineterm=""))


def _read(path: Path) -> str:
    """Read file content as utf-8; return empty string on failure (caller decides create vs. modify)."""
    try:
        return path.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        logger.debug("read failed for %s: %s", path, e)
        return ""


def _ensure_line(src: str, needle: str) -> tuple[str, bool]:
    """Ensure an exact line exists in src; returns (updated_text, changed?)."""
    # Exact match across lines to avoid partial substrings
    lines = src.splitlines()
    if any(line.strip() == needle.strip() for line in lines):
        return src, False
    if not src.endswith("\n"):
        src += "\n"
    return src + needle.rstrip("\n") + "\n", True


def _detect_registry_path() -> Path:
    """Return the most plausible registry module path; if none exist, choose canonical location to create."""
    candidates = [
        REPO_ROOT / "systems" / "synk" / "core" / "tools" / "registry.py",
        REPO_ROOT / "systems" / "feature" / "tool.py",
    ]
    for c in candidates:
        if c.exists():
            return c
    # Prefer synk registry as canonical creation target
    return candidates[0]


# ---------- Public entry ----------


def retrieval_guided_edits(
    step,
    mode: Literal["registry", "config", "prior_art", "tests"],
) -> str | None:
    """
    Apply deterministic retrieval-guided edits:
      - "registry": ensure tools required by acceptance.contracts.must_register are registered.
      - "config": ensure pyproject.toml contains formatter/linter config blocks.
      - "prior_art": create a missing module/function with a concrete, safe body and logging.
      - "tests": create a smoke test that imports target module and asserts function presence.
    Returns a unified diff string or None if no change is needed.
    """

    if mode == "registry":
        # Extract tool names from acceptance contracts (strings or dicts).
        acc = (step.objective or {}).get("acceptance", {})
        regs = acc.get("contracts", {}).get("must_register", []) or []

        tool_names: list[str] = []
        for r in regs:
            s = str(r)
            # Accept patterns like "tool 'name'" or {"tool": "name"} or plain "name"
            m = re.search(r"tool\s*['\"]([^'\"]+)['\"]", s)
            if m:
                tool_names.append(m.group(1).strip())
                continue
            m2 = re.search(r"'tool'\s*:\s*['\"]([^'\"]+)['\"]", s)
            if m2:
                tool_names.append(m2.group(1).strip())
                continue
            # Last resort: a clean token without spaces
            token = s.strip()
            if token and " " not in token and ":" not in token:
                tool_names.append(token)

        tool_names = sorted({t for t in tool_names if t})

        if not tool_names:
            return None

        reg_path = _detect_registry_path()
        old = _read(reg_path)

        # Ensure module has a register_tool symbol or create a minimal registry.
        new = old or (
            "# Auto-created tool registry\n"
            "from __future__ import annotations\n"
            "from typing import Dict, Any\n\n"
            "_REGISTRY: Dict[str, Dict[str, Any]] = {}\n\n"
            "def register_tool(name: str, metadata: Dict[str, Any] | None = None) -> None:\n"
            "    _REGISTRY[name] = dict(metadata or {})\n\n"
            "def has_tool(name: str) -> bool:\n"
            "    return name in _REGISTRY\n"
        )

        changed = False
        for name in tool_names:
            # Avoid false positives by searching for exact call pattern
            if re.search(rf"register_tool\(\s*['\"]{re.escape(name)}['\"]", new):
                continue
            new, did = _ensure_line(new, f"register_tool('{name}', metadata={{}})")
            changed = changed or did

        if not changed:
            return None
        return _unidiff(old, new, str(reg_path.relative_to(REPO_ROOT)))

    if mode == "config":
        p = REPO_ROOT / "pyproject.toml"
        old = _read(p)
        if not old:
            return None

        new = old
        blocks: list[str] = []

        if "[tool.ruff]" not in new:
            blocks.append("\n[tool.ruff]\nline-length = 100\n")
        if "[tool.isort]" not in new:
            blocks.append('\n[tool.isort]\nprofile = "black"\n')
        if "[tool.black]" not in new:
            blocks.append("\n[tool.black]\nline-length = 100\n")
        if "[tool.mypy]" not in new:
            blocks.append("\n[tool.mypy]\nignore_missing_imports = true\nstrict_optional = true\n")

        if not blocks:
            return None

        # Ensure single trailing newline
        if not new.endswith("\n"):
            new += "\n"
        new += "".join(blocks)
        return _unidiff(old, new, str(p.relative_to(REPO_ROOT)))

    if mode == "prior_art":
        # Create a missing module/function with a concrete, side-effect-free body.
        rel, sig = step.primary_target()
        if not rel:
            return None
        p = REPO_ROOT / rel
        if p.exists():
            return None

        old = ""
        header = f'"""Autogenerated scaffold for {rel} (Simula retrieval-guided)."""\n'
        body = (
            "from __future__ import annotations\n"
            "import logging\n"
            "from typing import Any\n"
            "logger = logging.getLogger(__name__)\n\n"
        )
        if sig:
            name = sig.split("(", 1)[0].strip()
            body += f"def {sig}:\n"
            body += f"    logger.info('{name} invoked')\n"
            body += "    # Return a deterministic neutral value to keep the system runnable\n"
            body += "    return None\n"
        new = header + body
        return _unidiff(old, new, rel)

    if mode == "tests":
        tests = step.match_tests(REPO_ROOT)
        write_targets = [t for t in tests if not t.exists()]
        if not write_targets:
            return None

        rel = str(write_targets[0].relative_to(REPO_ROOT))
        old = ""
        tgt_file, sig = step.primary_target()
        mod_path = tgt_file.replace("/", ".").rstrip(".py")
        fn_name = sig.split("(", 1)[0].strip() if sig else None

        content = [
            "import importlib",
            "",
            "def test_import_target():",
            f"    m = importlib.import_module('{mod_path}')",
        ]
        if fn_name:
            content.append(f"    assert hasattr(m, '{fn_name}')")
        content.append("")  # trailing newline
        new = "\n".join(content)
        return _unidiff(old, new, rel)

    return None

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\mutators\runner.py =====
# systems/simula/code_sim/mutation/runner.py
from __future__ import annotations

from dataclasses import dataclass

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config

from .mutants import generate_mutants


@dataclass
class MutationResult:
    total: int
    killed: int
    score: float


async def run_mutation_tests(
    changed_files: list[str],
    *,
    k_expr: str = "",
    timeout_sec: int = 900,
) -> dict[str, object]:
    muts = []
    for f in changed_files:
        muts.extend(generate_mutants(f))
    total = len(muts)
    if total == 0:
        return {"status": "noop", "score": 1.0, "total": 0, "killed": 0}
    killed = 0
    async with DockerSandbox(seed_config()).session() as sess:
        for m in muts:
            # apply mutant
            await sess._run_tool(
                [
                    "bash",
                    "-lc",
                    f"python - <<'PY'\nfrom pathlib import Path;Path({m.file!r}).write_text({m.after!r}, encoding='utf-8')\nPY",
                ],
            )
            ok, _ = await sess.run_pytest_select(["tests"], k_expr, timeout=timeout_sec)
            # If tests fail with mutant → killed
            if not ok:
                killed += 1
            # revert file back to original
            await sess._run_tool(
                [
                    "bash",
                    "-lc",
                    f"python - <<'PY'\nfrom pathlib import Path;Path({m.file!r}).write_text({m.before!r}, encoding='utf-8')\nPY",
                ],
            )
    score = killed / total
    return {"status": "done", "score": score, "total": total, "killed": killed}

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\portfolio\__init__.py =====
# systems/simula/code_sim/portfolio/__init__.py  (include structural strategies)
from __future__ import annotations

from typing import Any, Dict, List

from .strategies import generate_candidates as _gen_basic
from .strategies_structural import generate_structural_candidates as _gen_struct


async def generate_candidate_portfolio(
    *,
    job_meta: dict[str, Any],
    step: Any,
) -> list[dict[str, Any]]:
    desc = getattr(step, "name", None) or getattr(step, "desc", None) or str(step)
    target_file = "unknown.py"
    fn_name = None
    intent = "edit"
    if "::" in desc:
        parts = [p for p in desc.split("::") if p]
        intent = parts[0] if parts else "edit"
        target_file = parts[1] if len(parts) >= 2 else target_file
        fn_name = parts[2] if len(parts) >= 3 else None

    c_basic = _gen_basic(target_file, fn_name, intent=intent)
    c_struct = _gen_struct(target_file, fn_name)
    portfolio = []
    i = 0
    for c in (c_basic + c_struct)[:10]:
        portfolio.append(
            {
                "id": f"cand_{i}",
                "title": f"{c.risk.upper()}:{c.uid}",
                "diff": c.diff,
                "rationale": c.rationale,
                "risk": c.risk,
                "meta": {"generator": "simula.portfolio", **(c.meta or {})},
            },
        )
        i += 1
    return portfolio

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\portfolio\strategies.py =====
# systems/simula/code_sim/portfolio/strategies.py
from __future__ import annotations

import ast
import difflib
from dataclasses import dataclass
from pathlib import Path
from typing import Any


@dataclass
class CandidatePatch:
    uid: str
    rationale: str
    risk: str  # "low" | "medium" | "high"
    diff: str
    meta: dict[str, Any]


def _read(path: str) -> tuple[str, ast.AST | None]:
    try:
        text = Path(path).read_text(encoding="utf-8")
    except Exception:
        return "", None
    try:
        return text, ast.parse(text)
    except Exception:
        return text, None


def _unified(before: str, after: str, path: str) -> str:
    a = before.splitlines(keepends=True)
    b = after.splitlines(keepends=True)
    return "".join(difflib.unified_diff(a, b, fromfile=f"a/{path}", tofile=f"b/{path}", n=2))


def _insert_guard_none(src: str, fn_name: str) -> str | None:
    try:
        tree = ast.parse(src)
    except Exception:
        return None

    class Rewriter(ast.NodeTransformer):
        def visit_FunctionDef(self, node: ast.FunctionDef):
            if node.name != fn_name:
                return node
            guards = []
            for arg in node.args.args:
                if arg.arg in ("self", "cls"):
                    continue
                guards.append(ast.parse(f"if {arg.arg} is None:\n    return {arg.arg}").body[0])
            node.body = guards + node.body
            return node

    try:
        new = Rewriter().visit(tree)
        ast.fix_missing_locations(new)

        return ast.unparse(new) if hasattr(ast, "unparse") else None
    except Exception:
        return None


def _insert_logging(src: str, fn_name: str) -> str | None:
    # naive: add `import logging` (if absent) and a log line at top of function
    if "import logging" not in src:
        src = "import logging\n" + src
    try:
        tree = ast.parse(src)
    except Exception:
        return None

    class Rewriter(ast.NodeTransformer):
        def visit_FunctionDef(self, node: ast.FunctionDef):
            if node.name != fn_name:
                return node
            log = ast.parse(f'logging.debug("Simula:{fn_name} called")').body[0]
            node.body = [log] + node.body
            return node

    try:
        new = Rewriter().visit(tree)
        ast.fix_missing_locations(new)
        return ast.unparse(new) if hasattr(ast, "unparse") else None
    except Exception:
        return None


def _docstring_update(src: str, fn_name: str, note: str) -> str | None:
    try:
        tree = ast.parse(src)
    except Exception:
        return None

    class Rewriter(ast.NodeTransformer):
        def visit_FunctionDef(self, node: ast.FunctionDef):
            if node.name != fn_name:
                return node
            doc = ast.get_docstring(node)
            new_doc = (doc or "") + f"\n\nSimula: {note}"
            node.body.insert(0, ast.parse(f'"""%s"""' % new_doc).body[0])
            return node

    try:
        new = Rewriter().visit(tree)
        ast.fix_missing_locations(new)
        return ast.unparse(new) if hasattr(ast, "unparse") else None
    except Exception:
        return None


def generate_candidates(
    target_file: str,
    fn_name: str | None,
    *,
    intent: str,
) -> list[CandidatePatch]:
    before, _ = _read(target_file)
    if not before:
        return []

    cands: list[CandidatePatch] = []

    # Low-risk: docstring augmentation (acceptance/contract hint)
    if fn_name:
        after = _docstring_update(before, fn_name, f"intent={intent}")
        if after and after != before:
            cands.append(
                CandidatePatch(
                    uid="doc-hint",
                    rationale="Clarify contract via docstring to anchor acceptance/specs.",
                    risk="low",
                    diff=_unified(before, after, target_file),
                    meta={"strategy": "docstring_hint", "fn": fn_name},
                ),
            )

    # Medium: None-guard on parameters
    if fn_name:
        after = _insert_guard_none(before, fn_name)
        if after and after != before:
            cands.append(
                CandidatePatch(
                    uid="guard-none",
                    rationale="Add None-guards to function parameters to avoid TypeErrors.",
                    risk="medium",
                    diff=_unified(before, after, target_file),
                    meta={"strategy": "guard_none", "fn": fn_name},
                ),
            )

    # Medium: lightweight logging
    if fn_name:
        after = _insert_logging(before, fn_name)
        if after and after != before:
            cands.append(
                CandidatePatch(
                    uid="log-entry",
                    rationale="Add debug log on function entry to aid observability in large repos.",
                    risk="low",
                    diff=_unified(before, after, target_file),
                    meta={"strategy": "log_entry", "fn": fn_name},
                ),
            )

    # Fallback: whitespace/pep8 normalization (no-op safety)
    if not cands:
        if not before.endswith("\n"):
            after = before + "\n"
            cands.append(
                CandidatePatch(
                    uid="newline-eof",
                    rationale="Ensure newline at EOF.",
                    risk="low",
                    diff=_unified(before, after, target_file),
                    meta={"strategy": "formatting"},
                ),
            )

    return cands[:6]

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\portfolio\strategies_structural.py =====
# systems/simula/code_sim/portfolio/strategies_structural.py
from __future__ import annotations

import ast
import difflib
from dataclasses import dataclass
from pathlib import Path
from typing import Any


@dataclass
class CandidatePatch:
    uid: str
    rationale: str
    risk: str  # "low" | "medium" | "high"
    diff: str
    meta: dict[str, Any]


def _read_text(p: str) -> str:
    try:
        return Path(p).read_text(encoding="utf-8")
    except Exception:
        return ""


def _unified(before: str, after: str, path: str) -> str:
    a = before.splitlines(keepends=True)
    b = after.splitlines(keepends=True)
    return "".join(difflib.unified_diff(a, b, fromfile=f"a/{path}", tofile=f"b/{path}", n=2))


def _extract_function(src: str, fn: str) -> tuple[str | None, tuple[int, int] | None]:
    try:
        t = ast.parse(src)
    except Exception:
        return None, None
    for n in t.body:
        if isinstance(n, ast.FunctionDef) and n.name == fn:
            start = n.lineno - 1
            end = getattr(n, "end_lineno", None)
            if not end:
                # fallback: scan until next top-level def/class
                end = start + 1
                lines = src.splitlines()
                while end < len(lines) and not lines[end].startswith(("def ", "class ")):
                    end += 1
            return src.splitlines()[start:end], (start, end)
    return None, None


def _extract_function_to_module(src: str, fn: str, new_module: str) -> str | None:
    lines = src.splitlines()
    body, span = _extract_function(src, fn)
    if not body or not span:
        return None
    start, end = span
    # naive extraction: keep function, add import of new module and call-through
    "\n".join(body)
    call_through = f"\n\n# Simula extracted {fn} to {new_module}\nfrom {new_module} import {fn}  # type: ignore\n"
    new_src = "\n".join(lines[:start]) + call_through + "\n".join(lines[end:])
    return new_src


def _rename_function(src: str, old: str, new: str) -> str | None:
    try:
        t = ast.parse(src)
    except Exception:
        return None

    class Ren(ast.NodeTransformer):
        def visit_FunctionDef(self, node: ast.FunctionDef):
            if node.name == old:
                node.name = new
            return self.generic_visit(node)

        def visit_Call(self, node: ast.Call):
            if isinstance(node.func, ast.Name) and node.func.id == old:
                node.func.id = new
            return self.generic_visit(node)

    new = Ren().visit(t)
    ast.fix_missing_locations(new)
    return ast.unparse(new) if hasattr(ast, "unparse") else None


def _tighten_signature(src: str, fn: str) -> str | None:
    try:
        t = ast.parse(src)
    except Exception:
        return None

    class Tight(ast.NodeTransformer):
        def visit_FunctionDef(self, node: ast.FunctionDef):
            if node.name != fn:
                return node
            # add Optional[...] type hints if missing
            for a in node.args.args:
                if a.annotation is None and a.arg not in ("self", "cls"):
                    a.annotation = ast.Name(id="object")
            if node.returns is None:
                node.returns = ast.Name(id="object")
            return node

    new = Tight().visit(t)
    ast.fix_missing_locations(new)
    return ast.unparse(new) if hasattr(ast, "unparse") else None


def generate_structural_candidates(
    target_file: str,
    fn_name: str | None,
) -> list[CandidatePatch]:
    before = _read_text(target_file)
    if not before:
        return []

    cands: list[CandidatePatch] = []

    if fn_name:
        # 1) Rename function (safe adapter will be needed by tests)
        renamed = _rename_function(before, fn_name, f"{fn_name}_impl")
        if renamed and renamed != before:
            cands.append(
                CandidatePatch(
                    uid="rename-fn",
                    rationale=f"Rename {fn_name}→{fn_name}_impl to enable adapter injection.",
                    risk="medium",
                    diff=_unified(before, renamed, target_file),
                    meta={"strategy": "rename_function", "fn": fn_name},
                ),
            )

        # 2) Tighten signature
        typed = _tighten_signature(before, fn_name)
        if typed and typed != before:
            cands.append(
                CandidatePatch(
                    uid="tighten-signature",
                    rationale=f"Add type hints to {fn_name} to clarify contracts.",
                    risk="low",
                    diff=_unified(before, typed, target_file),
                    meta={"strategy": "tighten_signature", "fn": fn_name},
                ),
            )

        # 3) Extract to module (call-through)
        modex = _extract_function_to_module(before, fn_name, f"{Path(target_file).stem}_impl")
        if modex and modex != before:
            cands.append(
                CandidatePatch(
                    uid="extract-module",
                    rationale=f"Extract {fn_name} into companion module; original calls through.",
                    risk="high",
                    diff=_unified(before, modex, target_file),
                    meta={"strategy": "extract_module", "fn": fn_name},
                ),
            )

    return cands[:6]

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\repair\ddmin.py =====
# systems/simula/code_sim/repair/ddmin.py
from __future__ import annotations

import re
from dataclasses import dataclass

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config

_HUNK_RE = re.compile(r"^diff --git a/.+?$\n(?:.+\n)+?(?=^diff --git a/|\Z)", re.M)


def _split_hunks(diff_text: str) -> list[str]:
    hunks = _HUNK_RE.findall(diff_text or "")
    return hunks if hunks else ([diff_text] if diff_text else [])


@dataclass
class DDMinResult:
    status: str
    failing_hunk_index: int | None = None
    healed_diff: str | None = None
    notes: str | None = None


async def isolate_and_attempt_heal(
    diff_text: str,
    *,
    pytest_k: str | None = None,
    timeout_sec: int = 900,
) -> DDMinResult:
    """
    Heuristic ddmin: identify a single failing hunk by re-running tests after reverting each hunk.
    If reverting one hunk returns tests to green, emit a healed diff (original minus that hunk).
    """
    chunks = _split_hunks(diff_text)
    if not chunks:
        return DDMinResult(status="error", notes="empty diff")

    # First, confirm that the full patch is actually red
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        ok_apply = await sess.apply_unified_diff(diff_text)
        if not ok_apply:
            return DDMinResult(status="error", notes="cannot apply original diff")
        ok, _ = await sess.run_pytest_select(["tests"], pytest_k or "", timeout=timeout_sec)
        if ok:
            return DDMinResult(status="green", notes="full patch already green; no ddmin needed")

    # Try reverting each hunk and re-testing
    for idx, hunk in enumerate(chunks):
        async with DockerSandbox(cfg).session() as sess:
            # Apply full patch, then revert this single hunk
            if not await sess.apply_unified_diff(diff_text):
                return DDMinResult(status="error", notes="cannot re-apply diff during ddmin")
            _ = await sess.rollback_unified_diff(hunk)  # revert only this hunk
            ok, _ = await sess.run_pytest_select(["tests"], pytest_k or "", timeout=timeout_sec)
            if ok:
                # Capture healed diff from workspace (original minus reverted hunk)
                out = await sess._run_tool(
                    ["bash", "-lc", "git diff --unified=2 --no-color || true"],
                )
                healed = (out or {}).get("stdout") or ""
                return DDMinResult(
                    status="healed",
                    failing_hunk_index=idx,
                    healed_diff=healed,
                    notes="reverted one failing hunk",
                )
    return DDMinResult(status="unhealed", notes="no single-hunk revert could heal")

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\repair\engine.py =====
# systems/simula/code_sim/repair/engine.py
from __future__ import annotations

from collections.abc import Iterable
from dataclasses import dataclass
from pathlib import Path

import libcst as cst

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config

from .templates import TRANSFORMS, Patch


@dataclass
class RepairOutcome:
    status: str  # "healed" | "partial" | "unchanged" | "error"
    diff: str | None
    tried: int
    notes: str | None = None


def _generate_patches(paths: Iterable[str]) -> list[Patch]:
    """Generates candidate repair patches using a series of LibCST transformers."""
    patches: list[Patch] = []
    for path_str in paths:
        try:
            current_source = Path(path_str).read_text(encoding="utf-8")
            for name, transform_class in TRANSFORMS:
                context = cst.codemod.CodemodContext()
                transformer = transform_class(context)
                tree = cst.parse_module(current_source)
                updated_tree = transformer.transform_module(tree)
                new_source = updated_tree.code

                if new_source != current_source:
                    patches.append(
                        Patch(
                            path=path_str,
                            before=current_source,
                            after=new_source,
                            transform_id=name,
                        ),
                    )
                    current_source = new_source  # Apply transforms sequentially
        except Exception:
            continue  # Skip files that fail to parse or transform
    return patches


async def attempt_repair(paths: Iterable[str], *, timeout_sec: int = 900) -> RepairOutcome:
    """
    Tries a sequence of safe, AST-based transforms on given files, evaluates
    by running tests, and returns a cumulative diff if the tests pass.
    """
    patches = _generate_patches(paths)
    if not patches:
        return RepairOutcome(
            status="unchanged",
            diff=None,
            tried=0,
            notes="No applicable AST transforms found.",
        )

    cfg = seed_config()
    cumulative_diff = ""
    applied_patches = 0

    async with DockerSandbox(cfg).session() as sess:
        # Get baseline diff (in case workspace is dirty)
        initial_diff_result = await sess._run_tool(["git", "diff"])
        initial_diff = initial_diff_result.get("stdout", "")

        for patch in patches:
            # Apply the patch by completely overwriting the file with the new source
            write_ok_result = await sess._run_tool(
                [
                    "python",
                    "-c",
                    f"from pathlib import Path; Path('{patch.path}').write_text({repr(patch.after)}, encoding='utf-8')",
                ],
            )
            if write_ok_result.get("returncode", 1) != 0:
                continue  # Skip if we can't even write the file

            # Run tests to see if this patch fixed the issue
            ok, _ = await sess.run_pytest(list(paths), timeout=timeout_sec)

            if ok:
                # Test suite passed! This is a good patch.
                applied_patches += 1
                # We stop at the first successful repair to return a minimal fix.
                final_diff_result = await sess._run_tool(["git", "diff"])
                final_diff = final_diff_result.get("stdout", "")

                # We must subtract the initial diff to isolate only the changes from this engine
                # A proper diff library would be better, but this is a simple approximation.
                if final_diff.startswith(initial_diff):
                    cumulative_diff = final_diff[len(initial_diff) :]
                else:
                    cumulative_diff = final_diff

                return RepairOutcome(
                    status="healed",
                    diff=cumulative_diff,
                    tried=len(patches),
                    notes=f"Applied {applied_patches} AST patch(es).",
                )

            # Revert the changes if tests failed, to try the next patch from a clean slate
            await sess._run_tool(
                [
                    "python",
                    "-c",
                    f"from pathlib import Path; Path('{patch.path}').write_text({repr(patch.before)}, encoding='utf-8')",
                ],
            )

    return RepairOutcome(
        status="unchanged",
        diff=None,
        tried=len(patches),
        notes="No AST patch resulted in a passing test suite.",
    )

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\repair\templates.py =====
# systems/simula/code_sim/repair/templates.py
# --- PROJECT SENTINEL UPGRADE (FINAL) ---
from __future__ import annotations

import ast
from dataclasses import dataclass

# We now use LibCST for robust, syntax-aware transformations.
import libcst as cst
import libcst.matchers as m
from libcst.codemod import CodemodContext, VisitorBasedCodemodCommand


@dataclass
class Patch:
    path: str
    before: str
    after: str
    transform_id: str


class GuardNoneTransformer(VisitorBasedCodemodCommand):
    """
    An AST-based transformer that adds 'if x is None: return None' guards
    to the beginning of functions for non-self/cls parameters.
    """

    def __init__(self, context: CodemodContext) -> None:
        super().__init__(context)

    def leave_FunctionDef(
        self,
        original_node: cst.FunctionDef,
        updated_node: cst.FunctionDef,
    ) -> cst.FunctionDef:
        guards = []
        # Find parameters that are not 'self' or 'cls' and have no default value
        for param in updated_node.params.params:
            if param.name.value not in ("self", "cls") and param.default is None:
                guard_statement = cst.parse_statement(f"if {param.name.value} is None: return None")
                guards.append(guard_statement)

        # Insert guards after the docstring (if any)
        body_statements = list(updated_node.body.body)
        insert_pos = (
            1
            if (
                body_statements
                and m.matches(
                    body_statements[0],
                    m.SimpleStatementLine(body=[m.Expr(value=m.SimpleString())]),
                )
            )
            else 0
        )

        new_body_statements = body_statements[:insert_pos] + guards + body_statements[insert_pos:]
        return updated_node.with_changes(
            body=updated_node.body.with_changes(body=new_body_statements),
        )


class ImportFixTransformer(VisitorBasedCodemodCommand):
    """
    An AST-based transformer that finds and adds common missing imports.
    This is a placeholder for a more sophisticated import resolver.
    """

    def __init__(self, context: CodemodContext) -> None:
        super().__init__(context)
        self.found_any = False
        self.needs_typing_import = False

    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:
        # Check for un-imported but common type hints
        if "Optional" in ast.unparse(node.returns) or any(
            "Optional" in ast.unparse(p.annotation) for p in node.params.params if p.annotation
        ):
            self.needs_typing_import = True

    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:
        if self.needs_typing_import:
            # This is a simple version; a full implementation would check existing imports
            typing_import = cst.parse_statement("from typing import Any, Dict, List, Optional")
            new_body = [typing_import] + list(updated_node.body)
            return updated_node.with_changes(body=new_body)
        return updated_node


# The list of transforms to apply in order.
TRANSFORMS: list[tuple[str, VisitorBasedCodemodCommand.__class__]] = [
    ("guard_none", GuardNoneTransformer),
    ("import_fix", ImportFixTransformer),
]

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\report\proposal_report.py =====
# systems/simula/code_sim/report/proposal_report.py
from __future__ import annotations

from typing import Any


def _kv(d: dict[str, Any], keys: list[str]) -> str:
    parts = []
    for k in keys:
        v = d.get(k)
        if isinstance(v, float):
            parts.append(f"**{k}**: {v:.4f}")
        elif v is not None:
            parts.append(f"**{k}**: {v}")
    return " • ".join(parts)


def build_report_md(proposal: dict[str, Any]) -> str:
    ctx = proposal.get("context") or {}
    ev = proposal.get("evidence") or {}
    smt = ev.get("smt_verdict") or {}
    sim = ev.get("simulation") or {}
    hyg = ev.get("hygiene") or {}
    cov = ev.get("coverage_delta") or {}
    impact = ev.get("impact") or {}

    lines = []
    lines.append(f"# Proposal {proposal.get('proposal_id', '')}\n")
    lines.append("## Summary")
    lines.append(f"- Files changed: {len(impact.get('changed') or [])}")
    if impact.get("k_expr"):
        lines.append(f"- Focus tests (k): `{impact.get('k_expr')}`")
    lines.append("")
    lines.append("## SMT / Simulation")
    lines.append(f"- SMT: {_kv(smt, ['ok', 'reason'])}")
    lines.append(f"- Sim: {_kv(sim, ['p_success', 'p_safety_hit', 'reason'])}")
    lines.append("")
    lines.append("## Hygiene")
    lines.append(f"- Static: `{hyg.get('static')}`  •  Tests: `{hyg.get('tests')}`")
    if cov:
        pct = cov.get("pct_changed_covered", 0.0)
        lines.append(f"- ΔCoverage on changed lines: **{pct:.2f}%**")
    lines.append("")
    if ev.get("failing_tests"):
        lines.append("## Failing tests (parsed)")
        for ft in ev["failing_tests"]:
            name = ft.get("nodeid") or "unknown"
            msg = (ft.get("short") or "")[:240]
            lines.append(f"- `{name}` — {msg}")
        lines.append("")
    if ctx.get("diff"):
        lines.append("## Diff (excerpt)")
        diff_excerpt = "\n".join(ctx["diff"].splitlines()[:200])
        lines.append("```diff")
        lines.append(diff_excerpt)
        lines.append("```")
        lines.append("")
    return "\n".join(lines)

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\retrieval\context.py =====
# simula/code_sim/retrieval/context.py
"""
High‑Signal Code Retrieval for Patch Generation

Mission
-------
Feed the LLM diff generator with the *most relevant, compact* slices of the repo:
registry, nearby modules, test oracles, and any obvious spec/schema anchors.

Principles
----------
- **Deterministic & fast**: pure stdlib, linear scans with hard byte caps.
- **Signal‑dense**: prefer definitions, public APIs, and assertions over boilerplate.
- **Safe**: never slurp secrets; ignore large binaries; enforce size & file count limits.
- **Composable**: small helpers you can reuse in mutators/evaluators.

Public API
----------
default_neighbor_globs() -> list[str]
gather_neighbor_snippets(repo_root: Path, file_rel: str) -> dict[path->snippet]
"""

from __future__ import annotations

import re
from collections.abc import Iterable, Iterator
from dataclasses import dataclass
from pathlib import Path

# -------- Tunables (conservative defaults) --------

MAX_FILES = 24  # hard cap on files collected
MAX_BYTES_PER_FILE = 4_000  # truncate each file to this many bytes
MAX_TOTAL_BYTES = 48_000  # overall cap
PY_EXTS = {".py"}
TEXT_EXTS = {".md", ".rst", ".txt", ".yaml", ".yml", ".toml", ".ini"}
IGNORE_DIRS = {
    ".git",
    ".simula",
    ".venv",
    "venv",
    ".mypy_cache",
    "__pycache__",
    "node_modules",
    "dist",
    "build",
}

# Heuristic weights for ranking neighbors
WEIGHTS = {
    "tests": 1.0,
    "registry": 0.9,
    "same_pkg": 0.8,
    "same_dir": 0.7,
    "docs": 0.4,
    "spec": 0.85,
    "schemas": 0.6,
}

# Conventional anchor paths used elsewhere in EOS; safe if missing
CONVENTIONAL_ANCHORS = [
    "systems/synk/core/tools/registry.py",
    "systems/synk/specs/schema.py",
    "systems/axon/specs/schema.py",
]

# --------------------------------------------------


def default_neighbor_globs() -> list[str]:
    """Return the default set of globs we consider for snippets."""
    return [
        "systems/**/*.py",
        "tests/**/*.py",
        "tests/**/*.md",
        "docs/**/*.*",
        "examples/**/*.py",
        "pyproject.toml",
        "README.md",
    ]


def _is_textual(path: Path) -> bool:
    if path.suffix in PY_EXTS | TEXT_EXTS:
        return True
    # Basic sniff: avoid likely binaries
    try:
        b = path.read_bytes()[:512]
    except Exception:
        return False
    if b"\x00" in b:
        return False
    try:
        b.decode("utf-8")
        return True
    except Exception:
        return False


def _shorten(text: str, limit: int) -> str:
    if len(text) <= limit:
        return text
    # Keep header and tail (often contains exports/tests) with an ellipsis in the middle
    head = text[: int(limit * 0.7)]
    tail = text[-int(limit * 0.25) :]
    return head + "\n# …\n" + tail


def _read_text(path: Path, limit: int = MAX_BYTES_PER_FILE) -> str:
    try:
        data = path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return ""
    return _shorten(data, limit)


def _iter_globs(root: Path, patterns: Iterable[str]) -> Iterator[Path]:
    for pat in patterns:
        yield from root.glob(pat)


def _norm_rel(root: Path, p: Path) -> str:
    try:
        return str(p.relative_to(root).as_posix())
    except Exception:
        return p.as_posix()


@dataclass(frozen=True)
class Neighbor:
    path: Path
    rel: str
    score: float
    reason: str


def _rank_neighbors(root: Path, primary: Path, candidates: Iterable[Path]) -> list[Neighbor]:
    """
    Assign heuristic scores to candidate files based on proximity and role.
    """
    _norm_rel(root, primary)
    primary_dir = primary.parent
    primary_pkg = _pkg_root(primary)

    scored: list[Neighbor] = []
    for p in candidates:
        if not p.exists() or p.is_dir():
            continue
        # skip ignores
        if any(part in IGNORE_DIRS for part in p.parts):
            continue
        if not _is_textual(p):
            continue

        rel = _norm_rel(root, p)

        score = 0.0
        reason = []

        if rel.startswith("tests/") or "/tests/" in rel:
            score += WEIGHTS["tests"]
            reason.append("tests")

        if rel.endswith("registry.py") and "tools" in rel:
            score += WEIGHTS["registry"]
            reason.append("registry")

        if rel.endswith("schema.py") or "/specs/" in rel:
            score += WEIGHTS["spec"]
            reason.append("spec")

        if rel.startswith("docs/") or rel.endswith(".md"):
            score += WEIGHTS["docs"]
            reason.append("docs")

        # local proximity
        if primary_pkg and p.is_relative_to(primary_pkg):
            score += WEIGHTS["same_pkg"]
            reason.append("same_pkg")
        elif p.parent == primary_dir:
            score += WEIGHTS["same_dir"]
            reason.append("same_dir")

        if score == 0.0:
            # slight baseline for any python file near target
            if p.suffix in PY_EXTS:
                score = 0.2
                reason.append("nearby_py")

        scored.append(Neighbor(path=p, rel=rel, score=score, reason=",".join(reason) or "other"))

    scored.sort(key=lambda n: n.score, reverse=True)
    return scored


def _pkg_root(p: Path) -> Path | None:
    """
    Best-effort: walk upwards while __init__.py exists, return the top-most.
    """
    cur = p if p.is_dir() else p.parent
    top = None
    while True:
        init = cur / "__init__.py"
        if init.exists():
            top = cur
            if cur.parent == cur:
                break
            cur = cur.parent
            continue
        break
    return top


def _collect_candidates(root: Path, primary: Path) -> list[Path]:
    pats = default_neighbor_globs()
    cands = list(_iter_globs(root, pats))
    # Include conventional anchors even if not hit by globs
    for rel in CONVENTIONAL_ANCHORS:
        ap = (root / rel).resolve()
        if ap.exists():
            cands.append(ap)
    # Include siblings in the same dir as primary
    if primary.exists():
        for sib in primary.parent.glob("*"):
            if sib.is_file():
                cands.append(sib)
    # Dedup
    seen = set()
    uniq = []
    for p in cands:
        try:
            rp = p.resolve()
        except Exception:
            continue
        if rp in seen:
            continue
        seen.add(rp)
        uniq.append(rp)
    return uniq


_SIG_RE = re.compile(r"^\s*def\s+([a-zA-Z_]\w*)\s*\((.*?)\)\s*->?\s*.*?:", re.MULTILINE)
_CLASS_RE = re.compile(r"^\s*class\s+([A-Za-z_]\w*)\s*(\(|:)", re.MULTILINE)
_ASSERT_RE = re.compile(r"^\s*assert\s+.+$", re.MULTILINE)


def _high_signal_slice(text: str, *, limit: int) -> str:
    """
    Prefer:
      - top‑of‑file imports & constants block
      - function/class signatures (defs/classes)
      - test assertions
    Keep order; trim aggressively.
    """
    if len(text) <= limit:
        return text

    lines = text.splitlines()
    out: list[str] = []

    # 1) top header/import block (first ~80 lines)
    head = lines[: min(80, len(lines))]
    out += head

    # 2) defs/classes signatures (not bodies)
    sigs = []
    for m in _SIG_RE.finditer(text):
        sigs.append(m.group(0))
    for m in _CLASS_RE.finditer(text):
        sigs.append(m.group(0))
    if sigs:
        out.append("\n# --- signatures ---")
        out += sigs[:80]

    # 3) assertions (from tests)
    asserts = _ASSERT_RE.findall(text)
    if asserts:
        out.append("\n# --- assertions ---")
        out += asserts[:80]

    snippet = "\n".join(out)
    return _shorten(snippet, limit)


def gather_neighbor_snippets(repo_root: Path, file_rel: str) -> dict[str, str]:
    """
    Return a mapping of {rel_path: snippet_text} with hard caps respected.
    Ranking favors tests, registries, specs, then local proximity.
    """
    root = repo_root.resolve()
    primary = (root / file_rel).resolve()
    total_budget = MAX_TOTAL_BYTES

    # collect and rank
    cands = _collect_candidates(root, primary)
    ranked = _rank_neighbors(root, primary, cands)

    out: dict[str, str] = {}
    for nb in ranked:
        if len(out) >= MAX_FILES or total_budget <= 0:
            break
        try:
            raw = _read_text(nb.path, limit=MAX_BYTES_PER_FILE * 2)  # read a bit more; slice later
        except Exception:
            continue
        if not raw:
            continue
        # choose a high-signal slice
        snippet = _high_signal_slice(raw, limit=MAX_BYTES_PER_FILE)
        if not snippet.strip():
            continue

        # enforce overall budget
        budgeted = snippet[: min(len(snippet), total_budget)]
        total_budget -= len(budgeted)
        out[nb.rel] = budgeted

    return out

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\sandbox\deps.py =====
# systems/simula/code_sim/sandbox/deps.py
from __future__ import annotations

from .sandbox import DockerSandbox
from .seeds import seed_config


async def freeze_python() -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            [
                "bash",
                "-lc",
                "pip -q install pip-tools || true && pip-compile -q --generate-hashes -o requirements.txt || true && pip check || true",
            ],
        )
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "logs": out}


async def freeze_node() -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            ["bash", "-lc", "npm ci || true && npm audit --audit-level=high || true"],
        )
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "logs": out}


async def freeze_go() -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(["bash", "-lc", "go mod tidy || true && go mod verify || true"])
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "logs": out}

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\sandbox\profiles.py =====
# systems/simula/code_sim/sandbox/profiles.py
from __future__ import annotations

import os
from dataclasses import dataclass


@dataclass
class SandboxProfile:
    name: str
    xdist: bool
    nprocs: str
    mem_mb: int
    timeout_sec: int


def current_profile() -> SandboxProfile:
    return SandboxProfile(
        name=os.getenv("SIMULA_PROFILE", "balanced"),
        xdist=os.getenv("SIMULA_USE_XDIST", "1") != "0",
        nprocs=os.getenv("SIMULA_XDIST_PROCS", "auto"),
        mem_mb=int(os.getenv("SIMULA_MEM_MB", "4096")),
        timeout_sec=int(os.getenv("SIMULA_TIMEOUT", "900")),
    )

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\sandbox\quality.py =====
from __future__ import annotations

from typing import Any


class QualityMixin:
    async def run_cmd(self, args: list[str], timeout: int = 900) -> tuple[bool, dict[str, Any]]:
        """
        Implemented by DockerSandbox.session(); must execute args in the container
        and return (ok, logs_dict). If you already have `exec`, adapt to call it here.
        """
        raise NotImplementedError

    async def run_pytest(self, paths: list[str], timeout: int = 900) -> tuple[bool, dict[str, Any]]:
        return await self.run_cmd(["pytest", "-q", *paths], timeout=timeout)

    async def run_mypy(self, paths: list[str]) -> dict[str, Any]:
        ok, logs = await self.run_cmd(["mypy", "--hide-error-context", *paths], timeout=900)
        logs["ok"] = ok
        return logs

    async def run_ruff(self, paths: list[str]) -> dict[str, Any]:
        ok, logs = await self.run_cmd(["ruff", "check", *paths], timeout=600)
        logs["ok"] = ok
        return logs

    async def run_bandit(self, paths: list[str]) -> dict[str, Any]:
        ok, logs = await self.run_cmd(["bandit", "-q", "-r", *paths], timeout=600)
        logs["ok"] = ok
        return logs

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\sandbox\sandbox.py =====
# systems/simula/code_sim/sandbox/sandbox.py
# --- SENTINEL PATCH: host-path mount, python -c normalization/materialization, /app -> /workspace mapping ---
from __future__ import annotations

import asyncio
import os
import re
import shutil
import sys
from contextlib import asynccontextmanager
from dataclasses import dataclass, field, fields
from pathlib import Path
from typing import Any
from uuid import uuid4

from systems.simula.config import settings

# Host path of the repo (set via compose): e.g. /mnt/d/EcodiaOS (WSL2), /Users/me/EcodiaOS (macOS), /home/me/EcodiaOS (Linux)
HOST_REPO = os.getenv("SIMULA_HOST_APP_DIR")  # absolute host path (preferred)
# Container view of the repo (API container): usually /app
REPO_ROOT = Path(settings.repo_root).resolve()
# Scratch dir lives under the repo so the host daemon sees it through the same bind mount
SIMULA_META_DIR = REPO_ROOT / ".simula"


@dataclass
class SandboxConfig:
    mode: str = "docker"
    image: str = "ecodiaos:dev"
    timeout_sec: int = 1800
    workdir: str = "."
    env_allow: list[str] = field(default_factory=list)
    env_set: dict[str, str] = field(default_factory=dict)
    pip_install: list[str] = field(default_factory=list)
    # Docker specifics
    cpus: str = "2.0"
    memory: str = "4g"
    network: str | None = "bridge"
    mount_rw: list[str] = field(default_factory=list)


# --------- helpers ---------
def _normalize_c_code(code: str) -> str:
    s = code.strip()
    # split on any semicolon that is not inside quotes (simple heuristic)
    parts = [p.strip() for p in s.split(';')]
    s = "\n".join(p for p in parts if p)

    def _expand_block(pattern: str, text: str) -> str:
        return re.sub(
            pattern,
            lambda m: f"{m.group('pre')}\n    {m.group('body')}",
            text,
            flags=re.MULTILINE,
        )

    s = _expand_block(r"^(?P<pre>\s*if [^:]+:\s*)(?P<body>\S.+)$", s)
    s = _expand_block(r"^(?P<pre>\s*with [^:]+:\s*)(?P<body>\S.+)$", s)
    if not s.endswith("\n"):
        s += "\n"
    return s


# --------- sessions ---------
class BaseSession:
    def __init__(self, cfg: SandboxConfig):
        self.cfg = cfg
        self.repo = REPO_ROOT
        self.workdir = (self.repo / cfg.workdir).resolve()
        SIMULA_META_DIR.mkdir(parents=True, exist_ok=True)
        self.tmp = (SIMULA_META_DIR / f"sbx-{uuid4().hex}").resolve()
        self.tmp.mkdir(parents=True, exist_ok=True)

    @property
    def python_exe(self) -> str:
        raise NotImplementedError

    async def _run_tool(self, cmd: list[str], timeout: int | None = None) -> dict[str, Any]:
        raise NotImplementedError

    async def apply_unified_diff(self, diff: str, threeway: bool = False) -> bool:
        if not diff.strip():
            return True
        patch_path = self.tmp / "patch.diff"
        patch_path.write_text(diff, encoding="utf-8")
        args = ["git", "apply"]
        if threeway:
            args.append("-3")
        args += ["--whitespace=fix", str(patch_path)]
        out = await self._run_tool(args)
        return out.get("returncode", 1) == 0

    async def rollback_unified_diff(self, diff: str) -> bool:
        if not diff.strip():
            return True
        patch_path = self.tmp / "patch.diff"
        patch_path.write_text(diff, encoding="utf-8")
        out = await self._run_tool(["git", "apply", "-R", "--whitespace=fix", str(patch_path)])
        return out.get("returncode", 1) == 0

    async def run_pytest(self, paths: list[str], timeout: int = 900) -> tuple[bool, dict[str, Any]]:
        await self._ensure_tool("pytest", "pytest==8.2.0")
        cmd = [self.python_exe, "-m", "pytest", "-q", "--maxfail=1", *paths]
        out = await self._run_tool(cmd, timeout=timeout)
        return out.get("returncode", 1) == 0, out

    async def run_pytest_select(self, paths: list[str], k_expr: str, timeout: int = 900) -> tuple[bool, dict[str, Any]]:
        await self._ensure_tool("pytest", "pytest==8.2.0")
        cmd = [self.python_exe, "-m", "pytest", "-q", "--maxfail=1", *paths]
        if k_expr:
            cmd.extend(["-k", k_expr])
        out = await self._run_tool(cmd, timeout=timeout)
        return out.get("returncode", 1) == 0, out

    async def run_ruff(self, paths: list[str]) -> dict[str, Any]:
        await self._ensure_tool("ruff", "ruff==0.5.6")
        return await self._run_tool([self.python_exe, "-m", "ruff", "check", *paths])

    async def run_mypy(self, paths: list[str]) -> dict[str, Any]:
        await self._ensure_tool("mypy", "mypy==1.10.0")
        return await self._run_tool([self.python_exe, "-m", "mypy", "--pretty", *paths])

    def __del__(self):
        try:
            shutil.rmtree(self.tmp, ignore_errors=True)
        except Exception:
            pass


class DockerSession(BaseSession):
    @property
    def python_exe(self) -> str:
        # Use system python in the job image; tools installed on-demand via pip (no venv).
        return "python"

    def _host_repo_src(self) -> str:
        """
        Source for -v <src>:/workspace. Prefer SIMULA_HOST_APP_DIR.
        If unset, fall back to REPO_ROOT path (works only when Docker daemon can see it).
        """
        return HOST_REPO or REPO_ROOT.as_posix()

    def _docker_base_cmd(self) -> list[str]:
        args = [
            "docker", "run", "--rm", "--init",
            "--cpus", str(self.cfg.cpus),
            "--memory", str(self.cfg.memory),
            "--workdir", f"/workspace/{self.cfg.workdir}",
            "-v", f"{self._host_repo_src()}:/workspace:rw",
        ]
        if self.cfg.network:
            args += ["--network", self.cfg.network]
        # Pass through explicit env vars
        for k, v in self.cfg.env_set.items():
            args += ["-e", f"{k}={v}"]
        # Helpful defaults inside the job container
        args += ["-e", "PYTHONDONTWRITEBYTECODE=1", "-e", "SIMULA_REPO_ROOT=/workspace"]
        args.append(self.cfg.image)
        return args

    def _remap_args(self, args: list[str]) -> list[str]:
        """Map any /app/... paths (API container view) to /workspace/... (job container view)."""
        mapped: list[str] = []
        for a in args:
            if isinstance(a, str) and a.startswith("/app/"):
                mapped.append("/workspace/" + a[len("/app/"):])
            else:
                mapped.append(a)
        return mapped

    def _materialize_python_c(self, cmd: list[str]) -> list[str]:
        """
        If the command is `python -c <code> [args...]`, write <code> to a temp script
        under REPO_ROOT/.simula/sbx-... so it's visible to the daemon via the same bind,
        then run it as `python /workspace/.simula/.../inline_script.py ...`.
        """
        if len(cmd) >= 3 and cmd[0] == "python" and cmd[1] == "-c":
            code = _normalize_c_code(cmd[2])
            script_path = self.tmp / "inline_script.py"
            script_path.write_text(code, encoding="utf-8")
            container_script = f"/workspace/.simula/{self.tmp.name}/inline_script.py"
            return ["python", container_script, *cmd[3:]]
        return cmd

    async def _ensure_tool(self, module_name: str, pip_name: str) -> None:
        """Install missing tools into the job container's system Python."""
        probe = await self._run_tool(["python", "-c", f"import {module_name}"], timeout=30)
        if probe.get("returncode", 1) == 0:
            return
        # Install into system Python (no venv) to avoid FS/permission hassles on the bind mount.
        await self._run_tool(
            ["python", "-m", "pip", "install", "-U", "pip", "setuptools", "wheel"],
            timeout=600,
        )
        await self._run_tool(
            ["python", "-m", "pip", "install", "-U", pip_name],
            timeout=600,
        )

    async def _run_tool(self, cmd: list[str], timeout: int | None = None) -> dict[str, Any]:
        # 1) transform python -c to a real file (with normalization)
        cmd = self._materialize_python_c(cmd)
        # 2) remap any /app paths to /workspace
        cmd = self._remap_args(cmd)
        full_cmd = self._docker_base_cmd() + cmd

        proc = await asyncio.create_subprocess_exec(
            *full_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE,
        )
        try:
            stdout_b, stderr_b = await asyncio.wait_for(proc.communicate(), timeout=timeout or self.cfg.timeout_sec)
        except asyncio.TimeoutError:
            proc.kill()
            return {"returncode": 124, "stdout": "", "stderr": "Process timed out."}

        out = {
            "returncode": proc.returncode,
            "stdout": stdout_b.decode("utf-8", "replace"),
            "stderr": stderr_b.decode("utf-8", "replace"),
        }
        if out["returncode"] != 0:
            sys.stderr.write(
                f"[DockerSession] rc={out['returncode']}\n"
                f"CMD: {' '.join(full_cmd)}\n"
                f"STDOUT:\n{out['stdout']}\nSTDERR:\n{out['stderr']}\n"
            )
        return out


class LocalSession(BaseSession):
    @property
    def python_exe(self) -> str:
        win = self.repo / ".venv" / "Scripts" / "python.exe"
        nix = self.repo / ".venv" / "bin" / "python"
        return str(win if sys.platform == "win32" else nix)

    async def _run_tool(self, cmd: list[str], timeout: int | None = None) -> dict[str, Any]:
        proc = await asyncio.create_subprocess_exec(
            *cmd, cwd=self.workdir, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE,
        )
        try:
            stdout_b, stderr_b = await asyncio.wait_for(proc.communicate(), timeout=timeout or self.cfg.timeout_sec)
        except asyncio.TimeoutError:
            proc.kill()
            return {"returncode": 124, "stdout": "", "stderr": "Process timed out."}
        return {
            "returncode": proc.returncode,
            "stdout": stdout_b.decode("utf-8", "replace"),
            "stderr": stderr_b.decode("utf-8", "replace"),
        }


class DockerSandbox:
    def __init__(self, cfg_dict: dict[str, object]):
        known = {f.name for f in fields(SandboxConfig)}
        self.cfg = SandboxConfig(**{k: v for k, v in cfg_dict.items() if k in known})

    @asynccontextmanager
    async def session(self):
        mode = (os.getenv("SIMULA_SANDBOX_MODE") or self.cfg.mode or "docker").lower()
        if mode == "local":
            sess = LocalSession(self.cfg)
        elif mode == "docker":
            sess = DockerSession(self.cfg)
        else:
            raise NotImplementedError(f"Unsupported sandbox mode: {mode}")
        try:
            yield sess
        finally:
            pass

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\sandbox\seeds.py =====
# systems/simula/code_sim/sandbox/seeds.py
# --- PROJECT SENTINEL UPGRADE ---
from __future__ import annotations

import sys

from systems.simula.config import settings

# Define the required toolchain packages. These are now managed centrally.
REQUIRED_PIP: list[str] = [
    "pytest==8.2.0",
    "ruff==0.5.6",
    "mypy==1.10.0",
    "bandit==1.7.9",
    "pytest-xdist",
    "black",
]


def seed_config() -> dict[str, object]:
    """
    Derive the sandbox configuration directly from the central settings singleton.
    """
    sbx = settings.sandbox
    return {
        "mode": sbx.mode,
        "image": sbx.image,
        "timeout_sec": sbx.timeout_sec,
        "cpus": sbx.cpus,
        "memory": sbx.memory,
        "network": sbx.network,
        "workdir": ".",  # Always operate from the repo root
        "env_allow": ["PYTHONPATH"],
        "env_set": {
            "PYTHONDONTWRITEBYTECODE": "1",
            "SIMULA_REPO_ROOT": "/workspace",  # The path inside the container
        },
        # Persist these directories across runs for caching and performance
        "mount_rw": [".simula", ".venv", ".mypy_cache", ".pytest_cache"],
        "pip_install": sbx.pip_install or REQUIRED_PIP,
    }


async def ensure_toolchain(session) -> dict[str, str]:
    """
    Ensures a persistent toolchain in the repo's .venv/ directory.
    This logic is now robust and works for both Docker and Local sessions.
    """
    all_pkgs = session.cfg.pip_install

    # This Python script is executed inside the sandbox to bootstrap the environment
    code = (
        "import os, sys, subprocess, pathlib\n"
        "root = pathlib.Path('.').resolve()\n"
        "venv = root / '.venv'\n"
        "py_exe = venv / ('Scripts/python.exe' if os.name == 'nt' else 'bin/python')\n"
        "def run(cmd): subprocess.check_call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n"
        "if not py_exe.exists():\n"
        "    run([sys.executable, '-m', 'venv', str(venv)])\n"
        "run([str(py_exe), '-m', 'pip', 'install', '-U', 'pip', 'setuptools', 'wheel'])\n"
        f"run([str(py_exe), '-m', 'pip', 'install', '-U'] + {repr(all_pkgs)})\n"
        "print('VERS', 'python', '.'.join(map(str, sys.version_info[:3])))\n"
        "try:\n"
        "    from importlib.metadata import version as v\n"
        "except ImportError:\n"
        "    from importlib_metadata import version as v\n"
        "for pkg in ['pytest', 'ruff', 'mypy', 'bandit', 'black']:\n"
        "    try: print('VERS', pkg, v(pkg))\n"
        "    except Exception: print('VERS', pkg, 'missing')\n"
    )

    out = await session._run_tool([sys.executable, "-c", code], timeout=1200)

    # Extract versions from the structured stdout
    stdout = out.get("stdout", "")
    versions: dict[str, str] = {}
    for line in stdout.splitlines():
        if line.startswith("VERS "):
            parts = line.split(None, 2)
            if len(parts) == 3:
                _, name, value = parts
                versions[name] = value.strip()

    return versions

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\sandbox\snapshots.py =====
# systems/simula/code_sim/sandbox/snapshots.py
from __future__ import annotations

import time
from dataclasses import dataclass

from .sandbox import DockerSandbox
from .seeds import seed_config


@dataclass
class Snapshot:
    tag: str
    created_ts: float


async def create_snapshot(tag_prefix: str = "simula") -> Snapshot:
    """
    Create a lightweight workspace snapshot (git commit-ish) inside the sandbox.
    Caller can store the `tag` and later call `restore_snapshot(tag)`.
    """
    ts = time.time()
    tag = f"{tag_prefix}-{int(ts)}"
    async with DockerSandbox(seed_config()).session() as sess:
        await sess._run_tool(["bash", "-lc", "git add -A || true"])
        await sess._run_tool(["bash", "-lc", f"git commit -m {tag!r} || true"])
        await sess._run_tool(["bash", "-lc", f"git tag -f {tag} || true"])
    return Snapshot(tag=tag, created_ts=ts)


async def restore_snapshot(tag: str) -> tuple[bool, str]:
    """
    Restore a previous snapshot tag; returns (ok, message).
    """
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            ["bash", "-lc", f"git reset --hard {tag} && git clean -fd || true"],
        )
        ok = out.get("returncode", 0) == 0
        return ok, (out.get("stdout") or out.get("stderr") or "").strip()

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\security\secret_scan.py =====
# systems/simula/code_sim/security/secret_scan.py
from __future__ import annotations

import re
from dataclasses import dataclass


@dataclass
class Finding:
    path: str
    line: int
    snippet: str
    rule: str


AWS = re.compile(r"AKIA[0-9A-Z]{16}")
GH_PAT = re.compile(r"ghp_[A-Za-z0-9]{36}")
GENERIC_KEY = re.compile(r"(secret|token|api[_-]?key)\s*[:=]\s*['\"][A-Za-z0-9_\-]{16,}['\"]", re.I)


def scan_text(path: str, text: str) -> list[Finding]:
    out: list[Finding] = []
    for i, ln in enumerate(text.splitlines(), start=1):
        if AWS.search(ln):
            out.append(Finding(path, i, ln.strip()[:120], "aws_key"))
        if GH_PAT.search(ln):
            out.append(Finding(path, i, ln.strip()[:120], "gh_pat"))
        if GENERIC_KEY.search(ln):
            out.append(Finding(path, i, ln.strip()[:120], "generic_key"))
    return out


def scan_diff_for_secrets(diff_text: str) -> dict[str, object]:
    findings: list[Finding] = []
    cur = ""
    for ln in diff_text.splitlines():
        if ln.startswith("+++ b/"):
            cur = ln[6:].strip()
        if ln.startswith("+") and not ln.startswith("+++"):
            findings.extend(scan_text(cur or "UNKNOWN", ln[1:]))
    return {
        "ok": len(findings) == 0,
        "findings": [f.__dict__ for f in findings],
        "summary": {"count": len(findings)},
    }

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\specs\minimize_trace.py =====
# systems/simula/code_sim/spec/minimize_trace.py
from __future__ import annotations

import re


def minimize_pytest_stdout(stdout: str) -> list[tuple[str, int]]:
    """
    Return list of (file,line) likely causing failure, de-noising pytest output.
    """
    loc = []
    pat = re.compile(r"^(.+?):(\d+): in .+$")
    for ln in (stdout or "").splitlines():
        m = pat.match(ln.strip())
        if m:
            f, n = m.group(1), int(m.group(2))
            if "/site-packages/" in f:
                continue
            loc.append((f, n))
    return loc[:8]

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\specs\oracles.py =====
# systems/simula/code_sim/spec/oracles.py
from __future__ import annotations

import ast
from pathlib import Path


def _extract_examples_from_docstring(doc: str) -> list[str]:
    """
    Very light doctest-style example extractor: lines starting with >>> become asserts.
    """
    ex = []
    for ln in (doc or "").splitlines():
        if ln.strip().startswith(">>>"):
            ex.append(ln.strip()[3:].strip())
    return ex


def generate_oracle_tests(py_file: str, *, max_per_fn: int = 3) -> dict[str, object]:
    """
    Parse a module, collect docstring examples & type-hint oracles, and emit a test string.
    """
    p = Path(py_file)
    if not p.exists():
        return {"status": "error", "reason": "file not found"}

    text = p.read_text(encoding="utf-8", errors="ignore")
    try:
        tree = ast.parse(text)
    except Exception as e:
        return {"status": "error", "reason": f"parse failed: {e!r}"}

    parts: list[str] = ["# Auto-generated by Simula (oracle tests)", "import pytest", ""]
    count = 0

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            doc = ast.get_docstring(node) or ""
            examples = _extract_examples_from_docstring(doc)[:max_per_fn]
            if not examples:
                continue
            parts.append(f"def test_oracle_{node.name}():")
            for ex in examples:
                # If example is an expression, assert it truthy; otherwise just run it.
                if any(op in ex for op in ("==", "!=", ">", "<", " in ", " is ")):
                    parts.append(f"    assert {ex}")
                else:
                    parts.append(f"    {ex}")
            parts.append("")
            count += 1

    if count == 0:
        return {"status": "noop", "reason": "no examples found"}

    return {"status": "success", "tests": "\n".join(parts), "cases": count}

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\specs\synthesize.py =====
# systems/simula/code_sim/spec/synthesize.py  (v2)
from __future__ import annotations

import re
import time
import uuid
from pathlib import Path

from .minimize_trace import minimize_pytest_stdout
from .oracles import generate_oracle_tests


def _sanitize(s: str) -> str:
    return re.sub(r"[^A-Za-z0-9_]+", "_", s).strip("_") or "case"


def write_tests_from_stdout(
    pytest_stdout: str,
    *,
    suite_name: str = "acceptance",
) -> dict[str, object]:
    locs = minimize_pytest_stdout(pytest_stdout)
    if not locs:
        return {"status": "noop", "note": "no failure loci found"}

    ts = int(time.time())
    fname = f"tests/generated/test_{_sanitize(suite_name)}_{ts}_{uuid.uuid4().hex[:6]}.py"
    p = Path(fname)
    p.parent.mkdir(parents=True, exist_ok=True)

    lines = [
        "import pytest",
        "",
        f"# Auto-generated by Simula at {ts}",
        "",
    ]

    for i, (file, line) in enumerate(locs, start=1):
        doc = f"{file}:{line}"
        lines.append(f"def test_acceptance_{i}():")
        lines.append(f'    """Autogenerated acceptance: {doc}"""')
        lines.append("    # TODO: implement minimal reproducer; start from locus above")
        lines.append("    assert True  # placeholder")
        lines.append("")

    p.write_text("\n".join(lines) + "\n", encoding="utf-8")
    return {"status": "success", "file": fname, "cases": len(locs)}


def write_oracle_tests(py_file: str) -> dict[str, object]:
    out = generate_oracle_tests(py_file)
    if out.get("status") != "success":
        return out
    ts = int(time.time())
    fname = f"tests/generated/test_oracles_{_sanitize(Path(py_file).stem)}_{ts}.py"
    Path(fname).parent.mkdir(parents=True, exist_ok=True)
    Path(fname).write_text(out["tests"], encoding="utf-8")
    return {"status": "success", "file": fname, "cases": out.get("cases", 0)}

# ===== FILE: D:\EcodiaOS\systems\simula\code_sim\utils\repo_features.py =====
from __future__ import annotations

import ast
import subprocess
from pathlib import Path

REPO = Path("/app")


def file_degree(rel: str, max_files: int = 20000) -> int:
    """
    Rough import-degree: count files that import this module or are imported by it.
    """
    rel_p = REPO / rel
    if not rel_p.exists() or not rel.endswith(".py"):
        return 0
    name = rel[:-3].replace("/", ".")
    deg = 0
    scanned = 0
    for p in REPO.rglob("*.py"):
        scanned += 1
        if scanned > max_files:
            break
        try:
            tree = ast.parse(p.read_text(encoding="utf-8", errors="ignore"))
        except Exception:
            continue
        for n in ast.walk(tree):
            if isinstance(n, ast.Import):
                for a in n.names:
                    if a.name == name:
                        deg += 1
                        break
            elif isinstance(n, ast.ImportFrom) and n.module:
                if n.module == name or n.module.startswith(name + "."):
                    deg += 1
                    break
    return deg


def file_churn(rel: str, days: int = 180) -> int:
    """Number of commits touching this file in last N days."""
    try:
        out = subprocess.run(
            ["git", "log", f"--since={days}.days", "--pretty=oneline", "--", rel],
            cwd=str(REPO),
            capture_output=True,
            text=True,
            timeout=30,
        )
        if out.returncode != 0:
            return 0
        return len([l for l in out.stdout.splitlines() if l.strip()])
    except Exception:
        return 0


def plan_entropy(plan: list[dict]) -> float:
    """Spread of plan across dirs: simple entropy proxy in [0,1]."""
    from collections import Counter

    if not plan:
        return 0.0
    dirs = [(p.get("path") or "").split("/", 1)[0] for p in plan if p.get("path")]
    c = Counter(dirs)
    total = sum(c.values())
    import math

    H = -sum((v / total) * math.log2(v / total) for v in c.values() if v > 0)
    # normalize by max entropy log2(k)
    k = len(c)
    maxH = math.log2(k) if k > 1 else 1.0
    return float(min(1.0, H / (maxH or 1.0)))


def features_for_file(job_meta: dict, file_plan: dict) -> dict:
    rel = file_plan.get("path", "")
    return {
        "degree": file_degree(rel),
        "churn": file_churn(rel),
        "plan_entropy": plan_entropy(job_meta.get("plan", [])),
    }

# ===== FILE: D:\EcodiaOS\systems\simula\config\__init__.py =====
# systems/simula/config/__init__.py
# --- PROJECT SENTINEL UPGRADE ---
from __future__ import annotations

import json
import os
import subprocess
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml
from pydantic import Field, field_validator, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

# --- Helpers -----------------------------------------------------------------


def _normalize_path_string(p: str | Path) -> str:
    return str(Path(p).resolve()).replace("\\", "/")


def _git_root_cwd() -> str | None:
    try:
        out = subprocess.check_output(
            ["git", "rev-parse", "--show-toplevel"],
            stderr=subprocess.DEVNULL,
        )
        return _normalize_path_string(out.decode().strip())
    except Exception:
        return None


def _default_repo_root() -> str:
    return (
        os.getenv("SIMULA_REPO_ROOT")
        or os.getenv("SIMULA_WORKSPACE_ROOT")
        or _git_root_cwd()
        or "/ecodiaos"
    )


def _optional_json_or_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        return {}
    try:
        text = path.read_text(encoding="utf-8")
        if path.suffix.lower() in (".yaml", ".yml") and yaml:
            data = yaml.safe_load(text) or {}
        else:
            data = json.loads(text)
        return data if isinstance(data, dict) else {}
    except Exception:
        return {}


# --- Nested Settings Groups --------------------------------------------------


class GateSettings(BaseSettings):
    """Configuration for quality gates, formerly from gates.py."""

    model_config = SettingsConfigDict(env_prefix="SIMULA_GATE_")
    require_static_clean: bool = True
    require_tests_green: bool = True
    min_delta_cov: float = 0.0
    run_safety: bool = True
    pr_open: bool = True
    pr_draft: bool = True
    pr_labels: list[str] = Field(default_factory=lambda: ["simula", "auto"])

    @property
    def autopr_enabled(self) -> bool:
        return self.pr_open


class SandboxSettings(BaseSettings):
    """Sandbox runtime (Docker or Local)."""

    model_config = SettingsConfigDict(env_prefix="SIMULA_SANDBOX_")
    mode: str = "docker"
    image: str = "eodiaos:dev"
    timeout_sec: int = 1800
    cpus: str = "2.0"
    memory: str = "4g"
    network: str | None = "bridge"
    pip_install: list[str] = Field(default_factory=list)


class TimeoutSettings(BaseSettings):
    """Tool-specific timeouts."""

    model_config = SettingsConfigDict(env_prefix="SIMULA_TIMEOUT_")
    tool_default: int = 90
    test: int = 1800
    llm: int = 120


# --- Top-level Settings Class ------------------------------------------------


class SimulaSettings(BaseSettings):
    """Global Simula configuration (single source of truth)."""

    model_config = SettingsConfigDict(env_prefix="SIMULA_")

    repo_root: str = Field(default_factory=_default_repo_root)
    artifacts_root: str = Field(default="")

    # max_turns: int = 15  # <<< DELETED
    max_observation_length: int = 4000
    test_mode: bool = False

    sandbox: SandboxSettings = Field(default_factory=SandboxSettings)
    timeouts: TimeoutSettings = Field(default_factory=TimeoutSettings)
    gates: GateSettings = Field(default_factory=GateSettings)
    eos_policy_paths: list[str] | None = None

    @field_validator("test_mode", mode="before")
    @classmethod
    def _parse_test_mode(cls, v):
        if v is None:
            v = os.getenv("SIMULA_TEST_MODE", "0")
        return str(v).lower() in ("1", "true", "yes", "on")

    @model_validator(mode="after")
    def _harmonize_and_overlay(self):
        # 1. Normalize core paths
        if not self.artifacts_root:
            self.artifacts_root = str(Path(self.repo_root) / ".simula")
        self.repo_root = _normalize_path_string(self.repo_root)
        self.artifacts_root = _normalize_path_string(self.artifacts_root)

        # 2. Overlay team defaults from config files
        config_yaml = Path(self.repo_root) / ".simula" / "config.yaml"
        gates_json = Path(self.repo_root) / ".simula" / "gates.json"

        for cfg_path in [config_yaml, gates_json]:
            overlay = _optional_json_or_yaml(cfg_path)
            for key, value in overlay.items():
                if hasattr(self, key):
                    current_attr = getattr(self, key)
                    if isinstance(current_attr, BaseSettings) and isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if hasattr(current_attr, sub_key):
                                setattr(current_attr, sub_key, sub_value)
                    else:
                        setattr(self, key, value)

        # 3. Ensure critical directories exist
        try:
            Path(self.artifacts_root).mkdir(parents=True, exist_ok=True)
            for sub in ("runs", "logs", "cache", "policy"):
                (Path(self.artifacts_root) / sub).mkdir(parents=True, exist_ok=True)
        except OSError:
            pass

        return self


# --- Singleton Instance ---
settings = SimulaSettings()
# ===== FILE: D:\EcodiaOS\systems\simula\config\loader.py =====
# systems/simula/config/loader.py
from __future__ import annotations

from dataclasses import dataclass

from . import settings  # unified source


@dataclass
class SimulaConfig:
    delta_cov_min: float
    min_mutation_score: float
    use_xdist: bool
    enable_cache: bool
    eos_policy_paths: list[str] | None


def load_config() -> SimulaConfig:
    return SimulaConfig(
        delta_cov_min=settings.delta_cov_min,
        min_mutation_score=settings.min_mutation_score,
        use_xdist=settings.use_xdist,
        enable_cache=settings.enable_cache,
        eos_policy_paths=settings.eos_policy_paths,
    )

# ===== FILE: D:\EcodiaOS\systems\simula\examples\greeter.py =====
# greeter.py

def greet(name: str) -> str:
    """Returns a greeting message for the given name."""
    return f'Hello, {name}!'


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Greet a person.')
    parser.add_argument('--name', type=str, required=True, help='Name of the person to greet')
    args = parser.parse_args()
    print(greet(args.name))
# ===== FILE: D:\EcodiaOS\systems\simula\format\autoformat.py =====
# systems/simula/format/autoformat.py
from __future__ import annotations

from pathlib import Path

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config


async def autoformat_changed(paths: list[str]) -> dict[str, object]:
    """
    Best-effort, language-aware formatting for changed files.
    """
    exts = {Path(p).suffix for p in paths}
    cmds = []
    if any(e in {".py"} for e in exts):
        cmds += [
            "ruff check . --fix || true",
            "python -m black . || true",
            "python -m isort . || true",
        ]
    if any(e in {".js", ".jsx", ".ts", ".tsx", ".json", ".md", ".css"} for e in exts):
        cmds += ["npx -y prettier -w . || true"]
    if any(e in {".go"} for e in exts):
        cmds += ["gofmt -w . || true"]
    if any(e in {".java"} for e in exts):
        cmds += ["./gradlew spotlessApply || true || true"]
    if any(e in {".rs"} for e in exts):
        cmds += ["cargo fmt || true"]
    logs = []
    async with DockerSandbox(seed_config()).session() as sess:
        for cmd in cmds:
            logs.append(await sess._run_tool(["bash", "-lc", cmd]))
    return {"status": "success", "commands": cmds, "logs": logs}

# ===== FILE: D:\EcodiaOS\systems\simula\git\pr_annotations.py =====
# systems/simula/integrations/github/pr_annotations.py
from __future__ import annotations

import os
from typing import Any

import httpx

GITHUB_API = "https://api.github.com"


def _auth_headers() -> dict[str, str]:
    token = os.getenv("GITHUB_TOKEN") or os.getenv("GH_TOKEN") or os.getenv("GITHUB_PAT") or ""
    if not token:
        raise RuntimeError("Missing GITHUB_TOKEN/GH_TOKEN in env")
    return {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}


def format_proposal_comment(proposal: dict[str, Any]) -> str:
    ev = proposal.get("evidence") or {}
    cov = ev.get("coverage_delta") or {}
    hyg = ev.get("hygiene") or {}
    risk = ev.get("risk") or {}  # if you attach risk estimate later
    lines = []
    lines.append(f"### 🤖 Simula Proposal `{proposal.get('proposal_id', '?')}`")
    lines.append("")
    lines.append("**Hygiene**")
    lines.append(f"- static: `{hyg.get('static', '?')}`")
    lines.append(f"- tests: `{hyg.get('tests', '?')}`")
    if "pct_changed_covered" in cov:
        lines.append(f"- Δcoverage (changed lines): **{cov.get('pct_changed_covered', 0):.2f}%**")
    if risk:
        lines.append(f"- Risk: **{risk.get('grade', '?')}** ({risk.get('risk', '?')})")
    # Impact summary
    imp = ev.get("impact") or {}
    if imp:
        k = imp.get("k_expr") or ""
        files = imp.get("changed") or []
        lines.append("")
        lines.append("**Impact**")
        if k:
            lines.append(f"- focus: `{k}`")
        if files:
            sample = ", ".join(files[:8])
            more = "" if len(files) <= 8 else f" (+{len(files) - 8} more)"
            lines.append(f"- files: {sample}{more}")
    lines.append("")
    lines.append("<sub>Generated by Simula/Qora</sub>")
    return "\n".join(lines)


async def post_pr_comment(repo: str, pr_number: int, body: str) -> dict[str, Any]:
    headers = _auth_headers()
    url = f"{GITHUB_API}/repos/{repo}/issues/{pr_number}/comments"
    async with httpx.AsyncClient(timeout=30.0) as http:
        r = await http.post(url, headers=headers, json={"body": body})
        r.raise_for_status()
        return r.json()


async def set_commit_status(
    repo: str,
    sha: str,
    state: str,
    *,
    context: str = "simula/hygiene",
    description: str = "",
    target_url: str | None = None,
) -> dict[str, Any]:
    """
    state: 'error'|'failure'|'pending'|'success'
    """
    headers = _auth_headers()
    url = f"{GITHUB_API}/repos/{repo}/statuses/{sha}"
    payload = {"state": state, "context": context}
    if description:
        payload["description"] = description[:140]
    if target_url:
        payload["target_url"] = target_url
    async with httpx.AsyncClient(timeout=30.0) as http:
        r = await http.post(url, headers=headers, json=payload)
        r.raise_for_status()
        return r.json()

# ===== FILE: D:\EcodiaOS\systems\simula\git\rebase.py =====
# systems/simula/git/rebase.py
from __future__ import annotations

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config


async def rebase_diff_onto_branch(
    diff_text: str,
    *,
    base: str = "origin/main",
) -> dict[str, object]:
    """
    Try to apply the diff on top of latest base via 3-way; return conflicts if any.
    """
    async with DockerSandbox(seed_config()).session() as sess:
        await sess._run_tool(["bash", "-lc", "git fetch --all --tags || true"])
        await sess._run_tool(
            [
                "bash",
                "-lc",
                f"git checkout -B simula-rebase {base} || git checkout -B simula-rebase || true",
            ],
        )
        ok = await sess.apply_unified_diff(diff_text, threeway=True)
        if ok:
            return {"status": "success", "conflicts": []}
        # try to detect conflicts
        out = await sess._run_tool(["bash", "-lc", "git diff --name-only --diff-filter=U || true"])
        files = (out.get("stdout") or "").strip().splitlines()
        return {"status": "conflicts", "conflicts": files}

# ===== FILE: D:\EcodiaOS\systems\simula\learning\__init__.py =====
from __future__ import annotations

__all__ = [
    "AdviceEngine",
    "ErrorEventIngest",
    "RetrieveResult",
]

from .advice_engine import AdviceEngine
from .schemas import ErrorEventIngest, RetrieveResult

# ===== FILE: D:\EcodiaOS\systems\simula\learning\advice_engine.py =====
from __future__ import annotations

import json
import logging
import time
from typing import Any, Dict, List, Optional, Sequence

from core.utils.neo.cypher_query import cypher_query
from core.utils.llm_gateway_client import call_llm_service
from core.prompting.orchestrator import build_prompt

# Use the shared embedding helper (3072-d Gemini)
from core.llm.embeddings_gemini import get_embedding

from .schemas import ErrorEventIngest

log = logging.getLogger(__name__)

# ──────────────────────────────────────────────────────────────────────────────
# Tunables (env/pluggable via future settings.py)
# ──────────────────────────────────────────────────────────────────────────────
EMBED_MODEL = "gemini-embedding-001"
SIM_T1_CLUSTER = 0.84
SIM_T2_MERGE = 0.80
PROMOTE_MIN_T1 = 2
PROMOTE_MIN_T2 = 2
TOPK_INJECT = 6
HALF_LIFE_DAYS = 30


# ──────────────────────────────────────────────────────────────────────────────
# Embedding helpers (doc vs. query + multi-query compose)
# ──────────────────────────────────────────────────────────────────────────────
async def _embed_doc(text: str) -> List[float]:
    """Embedding for stored content (Advice nodes)."""
    return await get_embedding(text, task_type="RETRIEVAL_DOCUMENT", model=EMBED_MODEL)

async def _embed_query(q: str | Sequence[str]) -> List[float]:
    """
    Embedding for retrieval queries; supports a single string OR a list of strings.
    If a list is provided, returns the element-wise mean (robust multi-hint query).
    """
    if isinstance(q, str):
        return await get_embedding(q, task_type="RETRIEVAL_QUERY", model=EMBED_MODEL)

    # multi-query: mean pooling (element-wise)
    vecs: List[List[float]] = []
    for part in q:
        if not part:
            continue
        v = await get_embedding(part, task_type="RETRIEVAL_QUERY", model=EMBED_MODEL)
        vecs.append(v)
    if not vecs:
        # fallback: embed an empty query safely
        return await get_embedding("", task_type="RETRIEVAL_QUERY", model=EMBED_MODEL)

    dim = len(vecs[0])
    acc = [0.0] * dim
    for v in vecs:
        # defensive: ensure consistent dims
        if len(v) != dim:
            raise RuntimeError(f"[ADVICE] Inconsistent embedding dims; expected {dim}, got {len(v)}")
        for i in range(dim):
            acc[i] += v[i]
    n = float(len(vecs))
    return [x / n for x in acc]


class AdviceEngine:
    """Core ingest→cluster→synthesize→merge→retrieve→reward/decay engine."""

    def __init__(self, embed_model: str = EMBED_MODEL):
        self.embed_model = embed_model

    # ------------------------------------------------------------------ T1 CAPTURE

    async def ingest_error(self, e: ErrorEventIngest) -> str:
        """Create a level-1 Advice node from a concrete error event (T1)."""
        text = self._compose_t1_text(e)
        vec = await _embed_doc(text)
        rows = await cypher_query(
            """
            WITH $e AS e, $vec AS emb
            MERGE (ev:ErrorEvent {id: e.turn_id})
              ON CREATE SET ev += e, ev.created_at = timestamp()
            CREATE (a:Advice {
                id: randomUUID(),
                level: 1,
                kind: 'code_advice',
                text: e.message,
                checklist: [],
                donts: [],
                validation: [],
                scope: [x IN [e.symbol, e.file] WHERE x IS NOT NULL],
                weight: 1.0,
                sim_threshold: $t1_thr,
                occurrences: 1,
                last_seen: timestamp(),
                impact: 0.0,
                embedding: emb
            })
            CREATE (a)-[:DERIVED_FROM]->(ev)
            FOREACH (m IN CASE WHEN e.symbol IS NULL THEN [] ELSE [e.symbol] END |
              MERGE (s:Symbol {fqname: m})
              MERGE (a)-[:APPLIES_TO]->(s)
            )
            FOREACH (p IN CASE WHEN e.file IS NULL THEN [] ELSE [e.file] END |
              MERGE (m:Module {path: p})
              MERGE (a)-[:APPLIES_TO]->(m)
            )
            RETURN a.id AS id
            """,
            {"e": e.model_dump(mode="json"), "vec": vec, "t1_thr": SIM_T1_CLUSTER},
        )
        if not rows:
            raise RuntimeError("AdviceEngine.ingest_error: no row returned from Cypher")
        return rows[0]["id"]

    def _compose_t1_text(self, e: ErrorEventIngest) -> str:
        return (
            f"{e.message}\n"
            f"Tags:{' '.join(e.tags)}\n"
            f"File:{e.file}\n"
            f"Symbol:{e.symbol}\n"
            f"Diff:\n{e.diff}\n"
            f"Context:\n{e.context_snippet or ''}"
        )

    # ------------------------------------------------------------- CLUSTER / MATCH

    async def match_similar_t1(self, advice_id: str, topk: int = 12) -> List[Dict[str, Any]]:
        rows = await cypher_query(
            """
            MATCH (a:Advice {id:$id, level:1})
            CALL db.index.vector.queryNodes('advice_embedding_idx', $k, a.embedding) YIELD node, score
            WHERE node.level = 1 AND node.id <> a.id
            RETURN node.id AS id, score, node.text AS text, node.scope AS scope
            ORDER BY score DESC
            """,
            {"id": advice_id, "k": topk},
        )
        return rows or []

    # -------------------------------------------------------------- T2 SYNTHESIS

    async def synthesize_t2(self, seed_id: str) -> Optional[str]:
        """
        Merge a cluster of similar T1 incidents into an enforceable T2 Advice doc
        using the prompt spec scope `simula.advice.synth_t2`.
        """
        cluster = await self.match_similar_t1(seed_id)
        strong = [c for c in (cluster or []) if float(c.get("score", 0.0)) >= SIM_T1_CLUSTER]
        if len(strong) + 1 < PROMOTE_MIN_T1:
            return None

        t1_payloads = await self._fetch_t1_payloads([seed_id] + [c["id"] for c in strong])
        prompt = await build_prompt(
            scope="simula.advice.synth_t2",
            context={"incidents": t1_payloads},
            summary="Merge repetitive incidents into generalized, enforceable advice (strict JSON).",
        )
        resp = await call_llm_service(
            prompt_response=prompt,
            agent_name="Advice.Synthesizer",
            scope="simula.advice.synth_t2",
            timeout=45,
        )
        doc = self._parse_advice_json(resp.json or resp.text)
        if not doc:
            log.warning("[Advice] synth_t2 returned invalid JSON")
            return None

        vec = await _embed_doc(self._flatten_advice(doc))
        rows = await cypher_query(
            """
            WITH $doc AS d, $vec AS emb
            CREATE (a:Advice {
              id: randomUUID(),
              level: 2,
              kind: 'code_advice',
              text: d.text,
              checklist: d.checklist,
              donts: d.donts,
              validation: d.validation,
              scope: d.scope,
              weight: 2.0,
              sim_threshold: $thr,
              occurrences: size(d.source_ids),
              last_seen: timestamp(),
              impact: 0.0,
              embedding: emb
            })
            WITH a, d
            UNWIND d.source_ids AS sid
            MATCH (s:Advice {id:sid})
            MERGE (s)-[:MERGED_INTO]->(a)
            RETURN a.id AS id
            """,
            {"doc": doc, "vec": vec, "thr": SIM_T2_MERGE},
        )
        if not rows:
            log.warning("[Advice] synth_t2 create returned no rows")
            return None
        return rows[0]["id"]

    async def _fetch_t1_payloads(self, ids: List[str]) -> List[Dict[str, Any]]:
        return await cypher_query(
            """
            MATCH (a:Advice) WHERE a.id IN $ids
            OPTIONAL MATCH (a)-[:DERIVED_FROM]->(ev:ErrorEvent)
            RETURN a.id AS id, a.text AS text, ev.file AS file, ev.symbol AS symbol, ev.diff AS diff
            """,
            {"ids": ids},
        ) or []

    # -------------------------------------------------------------- T3 SYNTHESIS

    async def merge_t2_to_t3(self, t2_id: str) -> Optional[str]:
        """
        Merge several similar T2 Advice docs into a cross-module T3 Advice doc
        using the prompt spec scope `simula.advice.synth_t3`.
        """
        rows = await cypher_query(
            """
            MATCH (a:Advice {id:$id, level:2})
            CALL db.index.vector.queryNodes('advice_embedding_idx', 12, a.embedding) YIELD node, score
            WHERE node.level = 2 AND node.id <> a.id AND score >= $thr
            RETURN node.id AS id, score
            ORDER BY score DESC
            """,
            {"id": t2_id, "thr": SIM_T2_MERGE},
        )
        peer_ids = [t2_id] + [r["id"] for r in (rows or [])]
        if len(peer_ids) < PROMOTE_MIN_T2:
            return None

        t2_docs = await self._fetch_t2_docs(peer_ids)
        prompt = await build_prompt(
            scope="simula.advice.synth_t3",
            context={"advice_docs": t2_docs},
            summary="Create cross-module architectural guidance (strict JSON).",
        )
        resp = await call_llm_service(
            prompt_response=prompt,
            agent_name="Advice.Synthesizer",
            scope="simula.advice.synth_t3",
            timeout=60,
        )
        doc = self._parse_advice_json(resp.json or resp.text)
        if not doc:
            log.warning("[Advice] synth_t3 returned invalid JSON")
            return None

        vec = await _embed_doc(self._flatten_advice(doc))
        rows2 = await cypher_query(
            """
            WITH $doc AS d, $vec AS emb
            CREATE (a:Advice {
              id: randomUUID(),
              level: 3,
              kind: 'code_advice',
              text: d.text,
              checklist: d.checklist,
              donts: d.donts,
              validation: d.validation,
              scope: d.scope,
              weight: 3.0,
              sim_threshold: 0.75,
              occurrences: size(d.source_ids),
              last_seen: timestamp(),
              impact: 0.0,
              embedding: emb
            })
            WITH a, d
            UNWIND d.source_ids AS sid
            MATCH (s:Advice {id:sid})
            MERGE (s)-[:MERGED_INTO]->(a)
            RETURN a.id AS id
            """,
            {"doc": doc, "vec": vec},
        )
        if not rows2:
            log.warning("[Advice] merge_t2_to_t3 create returned no rows")
            return None
        return rows2[0]["id"]

    async def _fetch_t2_docs(self, ids: List[str]) -> List[Dict[str, Any]]:
        return await cypher_query(
            """
            MATCH (a:Advice) WHERE a.id IN $ids
            RETURN a.id AS id, a.text AS text, a.checklist AS checklist, a.donts AS donts,
                   a.validation AS validation, a.scope AS scope
            """,
            {"ids": ids},
        ) or []

    # ----------------------------------------------------------- RETRIEVAL/INJECT

    async def retrieve_for(
        self,
        *,
        goal: str,
        target_fqname: Optional[str],
        local_context: Optional[str],
        k: int = TOPK_INJECT,
        extra_queries: Optional[Sequence[str]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Programmatic retrieval (used by callers that don't go through the lens).
        For prompt-time retrieval, prefer the semantic lenses.

        Supports multi-query composition via `extra_queries` (mean-pooled).
        """
        q_parts: List[str] = [
            goal or "",
            f"Target:{target_fqname or ''}",
            f"Context:\n{local_context or ''}",
        ]
        if extra_queries:
            q_parts.extend([q for q in extra_queries if q])

        qvec = await _embed_query(q_parts)

        rows = await cypher_query(
            """
            CALL db.index.vector.queryNodes('advice_embedding_idx', $limit, $qvec) YIELD node, score
            WHERE node.level >= 1
            RETURN node.id AS id, node.level AS level, node.text AS text,
                   node.checklist AS checklist, node.donts AS donts, node.validation AS validation,
                   node.scope AS scope, node.weight AS weight, node.sim_threshold AS thr,
                   node.last_seen AS last_seen, score
            ORDER BY score DESC
            """,
            {"qvec": qvec, "limit": k * 4},
        ) or []

        def prox(scope: List[str]) -> float:
            if not scope:
                return 1.0
            tf = target_fqname or ""
            s = " ".join(scope)
            if tf and tf in s:
                return 1.2
            if tf and tf.split("::")[0] in s:
                return 1.1
            return 1.0

        def recency(last_seen_ms: Optional[int]) -> float:
            try:
                if not last_seen_ms:
                    return 1.0
                days = max(0.0, (time.time() * 1000.0 - float(last_seen_ms)) / (1000.0 * 60 * 60 * 24))
                return 1.0 + min(0.2, 0.02 * (1.0 / (1.0 + days)))
            except Exception:
                return 1.0

        ranked = sorted(
            rows,
            key=lambda r: float(r.get("score", 0.0))
            * float(r.get("weight", 1.0))
            * prox(r.get("scope") or [])
            * recency(r.get("last_seen")),
            reverse=True,
        )
        return ranked[:k]

    async def retrieve_for_context(self, ctx: Dict[str, Any], k: int = TOPK_INJECT) -> Dict[str, Any]:
        """
        Convenience for orchestrators/lenses: takes an SCL/Planner context.
        """
        goal = (ctx.get("goal") or "").strip()
        target = ctx.get("target_fqname")
        local = ctx.get("history_summary") or ctx.get("context_summary") or ""
        # Optional: allow callers to pass query hints: ctx["advice_query_hints"] = ["AST", "pytest -k ..."]
        hints = ctx.get("advice_query_hints") if isinstance(ctx, dict) else None
        items = await self.retrieve_for(goal=goal, target_fqname=target, local_context=local, k=k, extra_queries=hints)
        return {
            "advice_items": items,
            "advice_meta": {"ids": [i["id"] for i in items], "selected": len(items)},
        }

    # ------------------------------------------------------------- FEEDBACK LOOP

    async def record_injection(self, advice_ids: List[str], episode_id: str) -> None:
        """Persist that specific advices were injected for an episode/run."""
        if not advice_ids or not episode_id:
            return
        await cypher_query(
            """
            MATCH (a:Advice) WHERE a.id IN $ids
            MERGE (e:Episode {id:$ep})
              ON CREATE SET e.at = timestamp()
            MERGE (a)-[:APPLIED_IN {at: timestamp(), prevented: NULL}]->(e)
            """,
            {"ids": advice_ids, "ep": episode_id},
        )

    async def mark_prevented(self, advice_ids: List[str], episode_id: str, prevented: bool) -> None:
        """Mark whether an injected advice likely prevented a reoccurrence in this episode."""
        if not advice_ids or not episode_id:
            return
        await cypher_query(
            """
            MATCH (a:Advice)-[r:APPLIED_IN]->(e:Episode {id:$ep})
            WHERE a.id IN $ids
            SET r.prevented = $prevented
            """,
            {"ids": advice_ids, "ep": episode_id, "prevented": bool(prevented)},
        )

    async def reward(self, advice_ids: List[str], bonus: float = 0.25):
        """Positive update: advice helped; increase weight, bump impact and recency."""
        if not advice_ids:
            return
        await cypher_query(
            """
            MATCH (a:Advice) WHERE a.id IN $ids
            SET a.weight = a.weight + $bonus,
                a.last_seen = timestamp(),
                a.occurrences = coalesce(a.occurrences,0)+1,
                a.impact = coalesce(a.impact,0.0) + 0.02
            """,
            {"ids": advice_ids, "bonus": bonus},
        )

    async def punish(self, advice_ids: List[str], malus: float = 0.15):
        """Negative update: advice was misleading/noisy; reduce weight and impact."""
        if not advice_ids:
            return
        await cypher_query(
            """
            MATCH (a:Advice) WHERE a.id IN $ids
            SET a.weight =
                  CASE WHEN a.weight - $malus < 0.1 THEN 0.1 ELSE a.weight - $malus END,
                a.impact =
                  CASE WHEN coalesce(a.impact,0.0) - 0.01 < 0.0 THEN 0.0 ELSE a.impact - 0.01 END
            """,
            {"ids": advice_ids, "malus": malus},
        )

    async def decay(self):
        """Time-based decay so stale advice naturally loses influence."""
        await cypher_query(
            """
            MATCH (a:Advice)
            WITH a, duration({millis: timestamp() - a.last_seen}).days AS days
            SET a.weight = a.weight * pow(0.5, days / $hl)
            """,
            {"hl": HALF_LIFE_DAYS},
        )

    # ------------------------------------------------------------------- UTILITIES

    def _parse_advice_json(self, text_or_obj: Any) -> Optional[Dict[str, Any]]:
        """
        Tolerant JSON parser: accepts dict, list[0], or JSON-string (with/without ```json fences).
        Enforces required fields and list types.
        """
        try:
            obj = text_or_obj
            if isinstance(obj, list):
                obj = obj[0] if obj else {}
            if isinstance(obj, str):
                t = obj.strip()
                if t.startswith("```"):
                    t = t.strip("`")
                    if t[:4].lower() == "json":
                        t = t[4:]
                obj = json.loads(t)
            if not isinstance(obj, dict):
                return None
            needed = {"text", "checklist", "donts", "validation", "scope", "source_ids"}
            if not needed.issubset(obj.keys()):
                return None
            for k in ("checklist", "donts", "validation", "scope", "source_ids"):
                if not isinstance(obj.get(k), list):
                    return None
            return obj
        except Exception:
            return None

    def _flatten_advice(self, d: Dict[str, Any]) -> str:
        """Compact representation for re-embedding."""
        return json.dumps(
            {
                "text": d.get("text"),
                "checklist": d.get("checklist"),
                "donts": d.get("donts"),
                "validation": d.get("validation"),
                "scope": d.get("scope"),
            },
            ensure_ascii=False,
        )

# ===== FILE: D:\EcodiaOS\systems\simula\learning\daemon.py =====
# systems/simula/daemon.py
from __future__ import annotations

import asyncio
import os
import signal
import logging
from dataclasses import dataclass
from typing import Optional, Iterable, Sequence

from systems.simula.learning.advice_engine import AdviceEngine

log = logging.getLogger(__name__)


@dataclass
class DaemonConfig:
    # Intervals (seconds)
    t2_synth_interval_s: float = float(os.getenv("SIM_DAEMON_T2_SYNTH_INTERVAL_S", "1.0"))
    t3_merge_interval_s: float = float(os.getenv("SIM_DAEMON_T3_MERGE_INTERVAL_S", "1.0"))
    decay_interval_s: float     = float(os.getenv("SIM_DAEMON_DECAY_INTERVAL_S", "3600"))   # hourly
    validate_interval_s: float  = float(os.getenv("SIM_DAEMON_VALIDATE_INTERVAL_S", "900")) # 15 min

    # Concurrency limits
    t2_parallel: int = int(os.getenv("SIM_DAEMON_T2_PARALLEL", "2"))
    t3_parallel: int = int(os.getenv("SIM_DAEMON_T3_PARALLEL", "1"))

    # Backpressure
    queue_maxsize: int = int(os.getenv("SIM_DAEMON_QUEUE_MAXSIZE", "1000"))

    # Optional: auto-harvest recent L1 items from Neo (disabled by default)
    auto_harvest_t1: bool = os.getenv("SIM_DAEMON_AUTO_HARVEST_T1", "false").lower() in ("1", "true", "yes")
    harvest_interval_s: float = float(os.getenv("SIM_DAEMON_HARVEST_INTERVAL_S", "120"))
    harvest_window_minutes: int = int(os.getenv("SIM_DAEMON_HARVEST_WINDOW_MIN", "10"))


class SimulaDaemon:
    """
    Async daemon that:
      - Consumes T1 IDs and synthesizes T2
      - Consumes T2 IDs and merges into T3
      - Periodically runs weight decay
      - Periodically runs advice validation (stub you can wire to repo checks)
      - (Optional) auto-harvests fresh T1 Advice nodes to seed T2 synthesis
    """

    def __init__(self, cfg: Optional[DaemonConfig] = None):
        self.cfg = cfg or DaemonConfig()
        self.engine = AdviceEngine()

        # Work queues
        self._t1_to_t2: asyncio.Queue[str] = asyncio.Queue(maxsize=self.cfg.queue_maxsize)
        self._t2_to_t3: asyncio.Queue[str] = asyncio.Queue(maxsize=self.cfg.queue_maxsize)

        # Task handles
        self._tasks: list[asyncio.Task] = []
        self._stopping = asyncio.Event()

    # ----------------------- public API -----------------------

    async def start(self) -> None:
        """
        Launch background tasks. Idempotent if called once.
        """
        log.info("[SimulaDaemon] starting...")
        self._stopping.clear()

        # Consumers
        self._tasks.append(asyncio.create_task(self._t2_synth_loop(), name="sim-daemon:t2-synth"))
        self._tasks.append(asyncio.create_task(self._t3_merge_loop(), name="sim-daemon:t3-merge"))

        # Periodics
        self._tasks.append(asyncio.create_task(self._decay_periodic(), name="sim-daemon:decay"))
        self._tasks.append(asyncio.create_task(self._validate_periodic(), name="sim-daemon:validate"))

        # Optional harvester
        if self.cfg.auto_harvest_t1:
            self._tasks.append(asyncio.create_task(self._harvest_periodic(), name="sim-daemon:harvest"))

        # Graceful shutdown on SIGTERM/SIGINT (if running as a process)
        try:
            loop = asyncio.get_running_loop()
            for sig in (signal.SIGTERM, signal.SIGINT):
                loop.add_signal_handler(sig, self._stopping.set)
        except NotImplementedError:
            # Signals may be unavailable on Windows or within some containers
            pass

        log.info("[SimulaDaemon] started with %d tasks", len(self._tasks))

    async def stop(self) -> None:
        """
        Signal tasks to stop, drain gracefully, and cancel lingering tasks.
        """
        log.info("[SimulaDaemon] stopping...")
        self._stopping.set()

        # Wake up sleepers by putting sentinels if queues are empty
        for _ in range(self.cfg.t2_parallel):
            await self._t1_to_t2.put("__STOP__")
        for _ in range(self.cfg.t3_parallel):
            await self._t2_to_t3.put("__STOP__")

        # Wait a bit for natural completion
        try:
            await asyncio.wait_for(asyncio.gather(*self._tasks, return_exceptions=True), timeout=10)
        except asyncio.TimeoutError:
            log.warning("[SimulaDaemon] force-cancelling lingering tasks...")
            for t in self._tasks:
                t.cancel()
            await asyncio.gather(*self._tasks, return_exceptions=True)

        self._tasks.clear()
        log.info("[SimulaDaemon] stopped.")

    # Allow producers to enqueue work
    async def enqueue_t1(self, *advice_ids: str) -> int:
        count = 0
        for aid in advice_ids:
            await self._t1_to_t2.put(aid)
            count += 1
        return count

    async def enqueue_t2(self, *advice_ids: str) -> int:
        count = 0
        for aid in advice_ids:
            await self._t2_to_t3.put(aid)
            count += 1
        return count

    # Convenience for bulk ingestion from iterables
    async def enqueue_many_t1(self, ids: Iterable[str]) -> int:
        c = 0
        for i in ids:
            await self._t1_to_t2.put(i)
            c += 1
        return c

    async def enqueue_many_t2(self, ids: Iterable[str]) -> int:
        c = 0
        for i in ids:
            await self._t2_to_t3.put(i)
            c += 1
        return c

    # ----------------------- loops -----------------------

    async def _t2_synth_loop(self) -> None:
        """
        Consume T1 advice IDs and synthesize T2 documents.
        """
        log.info("[SimulaDaemon] T2 synth loop running (parallel=%d)", self.cfg.t2_parallel)

        sem = asyncio.Semaphore(self.cfg.t2_parallel)

        async def worker(aid: str):
            async with sem:
                try:
                    if aid == "__STOP__":
                        return
                    t2 = await self.engine.synthesize_t2(aid)
                    if t2:
                        # Immediately enqueue for T3 merge consideration
                        await self._t2_to_t3.put(t2)
                        log.info("[Daemon] T2 synthesized %s from T1 seed %s", t2, aid)
                except Exception as e:
                    log.exception("[Daemon] synth_t2 failed for %s: %r", aid, e)

        while not self._stopping.is_set():
            try:
                aid = await asyncio.wait_for(self._t1_to_t2.get(), timeout=self.cfg.t2_synth_interval_s)
            except asyncio.TimeoutError:
                continue
            # schedule but don't block the loop
            asyncio.create_task(worker(aid))

        log.info("[SimulaDaemon] T2 synth loop exiting")

    async def _t3_merge_loop(self) -> None:
        """
        Consume T2 advice IDs and attempt T3 merges.
        """
        log.info("[SimulaDaemon] T3 merge loop running (parallel=%d)", self.cfg.t3_parallel)

        sem = asyncio.Semaphore(self.cfg.t3_parallel)

        async def worker(aid: str):
            async with sem:
                try:
                    if aid == "__STOP__":
                        return
                    t3 = await self.engine.merge_t2_to_t3(aid)
                    if t3:
                        log.info("[Daemon] T3 merged %s from T2 seed %s", t3, aid)
                except Exception as e:
                    log.exception("[Daemon] merge_t2_to_t3 failed for %s: %r", aid, e)

        while not self._stopping.is_set():
            try:
                aid = await asyncio.wait_for(self._t2_to_t3.get(), timeout=self.cfg.t3_merge_interval_s)
            except asyncio.TimeoutError:
                continue
            asyncio.create_task(worker(aid))

        log.info("[SimulaDaemon] T3 merge loop exiting")

    async def _decay_periodic(self) -> None:
        """
        Periodically apply time-decay to advice weights.
        """
        log.info("[SimulaDaemon] Decay periodic loop running (every %.1fs)", self.cfg.decay_interval_s)
        while not self._stopping.is_set():
            try:
                await self.engine.decay()
                log.debug("[Daemon] Decay pass complete")
            except Exception as e:
                log.exception("[Daemon] Decay failed: %r", e)
            await asyncio.wait({self._stopping.wait()}, timeout=self.cfg.decay_interval_s)
        log.info("[SimulaDaemon] Decay loop exiting")

    async def _validate_periodic(self) -> None:
        """
        Periodically trigger validation of advice (stubbed job).
        Replace with repo-specific AST/pytest/integration checks.
        """
        log.info("[SimulaDaemon] Validate periodic loop running (every %.1fs)", self.cfg.validate_interval_s)
        while not self._stopping.is_set():
            try:
                # hook your validate job here (currently a stub)
                log.debug("[Daemon] Validate pass (stub)")
            except Exception as e:
                log.exception("[Daemon] Validate failed: %r", e)
            await asyncio.wait({self._stopping.wait()}, timeout=self.cfg.validate_interval_s)
        log.info("[SimulaDaemon] Validate loop exiting")

    async def _harvest_periodic(self) -> None:
        """
        (Optional) Periodically harvest recent L1 advice to seed T2 synthesis automatically.
        Requires the Neo4j vector index + Advice nodes being created by ingest_error().
        """
        from core.utils.neo.cypher_query import cypher_query

        log.info("[SimulaDaemon] Harvest periodic loop running (every %.1fs)", self.cfg.harvest_interval_s)
        while not self._stopping.is_set():
            try:
                rows = await cypher_query(
                    """
                    MATCH (a:Advice {level:1})
                    WHERE a.created_at IS NULL OR
                          (datetime().epochMillis - coalesce(a.last_seen, timestamp())) < $window_ms
                    RETURN a.id AS id
                    ORDER BY coalesce(a.last_seen, timestamp()) DESC
                    LIMIT 100
                    """,
                    {"window_ms": self.cfg.harvest_window_minutes * 60_000},
                )
                ids = [r["id"] for r in (rows or [])]
                if ids:
                    enq = await self.enqueue_many_t1(ids)
                    log.info("[Daemon] Harvested %d new T1 items (enqueued=%d)", len(ids), enq)
            except Exception as e:
                log.exception("[Daemon] Harvest failed: %r", e)

            await asyncio.wait({self._stopping.wait()}, timeout=self.cfg.harvest_interval_s)

        log.info("[SimulaDaemon] Harvest loop exiting")

# ===== FILE: D:\EcodiaOS\systems\simula\learning\schemas.py =====
from __future__ import annotations

from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field


class ErrorEventIngest(BaseModel):
    episode_id: str
    turn_id: str
    file: str
    symbol: Optional[str] = None
    diff: str = ""
    message: str
    tags: List[str] = Field(default_factory=list)
    context_snippet: Optional[str] = None
    tool: Optional[str] = None


class AdviceDoc(BaseModel):
    id: str
    level: int
    kind: str = "code_advice"
    text: str
    checklist: List[str] = Field(default_factory=list)
    donts: List[str] = Field(default_factory=list)
    validation: List[str] = Field(default_factory=list)
    scope: List[str] = Field(default_factory=list)
    weight: float = 1.0
    sim_threshold: float = 0.84
    occurrences: int = 1
    last_seen: int = 0
    impact: float = 0.0
    conflicts: List[str] = Field(default_factory=list)


class RetrieveResult(BaseModel):
    id: str
    level: int
    text: str
    checklist: List[str] = Field(default_factory=list)
    donts: List[str] = Field(default_factory=list)
    validation: List[str] = Field(default_factory=list)
    scope: List[str] = Field(default_factory=list)
    weight: float
    thr: float
    score: float

# ===== FILE: D:\EcodiaOS\systems\simula\learning\jobs\cluster_t1_to_t2.py =====
# systems/simula/learning/jobs/synthesize_t2.py
from __future__ import annotations

import argparse
import asyncio
import logging
from typing import Iterable, List, Optional

from systems.simula.learning.advice_engine import AdviceEngine

log = logging.getLogger(__name__)


async def _synthesize_one(engine: AdviceEngine, advice_id: str) -> Optional[str]:
    try:
        t2 = await engine.synthesize_t2(advice_id)
        if t2:
            log.info("[AdviceJob] Synthesized T2 %s from seed %s", t2, advice_id)
        else:
            log.info("[AdviceJob] Not enough support to promote seed %s -> T2", advice_id)
        return t2
    except Exception as e:
        log.exception("[AdviceJob] synth_t2 failed for %s: %r", advice_id, e)
        return None


async def run(seed_ids: Iterable[str], *, concurrency: int = 4) -> List[str]:
    """
    Promote clusters of T1 incidents to T2 advice documents.

    Args:
        seed_ids: Iterable of T1 Advice node IDs to attempt promotion from.
        concurrency: Max number of seeds to process concurrently.

    Returns:
        List of created T2 advice IDs.
    """
    engine = AdviceEngine()
    sem = asyncio.Semaphore(max(1, concurrency))

    async def _guarded(aid: str) -> Optional[str]:
        async with sem:
            return await _synthesize_one(engine, aid)

    tasks = [asyncio.create_task(_guarded(a)) for a in seed_ids]
    results = await asyncio.gather(*tasks, return_exceptions=False)
    return [r for r in results if r]


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Synthesize T2 advice from T1 seeds.")
    p.add_argument(
        "--seed-id",
        dest="seed_ids",
        action="append",
        required=True,
        help="T1 Advice ID to use as a seed (can be passed multiple times).",
    )
    p.add_argument(
        "--concurrency",
        type=int,
        default=4,
        help="Max number of seeds to process concurrently (default: 4).",
    )
    p.add_argument(
        "--log-level",
        default="INFO",
        choices=["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"],
        help="Logging level (default: INFO).",
    )
    return p.parse_args()


def _configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )


def main() -> None:
    args = _parse_args()
    _configure_logging(args.log_level)
    created: List[str] = asyncio.run(run(args.seed_ids, concurrency=args.concurrency))
    if created:
        log.info("[AdviceJob] Created %d T2 advices: %s", len(created), ", ".join(created))
    else:
        log.info("[AdviceJob] No T2 advice created.")


if __name__ == "__main__":
    main()

# ===== FILE: D:\EcodiaOS\systems\simula\learning\jobs\decay_weights.py =====
# systems/simula/learning/jobs/decay.py
from __future__ import annotations

import argparse
import asyncio
import logging

from systems.simula.learning.advice_engine import AdviceEngine

log = logging.getLogger(__name__)


async def run() -> None:
    """Apply time-based decay to advice weights."""
    engine = AdviceEngine()
    await engine.decay()
    log.info("[AdviceJob] Decay pass completed.")


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Apply time-based decay to Advice weights.")
    p.add_argument(
        "--log-level",
        default="INFO",
        choices=["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"],
        help="Logging level (default: INFO).",
    )
    return p.parse_args()


def _configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )


def main() -> None:
    args = _parse_args()
    _configure_logging(args.log_level)
    asyncio.run(run())


if __name__ == "__main__":
    main()

# ===== FILE: D:\EcodiaOS\systems\simula\learning\jobs\merge_t2_to_t3.py =====
# systems/simula/learning/jobs/merge_t2_to_t3.py
from __future__ import annotations

import argparse
import asyncio
import logging
from typing import Iterable, List, Optional

from systems.simula.learning.advice_engine import AdviceEngine

log = logging.getLogger(__name__)


async def _merge_one(engine: AdviceEngine, t2_id: str) -> Optional[str]:
    try:
        t3 = await engine.merge_t2_to_t3(t2_id)
        if t3:
            log.info("[AdviceJob] Merged T3 %s from T2 seed %s", t3, t2_id)
        else:
            log.info("[AdviceJob] Not enough support to promote T2 %s -> T3", t2_id)
        return t3
    except Exception as e:
        log.exception("[AdviceJob] merge_t2_to_t3 failed for %s: %r", t2_id, e)
        return None


async def run(t2_ids: Iterable[str], *, concurrency: int = 4) -> List[str]:
    """
    Merge clusters of T2 advice documents into T3 architectural advice.

    Args:
        t2_ids: Iterable of T2 Advice node IDs to attempt promotion from.
        concurrency: Max number of seeds to process concurrently.

    Returns:
        List of created T3 advice IDs.
    """
    engine = AdviceEngine()
    sem = asyncio.Semaphore(max(1, concurrency))

    async def _guarded(aid: str) -> Optional[str]:
        async with sem:
            return await _merge_one(engine, aid)

    tasks = [asyncio.create_task(_guarded(a)) for a in t2_ids]
    results = await asyncio.gather(*tasks, return_exceptions=False)
    return [r for r in results if r]


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Merge T2 advice into T3 architectural advice.")
    p.add_argument(
        "--t2-id",
        dest="t2_ids",
        action="append",
        required=True,
        help="T2 Advice ID to use as a seed (can be passed multiple times).",
    )
    p.add_argument(
        "--concurrency",
        type=int,
        default=4,
        help="Max number of seeds to process concurrently (default: 4).",
    )
    p.add_argument(
        "--log-level",
        default="INFO",
        choices=["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"],
        help="Logging level (default: INFO).",
    )
    return p.parse_args()


def _configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )


def main() -> None:
    args = _parse_args()
    _configure_logging(args.log_level)
    created: List[str] = asyncio.run(run(args.t2_ids, concurrency=args.concurrency))
    if created:
        log.info("[AdviceJob] Created %d T3 advices: %s", len(created), ", ".join(created))
    else:
        log.info("[AdviceJob] No T3 advice created.")


if __name__ == "__main__":
    main()

# ===== FILE: D:\EcodiaOS\systems\simula\learning\jobs\validate_advice.py =====
# systems/simula/learning/jobs/validate_advice.py
from __future__ import annotations

import argparse
import asyncio
import json
import logging
import os
import shlex
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple

from core.utils.neo.cypher_query import cypher_query

log = logging.getLogger(__name__)

# ──────────────────────────────────────────────────────────────────────────────
# Config
# ──────────────────────────────────────────────────────────────────────────────

DEFAULT_TIMEOUT_SECS = 120  # per validation rule
DEFAULT_CONCURRENCY = 4


# ──────────────────────────────────────────────────────────────────────────────
# Data
# ──────────────────────────────────────────────────────────────────────────────

@dataclass
class AdviceRecord:
    id: str
    level: int
    text: str
    validation: List[str]
    scope: List[str]


@dataclass
class RuleResult:
    advice_id: str
    rule: str
    ok: bool
    details: Dict[str, Any]


# ──────────────────────────────────────────────────────────────────────────────
# Neo4j IO
# ──────────────────────────────────────────────────────────────────────────────

async def _load_advice_for_validation(
    advice_ids: Optional[List[str]] = None,
    limit: Optional[int] = None,
    rule_filter: Optional[str] = None,
) -> List[AdviceRecord]:
    """
    Fetch advice nodes that have non-empty validation arrays.
    Optional: filter by advice_ids; optional rule substring filter.
    """
    params: Dict[str, Any] = {}
    where = ["size(coalesce(a.validation, [])) > 0"]

    if advice_ids:
        where.append("a.id IN $ids")
        params["ids"] = advice_ids

    q = f"""
    MATCH (a:Advice)
    WHERE {' AND '.join(where)}
    RETURN a.id AS id, coalesce(a.level,1) AS level, coalesce(a.text,'') AS text,
           coalesce(a.validation,[]) AS validation, coalesce(a.scope,[]) AS scope
    ORDER BY a.level DESC, a.last_seen DESC
    """
    if limit and limit > 0:
        q += "\nLIMIT $limit"
        params["limit"] = int(limit)

    rows = await cypher_query(q, params) or []

    out: List[AdviceRecord] = []
    for r in rows:
        rules: List[str] = list(r.get("validation") or [])
        if rule_filter:
            rf = rule_filter.lower()
            rules = [x for x in rules if rf in x.lower()]
        if not rules:
            continue
        out.append(
            AdviceRecord(
                id=r["id"],
                level=int(r["level"]),
                text=r.get("text") or "",
                validation=rules,
                scope=list(r.get("scope") or []),
            )
        )
    return out


async def _record_results(
    results: List[RuleResult],
    run_id: Optional[str] = None,
    dry_run: bool = False,
) -> None:
    """
    Persist validation results to the graph. Creates a ValidationRun node and
    (Advice)-[:VALIDATED {rule, ok, details, at}]->(ValidationRun).
    """
    if dry_run or not results:
        return

    run_id = run_id or "vrun_" + os.urandom(6).hex()
    await cypher_query(
        """
        MERGE (r:ValidationRun {id:$run_id})
          ON CREATE SET r.at = timestamp()
        """,
        {"run_id": run_id},
    )

    # Batch insert relationships
    for res in results:
        await cypher_query(
            """
            MATCH (a:Advice {id:$aid})
            MATCH (r:ValidationRun {id:$run})
            MERGE (a)-[v:VALIDATED {rule:$rule}]->(r)
            SET v.ok = $ok,
                v.details = $details,
                v.at = timestamp()
            """,
            {
                "aid": res.advice_id,
                "run": run_id,
                "rule": res.rule,
                "ok": bool(res.ok),
                "details": json.dumps(res.details, ensure_ascii=False),
            },
        )


# ──────────────────────────────────────────────────────────────────────────────
# Rule execution
# Supported prefixes:
#   - "pytest::<selector>"   -> run pytest -k <selector>
#   - "cmd::<shell command>" -> run an arbitrary command
#   - "ast::<hint>"          -> placeholder for AST checks (Phase B/C)
#   - otherwise: treated as "cmd::<rule>"
# ──────────────────────────────────────────────────────────────────────────────

async def _run_subprocess(cmd: List[str], timeout: int) -> Tuple[int, str, str]:
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    try:
        stdout_b, stderr_b = await asyncio.wait_for(proc.communicate(), timeout=timeout)
    except asyncio.TimeoutError:
        try:
            proc.kill()
        finally:
            return (124, "", f"Timed out after {timeout}s")
    rc = proc.returncode
    return rc, stdout_b.decode(errors="replace"), stderr_b.decode(errors="replace")


async def _exec_pytest(selector: str, timeout: int) -> RuleResult:
    cmd = ["pytest", "-q", "-k", selector]
    rc, out, err = await _run_subprocess(cmd, timeout)
    ok = (rc == 0)
    return RuleResult(
        advice_id="",  # filled by caller
        rule=f"pytest::{selector}",
        ok=ok,
        details={"rc": rc, "stdout": out[-4000:], "stderr": err[-4000:]},
    )


async def _exec_cmd(command: str, timeout: int) -> RuleResult:
    # Use shlex to split safely; if complex shell features are required, wrap in "bash -lc"
    parts = shlex.split(command)
    rc, out, err = await _run_subprocess(parts, timeout)
    ok = (rc == 0)
    return RuleResult(
        advice_id="",  # filled by caller
        rule=f"cmd::{command}",
        ok=ok,
        details={"rc": rc, "stdout": out[-4000:], "stderr": err[-4000:]},
    )


async def _exec_ast(hint: str) -> RuleResult:
    # Placeholder for repo-specific AST checks
    # Implement in Phase B/C; for now we mark as skipped with ok=True (neutral)
    return RuleResult(
        advice_id="",  # filled by caller
        rule=f"ast::{hint}",
        ok=True,
        details={"skipped": True, "reason": "AST validation not implemented yet"},
    )


async def _run_rule(advice_id: str, rule: str, timeout: int) -> RuleResult:
    if rule.startswith("pytest::"):
        rr = await _exec_pytest(rule.split("::", 1)[1], timeout)
    elif rule.startswith("cmd::"):
        rr = await _exec_cmd(rule.split("::", 1)[1], timeout)
    elif rule.startswith("ast::"):
        rr = await _exec_ast(rule.split("::", 1)[1])
    else:
        # default to cmd
        rr = await _exec_cmd(rule, timeout)
    rr.advice_id = advice_id
    return rr


# ──────────────────────────────────────────────────────────────────────────────
# Orchestration
# ──────────────────────────────────────────────────────────────────────────────

async def _validate_advice(
    records: Iterable[AdviceRecord],
    *,
    timeout: int,
    concurrency: int,
) -> List[RuleResult]:
    sem = asyncio.Semaphore(max(1, concurrency))
    results: List[RuleResult] = []

    async def _run_one(advice: AdviceRecord) -> None:
        for rule in advice.validation:
            async with sem:
                try:
                    rr = await _run_rule(advice.id, rule, timeout)
                    results.append(rr)
                    status = "OK" if rr.ok else "FAIL"
                    log.info("[validate_advice] %s %s :: %s", status, advice.id, rule)
                except Exception as e:
                    log.exception("[validate_advice] rule crashed %s :: %s", advice.id, rule)
                    results.append(
                        RuleResult(
                            advice_id=advice.id,
                            rule=rule,
                            ok=False,
                            details={"exception": repr(e)},
                        )
                    )

    await asyncio.gather(*[asyncio.create_task(_run_one(rec)) for rec in records])
    return results


# ──────────────────────────────────────────────────────────────────────────────
# CLI / Entry
# ──────────────────────────────────────────────────────────────────────────────

async def run(
    *,
    advice_ids: Optional[List[str]] = None,
    limit: Optional[int] = None,
    rule_filter: Optional[str] = None,
    timeout: int = DEFAULT_TIMEOUT_SECS,
    concurrency: int = DEFAULT_CONCURRENCY,
    dry_run: bool = False,
    run_id: Optional[str] = None,
) -> None:
    """Validate advice using its 'validation' rules and persist results."""
    records = await _load_advice_for_validation(advice_ids, limit, rule_filter)
    if not records:
        log.info("[AdviceJob] No advice found with validation rules.")
        return

    log.info(
        "[AdviceJob] Validating %d advice nodes (timeout=%ss, concurrency=%d, dry_run=%s)",
        len(records), timeout, concurrency, dry_run,
    )
    results = await _validate_advice(records, timeout=timeout, concurrency=concurrency)
    ok_count = sum(1 for r in results if r.ok)
    fail_count = len(results) - ok_count
    log.info("[AdviceJob] Validation complete. ok=%d fail=%d", ok_count, fail_count)

    await _record_results(results, run_id=run_id, dry_run=dry_run)


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Validate Advice against its 'validation' rules.")
    p.add_argument(
        "--advice-id",
        dest="advice_ids",
        action="append",
        help="Advice ID to validate (can be provided multiple times). If omitted, validates all with rules.",
    )
    p.add_argument(
        "--limit",
        type=int,
        default=0,
        help="Limit number of advice to validate (0 = no limit).",
    )
    p.add_argument(
        "--rule-filter",
        help="Substring filter to select only rules that contain this string (case-insensitive).",
    )
    p.add_argument(
        "--timeout",
        type=int,
        default=DEFAULT_TIMEOUT_SECS,
        help=f"Per-rule timeout in seconds (default: {DEFAULT_TIMEOUT_SECS}).",
    )
    p.add_argument(
        "--concurrency",
        type=int,
        default=DEFAULT_CONCURRENCY,
        help=f"Max concurrent rule executions (default: {DEFAULT_CONCURRENCY}).",
    )
    p.add_argument(
        "--dry-run",
        action="store_true",
        help="Do not write results back to Neo4j.",
    )
    p.add_argument(
        "--run-id",
        help="Optional ValidationRun id to group results under; autogenerated if omitted.",
    )
    p.add_argument(
        "--log-level",
        default="INFO",
        choices=["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"],
        help="Logging level (default: INFO).",
    )
    return p.parse_args()


def _configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )


def main() -> None:
    args = _parse_args()
    _configure_logging(args.log_level)
    asyncio.run(
        run(
            advice_ids=args.advice_ids,
            limit=(args.limit if args.limit and args.limit > 0 else None),
            rule_filter=args.rule_filter,
            timeout=args.timeout,
            concurrency=args.concurrency,
            dry_run=args.dry_run,
            run_id=args.run_id,
        )
    )


if __name__ == "__main__":
    main()

# ===== FILE: D:\EcodiaOS\systems\simula\memory\reinforcement_daemon.py =====
# systems/simula/memory/reinforcement_daemon.py

from __future__ import annotations
import asyncio
import logging
from typing import List, Dict

from core.services.synapse import SynapseClient
from core.utils.neo.cypher_query import cypher_query

log = logging.getLogger(__name__)

class ReinforcementDaemon:
    """
    A background service that closes the MDO's learning loop.
    It periodically syncs reflex performance scores from Synapse
    back to the SynapticTrace nodes in the Neo4j graph, adjusting
    their confidence scores so the MDO learns which reflexes to trust.
    """
    def __init__(self, poll_interval_seconds: int = 300):
        self.synapse = SynapseClient()
        self.poll_interval = poll_interval_seconds
        self.running = False

    async def _get_all_trace_ids(self) -> List[str]:
        """Fetches all trace_ids from the graph."""
        query = "MATCH (t:SynapticTrace) RETURN t.trace_id AS traceId"
        results = await cypher_query(query)
        return [r['traceId'] for r in results if r.get('traceId')]

    async def _update_confidence_scores(self, scores: Dict[str, float]):
        """Updates the confidence_score for multiple traces in a single transaction."""
        if not scores:
            return

        query = """
        UNWIND $scores AS score_update
        MATCH (t:SynapticTrace {trace_id: score_update.trace_id})
        SET t.confidence_score = score_update.new_score
        """
        # Format for UNWIND: [{"trace_id": "abc", "new_score": 0.8}, ...]
        params = [{"trace_id": tid, "new_score": score} for tid, score in scores.items()]
        await cypher_query(query, {"scores": params})
        log.info(f"[ReinforcementDaemon] Updated confidence scores for {len(scores)} traces.")

    async def run_once(self):
        """Performs a single cycle of fetching and updating scores."""
        log.info("[ReinforcementDaemon] Starting reinforcement cycle...")
        try:
            trace_ids = await self._get_all_trace_ids()
            if not trace_ids:
                log.info("[ReinforcementDaemon] No traces found in graph. Cycle complete.")
                return

            # The arm_id in Synapse is prefixed, e.g., "trace::trace_xyz"
            arm_ids_for_synapse = [f"trace::{tid}" for tid in trace_ids]
            
            # Fetch the latest scores from the bandit policy engine
            arm_scores = await self.synapse.get_arm_scores(arm_ids_for_synapse)

            scores_to_update: Dict[str, float] = {}
            for arm in arm_scores:
                # Strip the prefix to get the trace_id
                trace_id = arm.arm_id.split("::")[-1]
                # Normalize the score from Synapse into a 0-1 confidence value
                new_confidence = max(0.0, min(1.0, arm.score))
                scores_to_update[trace_id] = new_confidence

            if scores_to_update:
                await self._update_confidence_scores(scores_to_update)
            
            log.info("[ReinforcementDaemon] Reinforcement cycle finished successfully.")

        except Exception as e:
            log.error(f"[ReinforcementDaemon] Cycle failed: {e!r}", exc_info=True)

    async def start(self):
        """Starts the daemon's continuous polling loop."""
        self.running = True
        log.info(f"[ReinforcementDaemon] Starting with a poll interval of {self.poll_interval} seconds.")
        while self.running:
            await self.run_once()
            await asyncio.sleep(self.poll_interval)

    def stop(self):
        """Stops the daemon."""
        self.running = False

# Example of how to run this daemon
async def main():
    daemon = ReinforcementDaemon()
    await daemon.start()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
# ===== FILE: D:\EcodiaOS\systems\simula\memory\schemas.py =====
# systems/simula/memory/schemas.py

from __future__ import annotations
from uuid import uuid4
from pydantic import BaseModel, Field
from typing import List, Dict, Any

class SynapticTrace(BaseModel):
    """
    Represents a learned, reflexive memory. It maps a specific problem
    signature (the trigger) to a proven sequence of actions (the reflex).
    """
    trace_id: str = Field(default_factory=lambda: f"trace_{uuid4().hex}")
    
    # The multi-modal "smell" of a problem
    triggering_state_vector: List[float]
    
    # The exact, successful sequence of tool calls that solved the problem
    action_sequence: List[Dict[str, Any]]
    
    # The utility score from the run that created this trace
    outcome_utility: float
    
    # The bandit-managed confidence score (how reliable is this reflex?)
    confidence_score: float = Field(default=0.5, ge=0.0, le=1.0)
    
    # Metadata for analysis
    generation_timestamp: float
    last_applied_timestamp: float | None = None
    application_count: int = 0
# ===== FILE: D:\EcodiaOS\systems\simula\memory\trace_db.py =====
# systems/simula/memory/trace_db.py

from __future__ import annotations
from typing import List, Optional
import time
import logging
import json

from .schemas import SynapticTrace
from core.utils.neo.cypher_query import cypher_query

log = logging.getLogger(__name__)

class TraceDBClient:
    """
    [PRODUCTION IMPLEMENTATION]
    Manages the persistent storage and retrieval of Synaptic Traces in Neo4j.
    This client uses the graph database for both property storage and
    high-speed vector similarity search, forming the MDO's long-term memory.
    """

    async def save(self, trace: SynapticTrace):
        """
        Saves a new trace to the Neo4j graph. It uses MERGE to ensure that
        if a trace with the same ID is saved again, it's updated rather than duplicated.
        """
        log.info(f"[TraceDB-Neo4j] Persisting trace: {trace.trace_id}")
        
        # We store complex objects like action_sequence as JSON strings in the graph.
        props_to_set = trace.model_dump(exclude={"triggering_state_vector"})
        props_to_set["action_sequence_json"] = json.dumps(trace.action_sequence)
        del props_to_set["action_sequence"]

        query = """
        MERGE (t:SynapticTrace {trace_id: $trace_id})
        SET t += $props, t.triggering_state_vector = $vector
        """
        
        params = {
            "trace_id": trace.trace_id,
            "props": props_to_set,
            "vector": trace.triggering_state_vector
        }
        
        await cypher_query(query, params)

    async def search(
        self, 
        query_vector: List[float], 
        min_confidence: float, 
        similarity_threshold: float,
        top_k: int = 1
    ) -> Optional[SynapticTrace]:
        """
        Performs a high-speed vector similarity search using the Neo4j index.
        It finds the single best matching trace that meets our confidence and
        similarity thresholds.
        """
        if not query_vector:
            return None

        query = """
        CALL db.index.vector.queryNodes('synaptic_trace_trigger_vectors', $top_k, $vector)
        YIELD node AS trace, score
        WHERE score >= $similarity_threshold AND trace.confidence_score >= $min_confidence
        RETURN trace, score
        ORDER BY score DESC
        LIMIT 1
        """

        params = {
            "top_k": top_k,
            "vector": query_vector,
            "similarity_threshold": similarity_threshold,
            "min_confidence": min_confidence
        }

        result = await cypher_query(query, params)

        if not result:
            return None

        node_data = result[0]['trace']
        score = result[0]['score']
        
        log.info(f"[TraceDB-Neo4j] Found matching trace {node_data.get('trace_id')} with similarity {score:.4f}")

        # Reconstruct the Pydantic model from the raw graph data.
        action_sequence = json.loads(node_data.get("action_sequence_json", "[]"))
        
        return SynapticTrace(
            trace_id=node_data["trace_id"],
            triggering_state_vector=node_data["triggering_state_vector"],
            action_sequence=action_sequence,
            outcome_utility=node_data["outcome_utility"],
            confidence_score=node_data["confidence_score"],
            generation_timestamp=node_data["generation_timestamp"],
            last_applied_timestamp=node_data.get("last_applied_timestamp"),
            application_count=node_data.get("application_count", 0)
        )

    async def record_application(self, trace_id: str):
        """Updates metadata for a trace in the graph when it is used."""
        query = """
        MATCH (t:SynapticTrace {trace_id: $trace_id})
        SET t.application_count = coalesce(t.application_count, 0) + 1,
            t.last_applied_timestamp = $timestamp
        """
        params = {"trace_id": trace_id, "timestamp": time.time()}
        await cypher_query(query, params)
        log.debug(f"[TraceDB-Neo4j] Updated application metadata for trace {trace_id}")
# ===== FILE: D:\EcodiaOS\systems\simula\metamorphosis\cognitive_anomaly_detector.py =====
# systems/simula/metamorphosis/cognitive_anomaly_detector.py

from __future__ import annotations
import asyncio
import logging
from collections import defaultdict, deque
from typing import Dict, Deque
import os
import redis.asyncio as redis

from systems.simula.nscs.agent_tools import initiate_self_modification

log = logging.getLogger(__name__)

# Configuration for the Anomaly Detector
EVENT_STREAM_KEY = "mdo:event_stream"
MAX_EVENT_HISTORY = 100
ANOMALY_PATTERNS = {
    "REPEATED_PLAN_REJECTION": {
        "threshold": 3, # 3 rejections for the same reason in the history
        "meta_goal_template": "My deliberation process is consistently failing to produce an approved plan for tasks involving '{target_fqname}'. The rejection reason is '{reason}'. Analyze the deliberation logic in `systems/simula/agent/deliberation.py` and improve the Planner or Judge prompts to prevent this recurring failure."
    },
    "REPEATED_TOOL_CRASH": {
        "threshold": 3,
        "meta_goal_template": "The tool '{tool_name}' has crashed {threshold} times with the error: '{reason}'. Analyze the tool's implementation in `systems/simula/nscs/agent_tools.py` and add the necessary error handling or logic corrections to make it more robust."
    }
}

class CognitiveAnomalyDetector:
    """
    The MDO's Cerebral Cortex. It monitors the agent's performance, detects
    systemic weaknesses, and triggers the self-modification (Crucible) process
    to evolve the agent's own source code.
    """

    def __init__(self):
        self.event_history: Deque[Dict] = deque(maxlen=MAX_EVENT_HISTORY)
        self.redis_client = redis.from_url(os.getenv("REDIS_URL", "redis://redis:6379/0"), decode_responses=True)
        self.running = False
        self.repo_url = os.getenv("GIT_REPO_URL", "https://github.com/YourOrg/EcodiaOS.git") # IMPORTANT: Configure this env var

    async def _listen_for_events(self):
        """Listens to a Redis stream for events published by the Orchestrator."""
        log.info(f"[Cortex] Listening for MDO events on Redis stream '{EVENT_STREAM_KEY}'...")
        while self.running:
            try:
                # Using blocking read to wait for new events
                events = await self.redis_client.xread({EVENT_STREAM_KEY: "$"}, block=0, count=1)
                for stream, messages in events:
                    for message_id, event_data in messages:
                        log.info(f"[Cortex] Received event: {event_data.get('event_type')}")
                        self.event_history.append(event_data)
                        await self._analyze_for_anomalies()
            except Exception as e:
                log.error(f"[Cortex] Error in event listener: {e!r}", exc_info=True)
                await asyncio.sleep(10) # Prevent rapid-fire errors

    async def _analyze_for_anomalies(self):
        """Analyzes the recent event history to detect systemic failures."""
        # --- Detect Repeated Plan Rejection ---
        rejection_reasons = defaultdict(int)
        last_rejection_event = None
        for event in self.event_history:
            if event.get("event_type") == "plan_rejected":
                reason = event.get("reason", "Unknown Reason")
                rejection_reasons[reason] += 1
                last_rejection_event = event
        
        for reason, count in rejection_reasons.items():
            pattern = ANOMALY_PATTERNS["REPEATED_PLAN_REJECTION"]
            if count >= pattern["threshold"]:
                log.warning(f"[Cortex] Anomaly Detected: Repeated Plan Rejection (Reason: {reason})")
                meta_goal = pattern["meta_goal_template"].format(
                    target_fqname=last_rejection_event.get("target_fqname", "N/A"),
                    reason=reason
                )
                await self._trigger_self_modification(meta_goal)
                self.event_history.clear() # Clear history to prevent re-triggering
                return

        # --- Detect Repeated Tool Crash ---
        tool_crashes = defaultdict(int)
        last_crash_event = None
        for event in self.event_history:
            if event.get("event_type") == "tool_crashed":
                tool_name = event.get("tool_name", "Unknown Tool")
                tool_crashes[tool_name] += 1
                last_crash_event = event

        for tool_name, count in tool_crashes.items():
            pattern = ANOMALY_PATTERNS["REPEATED_TOOL_CRASH"]
            if count >= pattern["threshold"]:
                log.warning(f"[Cortex] Anomaly Detected: Repeated Tool Crash (Tool: {tool_name})")
                meta_goal = pattern["meta_goal_template"].format(
                    tool_name=tool_name,
                    threshold=count,
                    reason=last_crash_event.get("reason", "N/A")
                )
                await self._trigger_self_modification(meta_goal)
                self.event_history.clear() # Clear history
                return
    
    async def _trigger_self_modification(self, meta_goal: str):
        """Initiates the Crucible process in a fire-and-forget manner."""
        log.info(f"[Cortex] Triggering self-modification with meta-goal: '{meta_goal}'")
        # We don't await this; the Crucible is a long-running, autonomous process.
        asyncio.create_task(
            initiate_self_modification(
                meta_goal=meta_goal,
                repo_url=self.repo_url
            )
        )

    async def start(self):
        """Starts the Cortex's main loop."""
        self.running = True
        await self._listen_for_events()

    def stop(self):
        self.running = False
        log.info("[Cortex] Shutting down.")

# Example of how to run this daemon
async def main():
    detector = CognitiveAnomalyDetector()
    await detector.start()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
# ===== FILE: D:\EcodiaOS\systems\simula\nscs\agent_tools.py =====
# systems/simula/nscs/agent_tools.py
# --- DEFINITIVE, CONSOLIDATED, AND FULLY REFACTORED IMPLEMENTATION ---
# This file is the single source of truth for all of Simula's tool implementations.

from __future__ import annotations

import ast
import io
import logging
import re
import textwrap
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable, Any, Awaitable

# --- Core EcodiaOS/Simula Imports ---
from core.prompting.orchestrator import build_prompt
from core.utils.llm_gateway_client import call_llm_service, extract_json_flex
from core.utils.net_api import ENDPOINTS, get_http_client
from systems.nova.schemas import AuctionResult, InnovationBrief, InventionCandidate
from systems.qora import api_client as qora_client
from systems.simula.artifacts.package import create_artifact_bundle
from github import Github # MDO-FIX: Import the Github class

from systems.simula.build.run import run_build_and_tests
from systems.simula.ci.pipelines import render_ci
from systems.simula.code_sim.diagnostics.error_parser import parse_pytest_output
from systems.simula.code_sim.fuzz.hypo_driver import run_hypothesis_smoke
from systems.simula.code_sim.repair.engine import attempt_repair
from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import ensure_toolchain, seed_config
from systems.simula.code_sim.telemetry import track_tool
from systems.simula.format.autoformat import autoformat_changed
from systems.simula.git.rebase import rebase_diff_onto_branch
from systems.simula.ops.glue import quick_impact_and_cov, quick_policy_gate
from systems.simula.recipes.generator import append_recipe
from systems.simula.search.portfolio_runner import rank_portfolio
from systems.simula.vcs.commit_msg import render_conventional_commit, title_from_evidence
from systems.simula.vcs.pr_manager import open_pr as _open_pr_impl
from .evolution import execute_code_evolution

logger = logging.getLogger(__name__)

# ==============================================================================
# SECTION: Shared Helpers
# ==============================================================================

# --- Global registry ---
_TRACKED_TOOLS: dict[str, Callable[..., Any]] = {}

def track_tool(name: str):
    def decorator(fn: Callable[..., Any]):
        _TRACKED_TOOLS[name] = fn
        return fn
    return decorator

def get_tracked_tools() -> dict[str, Callable[..., Any]]:
    return _TRACKED_TOOLS.copy()


def _normalize_paths(paths: Optional[List[str]]) -> List[str]:
    """Provides a default path if none are given and filters empty strings."""
    return [p for p in (paths or ["."]) if p]

def _strip_markdown_fences(text: str) -> str:
    """Removes typical markdown/code fences from LLM output."""
    if not isinstance(text, str): return ""
    match = re.search(r"```(?:[a-zA-Z0-9]*)?\s*(.*?)```", text, re.DOTALL)
    return match.group(1).strip() if match else text.strip()

def _discover_functions(src: str) -> list[str]:
    """Parses source code to find top-level function definitions."""
    names: list[str] = []
    try:
        tree = ast.parse(src)
        for n in tree.body:
            if isinstance(n, ast.FunctionDef) and not n.name.startswith("_"):
                names.append(n.name)
    except Exception:
        pass
    return names

async def _api_call(
    method: str, endpoint_name: str, payload: dict[str, Any] | None = None, timeout: float = 60.0
) -> dict[str, Any]:
    """A single, robust helper for making API calls to internal services."""
    try:
        http = await get_http_client()
        url = getattr(ENDPOINTS, endpoint_name)
        if method.upper() == "POST":
            response = await http.post(url, json=payload or {}, timeout=timeout)
        elif method.upper() == "GET":
            response = await http.get(url, params=payload or {}, timeout=timeout)
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")
        response.raise_for_status()
        return {"status": "success", "result": response.json() or {}}
    except AttributeError:
        return {"status": "error", "reason": f"Config error: Endpoint '{endpoint_name}' not found."}
    except Exception as e:
        return {"status": "error", "reason": f"API call to '{endpoint_name}' failed: {e!r}"}
        
async def _post(path: str, payload: dict[str, Any], timeout: float = 30.0) -> dict[str, Any]:
    http = await get_http_client()
    r = await http.post(path, json=payload, timeout=timeout)
    r.raise_for_status()
    return r.json() or {}

async def _get(path: str, params: dict[str, Any], timeout: float = 30.0) -> dict[str, Any]:
    http = await get_http_client()
    r = await http.get(path, params=params, timeout=timeout)
    r.raise_for_status()
    return r.json() or {}

async def _bb_write(key: str, value: Any) -> dict[str, Any]:
    url = getattr(ENDPOINTS, "QORA_BB_WRITE", "/qora/bb/write")
    return await _post(url, {"key": key, "value": value})

async def _bb_read(key: str) -> Any:
    url = getattr(ENDPOINTS, "QORA_BB_READ", "/qora/bb/read")
    out = await _get(url, {"key": key})
    return out.get("value") if isinstance(out, dict) else out

# ==============================================================================
# SECTION: High-Level / Meta Tools
# ==============================================================================

@track_tool("propose_intelligent_patch")
async def propose_intelligent_patch(*, goal: str, objective: dict) -> Dict[str, Any]:
    """
    Triggers the full, self-contained, multi-step code evolution engine.

    This is a powerful, high-level tool that autonomously attempts to solve a complex
    coding task. Use this for broad goals like 'implement feature X' or 'refactor module Y'
    that require planning, code generation, and verification.

    Args:
        goal: The high-level objective for the code evolution.
        objective: A structured dictionary providing more specific details or constraints.
    """
    return await execute_code_evolution(goal=goal, objective=objective)

@track_tool("plan_and_critique_strategy")
async def plan_and_critique_strategy(*, goal: str, dossier: dict, turn_history: list) -> dict[str, Any]:
    """
    Performs a two-step strategic deliberation: first, propose a plan, then critique it to find flaws.

    This is a powerful meta-planning tool to prevent naive or flawed initial plans. It forces
    the agent to "think twice" by generating a strategy, then immediately using a separate
    "red team" persona to find weaknesses in that strategy, returning both for final consideration.
    Use this at the beginning of a complex task to ensure a robust approach.

    Args:
        goal: The high-level objective for the task.
        dossier: The context dossier for the code being worked on.
        turn_history: A history of previous turns in the current session to learn from.
    """
    try:
        # Step 1: Generate the initial plan
        plan_prompt = await build_prompt(
            scope="simula.strategic_planner",
            context={"goal": goal, "dossier": dossier, "turn_history": turn_history},
            summary="Generate a high-level strategic plan to accomplish the goal.",
        )
        plan_response = await call_llm_service(plan_prompt, agent_name="Simula.Strategist")
        initial_plan = extract_json_flex(plan_response.text)
        if not initial_plan:
            return {"status": "error", "reason": "Failed to generate an initial strategic plan."}

        # Step 2: Critique the plan
        critique_prompt = await build_prompt(
            scope="simula.strategy_critique",
            context={"goal": goal, "plan_to_critique": initial_plan},
            summary="Critique the proposed strategic plan for flaws, risks, and missed opportunities.",
        )
        critique_response = await call_llm_service(critique_prompt, agent_name="Simula.RedTeam")
        critique = extract_json_flex(critique_response.text)
        if not critique:
            return {"status": "error", "reason": "Failed to generate a critique of the plan."}
            
        return {
            "status": "success",
            "result": {
                "initial_plan": initial_plan,
                "critique": critique,
            },
        }
    except Exception as e:
        return {"status": "error", "reason": f"Strategic deliberation failed: {e!r}"}

@track_tool("create_pull_request")
async def create_pull_request(repo_slug: str, title: str, head_branch: str, base_branch: str, body: str) -> Dict[str, Any]:
    """
    [CORTEX TOOL] Creates a pull request on GitHub.
    Requires a GITHUB_TOKEN environment variable with repo access.
    """
    token = os.getenv("GITHUB_TOKEN")
    if not token:
        return {"status": "error", "reason": "GITHUB_TOKEN environment variable not set."}
    
    try:
        g = Github(token)
        repo = g.get_repo(repo_slug)
        pr = repo.create_pull(
            title=title,
            body=body,
            head=head_branch,
            base=base_branch
        )
        logger.info(f"Successfully created PR #{pr.number}: {pr.html_url}")
        return {"status": "success", "result": {"pr_number": pr.number, "url": pr.html_url}}
    except Exception as e:
        logger.error(f"Failed to create pull request: {e!r}", exc_info=True)
        return {"status": "error", "reason": f"GitHub API call failed: {e!r}"}
    
@track_tool("local_select_patch")
async def local_select_patch(*, candidates: list[dict], top_k: int = 3) -> dict[str, Any]:
    """
    Locally ranks a list of candidate code patches to select the most promising ones.

    When multiple solutions have been generated for a task, this tool provides a fast,
    local evaluation to filter them down before running more expensive verification.

    Args:
        candidates: A list of candidate solutions, each typically containing a 'diff'.
        top_k: The number of top-ranked candidates to return.
    """
    ranked = await rank_portfolio(candidates, top_k=top_k)
    return {"status": "success", "top": ranked}

@track_tool("record_recipe")
async def record_recipe(*, goal: str, context_fqname: str, steps: list[str], success: bool, impact_hint: str = "") -> dict[str, Any]:
    """
    Saves a successful or failed sequence of actions as a 'recipe' for future learning.

    This tool is used for meta-learning. By recording the steps taken to solve a problem,
    the system can learn effective (or ineffective) patterns for similar tasks in the future.

    Args:
        goal: The original goal of the task.
        context_fqname: The fully-qualified name of the code context (e.g., file path).
        steps: A list of the actions/tool calls that were executed.
        success: Whether the final outcome was successful.
        impact_hint: A hint about the impact of the change.
    """
    r = append_recipe(goal=goal, context_fqname=context_fqname, steps=steps, success=success, impact_hint=impact_hint)
    return {"status": "success", "recipe": r.__dict__}

# ==============================================================================
# SECTION: Qora & Nova Adapters (Reasoning & Learning)
# ==============================================================================

@track_tool("get_context_dossier")
async def get_context_dossier(*, target_fqname: str, intent: str) -> Dict[str, Any]:
    """
    Fetches a comprehensive 'dossier' of context about a specific code element from Qora.

    This is the primary tool for gathering information before modifying existing code.
    The dossier includes the source code, related tests, call graphs, documentation, and more.
    AHAHSUH SHUIAS HFIAHI
    Args:
        target_fqname: The fully-qualified name of the target symbol (e.g., 'path/to/file.py::my_function').
        intent: The reason for fetching the dossier (e.g., 'implement', 'refactor', 'debug').
    """
    return await _api_call("POST", "QORA_DOSSIER_BUILD", {"target_fqname": target_fqname, "intent": intent})

@track_tool("qora_semantic_search")
async def qora_semantic_search(*, query_text: str, top_k: int = 5) -> dict[str, Any]:
    """
    Performs a semantic search across the entire codebase for relevant code snippets.

    Use this tool to find examples, related logic, or alternative implementations
    when you are unsure where to start or need more context than a dossier provides.

    Args:
        query_text: The natural language search query.
        top_k: The maximum number of search results to return.
    """
    return await _api_call("POST", "QORA_SEMANTIC_SEARCH", {"query_text": query_text, "top_k": top_k})

@track_tool("qora_get_call_graph")
async def qora_get_call_graph(*, target_fqn: str) -> dict[str, Any]:
    """
    Retrieves the call graph (both upstream and downstream) for a specific function.

    This helps understand the dependencies and potential impact of changing a function.
    Use it to see what functions call this one, and what functions this one calls.

    Args:
        target_fqn: The fully-qualified name of the target function.
    """
    return await _api_call("POST", "QORA_GET_CALL_GRAPH", {"target_fqn": target_fqn})

@track_tool("qora_get_goal_context")
async def qora_get_goal_context(*, query_text: str, top_k: int = 3) -> dict[str, Any]:
    """
    A high-level Qora tool to find the most relevant code context for a given task goal.

    This is a good starting point if you only have a natural language goal and don't
    know which specific file or function is the target.

    Args:
        query_text: The natural language description of the goal.
        top_k: The number of context candidates to return.
    """
    return await _api_call("POST", "QORA_GET_GOAL_CONTEXT", {"query_text": query_text, "top_k": top_k})

@track_tool("qora_hygiene_check")
async def qora_hygiene_check(*, diff: str, auto_heal: bool = True, timeout_sec: int = 300) -> dict[str, Any]:
    """
    Runs a suite of static analysis, linting, and style checks on a proposed code change.

    This tool ensures that any new code adheres to the project's quality standards.
    The `auto_heal` option can automatically fix simple style issues.

    Args:
        diff: A git-formatted unified diff string of the code change.
        auto_heal: If True, attempts to automatically fix detected issues.
        timeout_sec: The maximum time allowed for the check.
    """
    payload = {"diff": diff, "auto_heal": auto_heal, "timeout_sec": timeout_sec}
    return await _api_call("POST", "QORA_HYGIENE_CHECK", payload, timeout=timeout_sec + 30)

@track_tool("qora_request_critique")
async def request_critique(params: dict) -> dict[str, Any]:
    """
    Submits a plan, code change, or idea to a multi-agent deliberation service for critique.

    This tool provides a mechanism for getting feedback from other specialized agents,
    helping to identify flaws or potential improvements in a proposed solution before execution.

    Args:
        params: The payload for the critique request, typically including the item to be critiqued.
    """
    return await qora_client.request_critique(**params)

@track_tool("qora_find_similar_failures")
async def find_similar_failures(params: dict) -> dict[str, Any]:
    """
    Searches a historical database for past failures that are similar to the current situation.

    A key tool for learning from mistakes. By analyzing similar past failures, the agent can
    avoid repeating them and find solutions that have worked before.

    Args:
        params: The payload for the search, typically including an error message or test failure summary.
    """
    return await qora_client.find_similar_failures(**params)

@track_tool("reindex_code_graph")
async def qora_reindex_code_graph(*, root: str = ".") -> dict[str, Any]:
    """
    Triggers a full re-indexing of the codebase to update the Qora knowledge graph.

    This is an administrative tool that should be used after significant changes to the
    codebase, such as merging a large feature branch, to ensure Qora's data is fresh.

    Args:
        root: The root directory of the codebase to index.
    """
    return await qora_client.reindex_code_graph(root=root)

@track_tool("qora_shadow_run")
async def qora_shadow_run(*, diff: str, min_delta_cov: float = 0.0, timeout_sec: int = 1200, run_safety: bool = True, use_xdist: bool = True) -> dict[str, Any]:
    """
    Executes a comprehensive system simulation to predict the impact of a code change.

    This is a more detailed version of run_system_simulation, offering more control.
    It's a crucial step for verifying changes in complex systems.

    Args:
        diff: A git-formatted unified diff of the proposed change.
        min_delta_cov: The minimum required increase in test coverage.
        timeout_sec: The maximum execution time in seconds.
        run_safety: Whether to include safety-specific checks.
        use_xdist: Whether to parallelize the test run.
    """
    payload = {"diff": diff, "min_delta_cov": min_delta_cov, "timeout_sec": timeout_sec, "run_safety": run_safety, "use_xdist": use_xdist}
    return await _api_call("POST", "QORA_SHADOW_RUN", payload)

@track_tool("propose_and_auction")
async def propose_and_auction(*, brief: dict[str, Any], decision_id: str, budget_ms: int | None = 15000) -> dict[str, Any]:
    """
    A composite tool that engages the Nova market system to generate and select a solution.

    This high-level tool automates the process of creating a brief, soliciting proposals
    from other agents, evaluating them, and running an auction to select the best one.

    Args:
        brief: A dictionary describing the problem to be solved (an InnovationBrief).
        decision_id: A unique ID for tracking this decision process.
        budget_ms: The time budget in milliseconds for the proposal generation phase.
    """
    client = await get_http_client()
    
    async def propose_solutions(*, brief: dict[str, Any], decision_id: str, budget_ms: int | None) -> list[dict[str, Any]]:
        headers = {"x-decision-id": decision_id, "x-budget-ms": str(budget_ms)}
        validated_brief = InnovationBrief(**brief)
        response = await client.post(ENDPOINTS.NOVA_PROPOSE, json=validated_brief.model_dump(), headers=headers)
        response.raise_for_status()
        return response.json()

    async def evaluate_candidates(*, candidates: list[dict[str, Any]], decision_id: str) -> list[dict[str, Any]]:
        headers = {"x-decision-id": decision_id}
        validated_candidates = [InventionCandidate(**c).model_dump() for c in candidates]
        response = await client.post(ENDPOINTS.NOVA_EVALUATE, json=validated_candidates, headers=headers)
        response.raise_for_status()
        return response.json()

    async def auction_and_select_winner(*, evaluated_candidates: list[dict[str, Any]], decision_id: str) -> dict[str, Any]:
        headers = {"x-decision-id": decision_id}
        validated_candidates = [InventionCandidate(**c).model_dump() for c in evaluated_candidates]
        response = await client.post(ENDPOINTS.NOVA_AUCTION, json=validated_candidates, headers=headers)
        response.raise_for_status()
        return AuctionResult(**response.json()).model_dump()

    candidates_raw = await propose_solutions(brief=brief, decision_id=decision_id, budget_ms=budget_ms)
    if not candidates_raw:
        return AuctionResult(winners=[], market_receipt={"status": "no_candidates"}).model_dump()
    
    evaluated_candidates_raw = await evaluate_candidates(candidates=candidates_raw, decision_id=decision_id)
    
    auction_result = await auction_and_select_winner(evaluated_candidates=evaluated_candidates_raw, decision_id=decision_id)
    return auction_result

# ==============================================================================
# SECTION: Filesystem & Code Operations (Sandboxed)
# ==============================================================================

@track_tool("check_file_exists")
async def check_file_exists(*, path: str) -> Dict[str, Any]:
    """
    Checks existence and basic metadata of a path inside the DockerSandbox.
    Returns: { status, result: { path, exists, is_file, is_dir, size } }
    """
    cfg = seed_config()
    script = (
        "import json, sys; from pathlib import Path;"
        "p = Path(sys.argv[1]); "
        "exists = p.exists(); is_file = p.is_file(); is_dir = p.is_dir(); "
        "size = (p.stat().st_size if (exists and is_file) else None); "
        "print(json.dumps({'path': str(p), 'exists': exists, 'is_file': is_file, 'is_dir': is_dir, 'size': size}))"
    )
    cmd = ["python", "-c", script, path]
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(cmd, timeout=30)

    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": f"Existence check failed: {out.get('stderr')}"}

    try:
        import json
        info = json.loads(out.get("stdout") or "{}")
    except Exception as e:
        return {"status": "error", "reason": f"Malformed checker output: {e} | raw={out.get('stdout')!r}"}

    return {"status": "success", "result": info}


@track_tool("read_file")
async def read_file(*, path: str) -> Dict[str, Any]:
    """
    Reads and returns the full content of a specified file from inside the DockerSandbox.
    Avoids shell deps by using Python in the container.
    """
    cfg = seed_config()
    script = (
        "import sys, json; from pathlib import Path; "
        "p = Path(sys.argv[1]); "
        "import io; "
        "if not p.exists() or not p.is_file(): "
        "    print(json.dumps({'ok': False, 'error': 'File not found.'})); sys.exit(0); "
        "data = p.read_text(encoding='utf-8', errors='replace'); "
        "print(json.dumps({'ok': True, 'content': data}))"
    )
    cmd = ["python", "-c", script, path]
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(cmd, timeout=60)

    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": f"File read failed: {out.get('stderr')}"}

    try:
        import json
        payload = json.loads(out.get("stdout") or "{}")
    except Exception as e:
        return {"status": "error", "reason": f"Malformed read output: {e} | raw={out.get('stdout')!r}"}

    if not payload.get("ok"):
        return {"status": "error", "reason": payload.get("error", "unknown error")}

    return {
        "status": "success",
        "result": {"path": path, "content": payload.get("content", "")},
    }


@track_tool("write_file")
async def write_file(*, path: str, content: str, append: bool = False) -> Dict[str, Any]:
    """
    Creates, overwrites, or appends to a file in the sandbox using Python (no shell redirection).
    """
    cfg = seed_config()
    mode = "a" if append else "w"
    script = (
        "import sys; from pathlib import Path; "
        "path, content, mode = sys.argv[1], sys.argv[2], sys.argv[3]; "
        "p = Path(path); p.parent.mkdir(parents=True, exist_ok=True); "
        "with p.open(mode, encoding='utf-8', newline='\\n') as f: f.write(content)"
    )
    cmd = ["python", "-c", script, path, content, mode]
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(cmd, timeout=60)

    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": f"File write failed: {out.get('stderr')}"}

    return {"status": "success", "result": {"path": path}}


@track_tool("list_files")
async def list_files(*, path: str = ".", recursive: bool = False, max_depth: int = 3) -> Dict[str, Any]:
    """
    Lists files and directories to explore the project structure.

    Essential for discovering the codebase, finding relevant files to read or edit,
    or identifying where a new file should be created.

    Args:
        path: The directory to start listing from. Defaults to the project root.
        recursive: If True, lists files in all subdirectories.
        max_depth: The maximum depth for recursion.
    """
    cfg = seed_config()
    cmd = ["find", path, "-maxdepth", str(max_depth if recursive else 1)]
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(cmd, timeout=60)
    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": out.get("stderr") or "List files command failed."}
    items = [item for item in out.get("stdout", "").strip().splitlines() if item != path]
    return {"status": "success", "result": {"items": sorted(items[:2000])}}

@track_tool("file_search")
async def file_search(*, pattern: str, path: str = ".") -> Dict[str, Any]:
    """
    Searches for a regex pattern within files in the specified directory.

    Useful for finding where a specific function is used, locating error messages,
    or searching for configuration values within the codebase. Returns a list of matching file paths.

    Args:
        pattern: The regular expression to search for.
        path: The directory or file to search within. Defaults to the project root.
    """
    cfg = seed_config()
    cmd = ["grep", "-r", "-l", "-E", pattern, path]
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(cmd, timeout=120)
    if out.get("returncode", 1) > 1:
        return {"status": "error", "reason": out.get("stderr") or "Search command failed."}
    matches = out.get("stdout", "").strip().splitlines()
    return {"status": "success", "result": {"matches": matches}}

@track_tool("delete_file")
async def delete_file(*, path: str) -> Dict[str, Any]:
    """
    Deletes a file from the sandbox.

    A straightforward filesystem operation. Use with caution.

    Args:
        path: The relative path to the file to be deleted.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(["rm", "-f", path], timeout=30)
    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": f"File deletion failed: {out.get('stderr')}"}
    return {"status": "success", "result": {"path": path}}

@track_tool("rename_file")
async def rename_file(*, source_path: str, destination_path: str) -> Dict[str, Any]:
    """
    Renames or moves a file from a source path to a destination path.

    Useful for refactoring tasks, such as renaming a module or moving it to a different directory.

    Args:
        source_path: The current path of the file.
        destination_path: The new path for the file.
    """
    cfg = seed_config()
    setup_cmd = ["mkdir", "-p", str(Path(destination_path).parent)]
    mv_cmd = ["mv", source_path, destination_path]
    async with DockerSandbox(cfg).session() as sess:
        await sess._run_tool(setup_cmd, timeout=30)
        out = await sess._run_tool(mv_cmd, timeout=30)
    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": f"File rename/move failed: {out.get('stderr')}"}
    return {"status": "success", "result": {"from": source_path, "to": destination_path}}

@track_tool("create_directory")
async def create_directory(*, path: str) -> Dict[str, Any]:
    """
    Creates a new directory, including any necessary parent directories.

    A standard filesystem utility, often used to set up the structure for a new module or component.

    Args:
        path: The path of the directory to create.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        out = await sess._run_tool(["mkdir", "-p", path], timeout=30)
    if out.get("returncode", 1) != 0:
        return {"status": "error", "reason": f"Directory creation failed: {out.get('stderr')}"}
    return {"status": "success", "result": {"path": path}}

@track_tool("apply_refactor")
async def apply_refactor(*, diff: str, verify_paths: list[str] | None = None) -> dict[str, Any]:
    """
    Applies a git-formatted unified diff to the codebase in the sandbox.

    This is the standard way to apply code changes generated by an LLM or another tool.
    Can optionally run tests against the change immediately after applying.

    Args:
        diff: A string containing the unified diff to apply.
        verify_paths: An optional list of test files to run after applying the patch.
    """
    if not diff:
        return {"status": "error", "reason": "diff required"}
    
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        applied = await sess.apply_unified_diff(diff)
        if not applied:
            return {"status": "error", "reason": "git apply failed"}
        if verify_paths:
            ok, logs = await sess.run_pytest(verify_paths, timeout=900)
            return {"status": "success" if ok else "failed", "logs": logs}
        return {"status": "success"}

# ==============================================================================
# SECTION: Quality, Testing & Hygiene (Sandboxed)
# ==============================================================================


@track_tool("generate_test_scenarios")
async def generate_test_scenarios(*, goal: str, diff: str) -> dict[str, Any]:
    """
    Analyzes a goal and a proposed code change (diff) to brainstorm potential test scenarios.

    This tool helps ensure thorough testing by thinking about edge cases, positive and negative
    paths, and potential security vulnerabilities that might not be covered by existing tests.
    It returns a list of human-readable scenarios to guide test implementation.

    Args:
        goal: The original high-level goal for the code change.
        diff: The git-formatted unified diff of the proposed code change.
    """
    try:
        prompt = await build_prompt(
            scope="simula.test_scenario_generator",
            context={"goal": goal, "diff": diff},
            summary="Generate a comprehensive list of test scenarios for a given code change.",
        )
        response = await call_llm_service(prompt, agent_name="Simula.TestStrategist")
        scenarios = extract_json_flex(response.text)
        return {"status": "success", "result": {"scenarios": scenarios or []}}
    except Exception as e:
        return {"status": "error", "reason": f"Failed to generate test scenarios: {e!r}"}
        
@track_tool("code_linter_and_reviewer")
async def code_linter_and_reviewer(*, diff: str) -> dict[str, Any]:
    """
    Performs an AI-powered code review on a diff, checking for quality, maintainability, and bugs.

    Goes beyond a simple linter by using an LLM to analyze the code for logical errors, anti-patterns,
    unclear naming, insufficient comments, and potential performance or security issues.
    Provides structured feedback as if from a senior engineer.

    Args:
        diff: The git-formatted unified diff of the code to be reviewed.
    """
    try:
        prompt = await build_prompt(
            scope="simula.code_reviewer",
            context={"diff": diff},
            summary="Review a code diff for quality, correctness, and maintainability.",
        )
        response = await call_llm_service(prompt, agent_name="Simula.CodeReviewer")
        review = extract_json_flex(response.text)
        return {"status": "success", "result": {"review": review or {}}}
    except Exception as e:
        return {"status": "error", "reason": f"AI code review failed: {e!r}"}


@track_tool("run_tests_and_diagnose_failures")
async def run_tests_and_diagnose_failures(*, paths: Optional[List[str]] = None, k_expr: str = "") -> Dict[str, Any]:
    """
    Runs tests and, if they fail, parses the output to provide a structured diagnosis and suggests a root cause.

    This is more powerful than `run_tests` alone. It not only reports failure but also
    extracts the specific error messages and tracebacks, and then uses an LLM to hypothesize
    about the likely root cause of the failure, providing a strong starting point for debugging.

    Args:
        paths: The test files or directories to run.
        k_expr: An optional keyword expression to select a subset of tests.
    """
    test_result = await run_tests_k(paths=paths, k_expr=k_expr)
    if test_result.get("result", {}).get("passed"):
        return test_result
        
    stdout = test_result.get("result", {}).get("logs", {}).get("stdout", "")
    try:
        failures = parse_pytest_output(stdout)
        diagnostics = {"parsed_failures": [f.__dict__ for f in failures]}
        
        # ADDED: Use an LLM to suggest a root cause
        if failures:
            diag_prompt = await build_prompt(
                scope="simula.failure_diagnoser",
                context={"test_failures": diagnostics},
                summary="Analyze test failures and suggest a probable root cause.",
            )
            diag_response = await call_llm_service(diag_prompt, agent_name="Simula.Diagnoser")
            diagnostics["root_cause_hypothesis"] = diag_response.text
            
        test_result["result"]["diagnostics"] = diagnostics
        return test_result
    except Exception as e:
        return {"status": "error", "reason": f"Test diagnostics failed: {e!r}", "logs": test_result.get("result", {}).get("logs")}


@track_tool("debug_with_runtime_trace")
async def debug_with_runtime_trace(*, test_path: str, test_name: str) -> dict[str, Any]:
    """
    (Conceptual) Executes a specific failing test with a debugger to capture a detailed runtime trace.

    This advanced tool would instrument the code and test execution to capture the state
    of variables at each step leading up to the failure. It provides the ultimate level of
    detail for debugging complex logical errors. NOTE: This is a conceptual tool; its
    implementation requires a sophisticated debugger integration.

    Args:
        test_path: The path to the test file.
        test_name: The name of the specific test function to run and trace.
    """
    logger.warning("Conceptual tool 'debug_with_runtime_trace' was called. This feature is not fully implemented.")
    # In a real implementation, this would involve using a tool like pdb, bird-eye, or a custom tracer
    # to execute the test and capture the execution flow and variable states.
    return {
        "status": "success",
        "result": {
            "trace": [
                {"line": 42, "file": "my_module.py", "function": "my_func", "variables": {"x": 10, "y": None}},
                {"line": 43, "file": "my_module.py", "function": "my_func", "error": "AttributeError: 'NoneType' object has no attribute 'name'"},
            ],
            "notes": "Placeholder trace. Full implementation requires debugger integration."
        }
    }


@track_tool("generate_tests")
async def generate_tests(*, module: str) -> dict[str, Any]:
    """
    Generates a new test file with placeholder smoke tests for a given Python module.

    A useful tool for bootstrapping the testing process for new code. It discovers
    public functions in the target module and creates a basic test case for each.

    Args:
        module: The path to the Python module to generate tests for.
    """
    path = Path(module)
    if not path.exists():
        return {"status": "error", "reason": f"Module not found: {module}"}
    
    src = path.read_text(encoding="utf-8")
    fn_names = _discover_functions(src)
    test_path = Path("tests") / f"test_{path.stem}.py"
    
    body = io.StringIO()
    body.write("# This file was generated by Simula.\nimport pytest\n")
    rel = path.as_posix()
    import_line = f"from {rel[:-3].replace('/', '.')} import *" if rel.endswith(".py") else ""
    if import_line: body.write(import_line + "\n\n")

    if not fn_names:
        body.write("def test_module_imports():\n    assert True\n")
    else:
        for name in fn_names[:15]:
            body.write(textwrap.dedent(f"""
            def test_{name}_smoke():
                # TODO: Implement a real test for {name}
                assert '{name}' in globals()
            """))
    content = body.getvalue()
    await write_file(path=str(test_path), content=content)
    return {"status": "success", "result": {"path": str(test_path)}}

@track_tool("generate_property_test")
async def generate_property_test(*, file_path: str, function_signature: str) -> dict[str, Any]:
    """
    Uses an LLM to generate a property-based test for a given function using Hypothesis.

    This tool creates more robust tests than simple example-based tests by defining
    properties that should hold true for a wide range of inputs.

    Args:
        file_path: The path to the file containing the function.
        function_signature: The signature of the function to test (e.g., 'def my_func(x: int) -> int:').
    """
    try:
        prompt = await build_prompt(
            scope="simula.codegen.property_test",
            summary="Generate a Hypothesis property-based test",
            context={"vars": {"file_path": file_path, "function_signature": function_signature}}
        )
        llm_response = await call_llm_service(prompt, agent_name="Simula.GenPropTest")
        test_code = _strip_markdown_fences(llm_response.text)
        test_file_path = f"tests/property/test_prop_{Path(file_path).stem}.py"
        await write_file(path=test_file_path, content=test_code)
        return {"status": "success", "result": {"path": test_file_path}}
    except Exception as e:
        return {"status": "error", "reason": str(e)}

@track_tool("run_tests")
async def run_tests(*, paths: Optional[List[str]] = None, timeout_sec: int = 900) -> Dict[str, Any]:
    """
    Executes the test suite using pytest to verify code correctness.

    This is a critical step to ensure that code changes have not introduced any
    regressions and that new functionality works as expected. Always run this
    after writing or modifying code.

    Args:
        paths: Specific test files or directories to run. If None, runs all tests.
        timeout_sec: Maximum time in seconds to allow the test run to complete.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        await ensure_toolchain(sess)
        ok, logs = await sess.run_pytest(_normalize_paths(paths), timeout=timeout_sec)
        return {"status": "success" if ok else "failed", "result": {"passed": ok, "logs": logs}}

@track_tool("run_tests_k")
async def run_tests_k(*, paths: Optional[List[str]] = None, k_expr: str = "", timeout_sec: int = 600) -> Dict[str, Any]:
    """
    Runs a subset of tests matching a specific keyword expression using 'pytest -k'.

    Useful for quickly running only the tests relevant to a specific change, which is
    much faster than executing the entire suite.

    Args:
        paths: Specific test files or directories to search for tests.
        k_expr: The keyword expression to select tests (e.g., 'my_function and not slow').
        timeout_sec: Maximum time in seconds for the test run.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        await ensure_toolchain(sess)
        ok, logs = await sess.run_pytest_select(_normalize_paths(paths), k_expr=k_expr, timeout=timeout_sec)
        return {"status": "success" if ok else "failed", "result": {"passed": ok, "logs": logs}}

@track_tool("run_tests_xdist")
async def run_tests_xdist(*, paths: Optional[List[str]] = None, nprocs: str = "auto", timeout_sec: int = 900) -> dict:
    """
    Runs the pytest suite in parallel across multiple processes for speed.

    This is the preferred tool for running the full test suite, as it can be significantly
    faster than a single-threaded run.

    Args:
        paths: Specific test files or directories to run.
        nprocs: The number of processes to use (e.g., '4' or 'auto').
        timeout_sec: Maximum time in seconds for the entire test run.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        await ensure_toolchain(sess)
        ok, logs = await sess.run_pytest_xdist(_normalize_paths(paths), nprocs=nprocs, timeout=timeout_sec)
        return {"status": "success" if ok else "failed", "result": {"passed": ok, "logs": logs}}

@track_tool("static_check")
async def static_check(*, paths: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Runs static analysis (ruff for linting, mypy for type checking) on the code.

    A crucial tool for catching bugs, style errors, and type inconsistencies before
    running any code. Should be used frequently during development.

    Args:
        paths: A list of files or directories to check.
    """
    cfg = seed_config()
    async with DockerSandbox(cfg).session() as sess:
        await ensure_toolchain(sess)
        ruff_out = await sess.run_ruff(_normalize_paths(paths))
        mypy_out = await sess.run_mypy(_normalize_paths(paths))
        ruff_ok = ruff_out.get("returncode", 1) == 0
        mypy_ok = mypy_out.get("returncode", 1) == 0
        return {"status": "success" if ruff_ok and mypy_ok else "failed", "result": {"ruff_ok": ruff_ok, "mypy_ok": mypy_ok, "ruff": ruff_out, "mypy": mypy_out}}

@track_tool("run_repair_engine")
async def run_repair_engine(*, paths: Optional[List[str]] = None, timeout_sec: int = 600) -> Dict[str, Any]:
    """
    Invokes the autonomous repair engine to attempt to fix failing tests.

    When tests fail, this tool can be used to automatically generate and apply a patch
    that resolves the issue. It uses LLMs and iterative testing to find a solution.

    Args:
        paths: The test files or source files associated with the failure.
        timeout_sec: The maximum time to spend on the repair attempt.
    """
    out = await attempt_repair(_normalize_paths(paths), timeout_sec=timeout_sec)
    return {"status": out.status, "result": {"diff": out.diff, "tried": out.tried, "notes": out.notes}}

@track_tool("run_fuzz_smoke")
async def run_fuzz_smoke(*, module: str, function: str, timeout_sec: int = 600) -> Dict[str, Any]:
    """
    Runs a Hypothesis-based smoke test against a single function to find edge cases.

    This tool generates a wide range of inputs for a function to quickly find
    unexpected errors or behaviors that might be missed by example-based tests.

    Args:
        module: The module containing the function (e.g., 'path.to.module').
        function: The name of the function to test.
        timeout_sec: Maximum time to run the fuzzer.
    """
    ok, logs = await run_hypothesis_smoke(module, function, timeout_sec=timeout_sec)
    return {"status": "success" if ok else "failed", "result": {"passed": ok, "logs": logs}}

@track_tool("run_tests_and_diagnose_failures")
async def run_tests_and_diagnose_failures(*, paths: Optional[List[str]] = None, k_expr: str = "") -> Dict[str, Any]:
    """
    Runs tests and, if they fail, parses the output to provide a structured diagnosis.

    This is more powerful than `run_tests` alone. It not only reports failure but also
    extracts the specific error messages, tracebacks, and failed test names into a
    machine-readable format, which is very useful for debugging.

    Args:
        paths: The test files or directories to run.
        k_expr: An optional keyword expression to select a subset of tests.
    """
    test_result = await run_tests_k(paths=paths, k_expr=k_expr)
    if test_result.get("result", {}).get("passed"):
        return test_result
    stdout = test_result.get("result", {}).get("logs", {}).get("stdout", "")
    try:
        failures = parse_pytest_output(stdout)
        diagnostics = {"parsed_failures": [f.__dict__ for f in failures]}
        test_result["result"]["diagnostics"] = diagnostics
        return test_result
    except Exception as e:
        return {"status": "error", "reason": f"Test diagnostics failed: {e!r}", "logs": test_result.get("result", {}).get("logs")}

# ==============================================================================
# SECTION: VCS, CI/CD & Deployment
# ==============================================================================

@track_tool("open_pr")
async def open_pr(*, diff: str, title: str, evidence: Optional[dict] = None, base: str = "main") -> dict:
    """
    Opens a new Pull Request (PR) in the version control system.

    This is typically one of the final steps in a development task, packaging the
    final code change for human review and merging.

    Args:
        diff: The git-formatted unified diff for the PR.
        title: The title of the Pull Request.
        evidence: A dictionary containing evidence of the change's validity (e.g., test results).
        base: The name of the base branch to open the PR against (e.g., 'main' or 'develop').
    """
    res = await _open_pr_impl(diff, title=title, evidence=evidence or {}, base=base)
    return res.__dict__

@track_tool("format_patch")
async def format_patch(*, paths: List[str]) -> dict:
    """
    Automatically formats code using the project's autoformatter (e.g., Black, ruff format).

    This should be run before committing code to ensure it conforms to the project's style guide.

    Args:
        paths: A list of files or directories to format.
    """
    return await autoformat_changed(_normalize_paths(paths))

@track_tool("rebase_patch")
async def rebase_patch(*, diff: str, base: str = "origin/main") -> dict:
    """
    Rebases a proposed diff onto the tip of a target branch.

    This is used to resolve conflicts and ensure that a change applies cleanly to the
    most recent version of the codebase before it is merged.

    Args:
        diff: The diff to rebase.
        base: The target branch to rebase onto.
    """
    return await rebase_diff_onto_branch(diff, base=base)

@track_tool("run_ci_locally")
async def run_ci_locally(*, paths: Optional[List[str]] = None, timeout_sec: int = 2400) -> dict:
    """
    Runs the entire Continuous Integration (CI) pipeline in the local sandbox.

    This simulates what the remote CI server will do, including building, testing,
    and static analysis. It's a comprehensive way to validate a change before opening a PR.

    Args:
        paths: Optional list of specific paths to focus the CI run on.
        timeout_sec: The maximum time allowed for the entire CI run.
    """
    return await run_build_and_tests(paths=paths, timeout_sec=timeout_sec)

@track_tool("package_artifacts")
async def package_artifacts(*, proposal_id: str, evidence: dict, extra_paths: list[str] | None = None) -> dict[str, Any]:
    """
    Packages code changes and related evidence into a standardized artifact bundle.

    This tool is used to create a verifiable package containing the final diff, test results,
    and other metadata, which can be used for deployment or auditing.

    Args:
        proposal_id: A unique ID for the proposal being packaged.
        evidence: A dictionary of evidence to include in the bundle.
        extra_paths: An optional list of other files to include in the artifact bundle.
    """
    out = create_artifact_bundle(proposal_id=proposal_id, evidence=evidence, extra_paths=extra_paths or [])
    return {"status": "success", "bundle": {"path": out.path, "manifest": out.manifest_path, "sha256": out.sha256}}

@track_tool("policy_gate")
async def policy_gate(*, diff: str) -> dict[str, Any]:
    """
    Checks a proposed change against a set of predefined project policies.

    This is a governance tool that can enforce rules such as 'no changes to core authentication logic'
    or 'all database migrations must be approved'.

    Args:
        diff: The proposed code change as a unified diff.
    """
    return quick_policy_gate(diff)

@track_tool("impact_cov")
async def impact_cov(*, diff: str) -> dict[str, Any]:
    """
    Performs a quick analysis of a diff to estimate its impact and test coverage.

    A lightweight tool to get a fast signal on the risk and quality of a change without
    running a full simulation.

    Args:
        diff: The proposed code change as a unified diff.
    """
    return quick_impact_and_cov(diff)

@track_tool("render_ci_yaml")
async def render_ci_yaml(*, provider: str = "github", use_xdist: bool = True) -> dict[str, Any]:
    """
    Generates a CI configuration file (e.g., for GitHub Actions).

    A utility for setting up or modifying the project's CI/CD pipeline definition.

    Args:
        provider: The CI provider, e.g., 'github', 'gitlab'.
        use_xdist: If True, configures the test step to run in parallel.
    """
    return {"status": "success", "yaml": render_ci(provider, use_xdist=use_xdist)}

@track_tool("conventional_commit_title")
async def conventional_commit_title(*, evidence: dict) -> dict[str, Any]:
    """
    Generates a Conventional Commits-compliant title from evidence.

    Analyzes evidence like test results and the goal to create a standardized,
    semantic commit message title (e.g., 'feat(api): add new user endpoint').

    Args:
        evidence: A dictionary of evidence about the change.
    """
    title = title_from_evidence(evidence)
    return {"status": "success", "title": title}

@track_tool("conventional_commit_message")
async def conventional_commit_message(*, type_: str, scope: str | None, subject: str, body: str | None) -> dict[str, Any]:
    """
    Constructs a full Conventional Commits message from its component parts.

    This tool helps create well-formatted, detailed commit messages that follow a standard structure.

    Args:
        type_: The commit type (e.g., 'feat', 'fix', 'chore').
        scope: The part of the codebase affected (e.g., 'api', 'db').
        subject: The short, imperative-mood description of the change.
        body: A more detailed explanation of the change.
    """
    return {"status": "success", "message": render_conventional_commit(type_=type_, scope=scope, subject=subject, body=body)}

# ==============================================================================
# SECTION: Generic System & Memory Tools
# ==============================================================================

@track_tool("execute_system_tool")
async def execute_system_tool(params: dict[str, Any]) -> dict[str, Any]:
    """
    Executes a registered system tool from the central Qora architecture tool registry.

    This is a generic tool that acts as a gateway to other, non-Simula tools
    that are available in the wider EcodiaOS ecosystem.

    Args:
        params: A dictionary containing the tool identifier (UID or query) and its arguments.
    """
    uid = params.get("uid")
    query = params.get("query")
    args = params.get("args") or {}
    if not uid and not query:
        return {"status": "error", "reason": "Provide either uid or query"}
    
    async def _qora_exec_by_uid(uid: str, args: dict[str, Any]) -> dict[str, Any]:
        url = getattr(ENDPOINTS, "QORA_ARCH_EXECUTE_BY_UID", "/qora/arch/execute-by-uid")
        return await _post(url, {"uid": uid, "args": args})

    async def _qora_exec_by_query(query: str, args: dict[str, Any]) -> dict[str, Any]:
        url = getattr(ENDPOINTS, "QORA_ARCH_EXECUTE_BY_QUERY", "/qora/arch/execute-by-query")
        return await _post(url, {"query": query, "args": args})

    if uid:
        return await _qora_exec_by_uid(uid, args)
    return await _qora_exec_by_query(query, args)

@track_tool("memory_write")
async def memory_write(*, key: str, value: Any) -> dict[str, Any]:
    """
    Writes a value to the agent's short-term key-value memory store (blackboard).

    Useful for saving state, intermediate results, or important context within a single
    multi-step task.

    Args:
        key: The key to store the value under.
        value: The value to store (can be any JSON-serializable type).
    """
    if not key or value is None:
        return {"status": "error", "reason": "key and value are required"}
    await _bb_write(key, value)
    return {"status": "success"}

@track_tool("memory_read")
async def memory_read(*, key: str) -> dict[str, Any]:
    """
    Reads a value from the agent's short-term key-value memory store (blackboard).

    Used to retrieve information that was previously saved with memory_write.

    Args:
        key: The key of the value to retrieve.
    """
    if not key:
        return {"status": "error", "reason": "key is required"}
    out = await _bb_read(key)
    return {"status": "success", "value": out}

# ==============================================================================
# SECTION: Hierarchical Skills
# ==============================================================================

@track_tool("continue_hierarchical_skill")
async def continue_hierarchical_skill(*, episode_id: str, step: dict) -> dict[str, Any]:
    """
    Continues the execution of a multi-step, hierarchical skill managed by Synapse.

    This tool is used to proceed to the next step of a complex, predefined workflow.

    Args:
        episode_id: The ID of the ongoing skill execution episode.
        step: A dictionary representing the result of the previous step.
    """
    if not episode_id:
        return {"status": "error", "reason": "episode_id required"}
    try:
        return await _api_call("POST", "SYNAPSE_CONTINUE_SKILL", {"episode_id": episode_id, "step": step or {}})
    except Exception as e:
        return {"status": "error", "reason": f"continue_skill HTTP failed: {e!r}"}

@track_tool("request_skill_repair")
async def request_skill_repair(*, episode_id: str, failed_step_index: int, error_observation: dict) -> dict[str, Any]:
    """
    Requests a repair for a failed step within a hierarchical skill.

    When a step in a complex skill fails, this tool can be used to invoke a repair
    sub-agent to try and fix the problem before aborting the entire skill.

    Args:
        episode_id: The ID of the skill execution episode that failed.
        failed_step_index: The index of the step that failed.
        error_observation: A dictionary describing the error that occurred.
    """
    if not all([episode_id, failed_step_index is not None, error_observation is not None]):
        return {"status": "error", "reason": "episode_id, failed_step_index, and error_observation are required"}
    try:
        payload = {
            "episode_id": episode_id,
            "failed_step_index": int(failed_step_index),
            "error_observation": error_observation,
        }
        return await _api_call("POST", "SYNAPSE_REPAIR_SKILL_STEP", payload)
    except Exception as e:
        return {"status": "error", "reason": f"repair_skill_step HTTP failed: {e!r}"}
# ===== FILE: D:\EcodiaOS\systems\simula\nscs\context.py =====
# systems/simula/nscs/context.py
from __future__ import annotations

import json
import pathlib
import time
from typing import Any, Optional, Dict

class ContextStore:
    """A stateful, persisted working memory for a single agent run."""

    def __init__(self, run_dir: str):
        self.run_dir = run_dir
        self.path = pathlib.Path(run_dir) / "session_state.json"
        self.state: dict[str, Any] = {}
        self.load()

    def load(self) -> None:
        try:
            if self.path.exists():
                self.state = json.loads(self.path.read_text(encoding="utf-8"))
            else:
                self.state = self._default_state()
        except Exception:
            self.state = self._default_state()

    def save(self) -> None:
        try:
            self.path.parent.mkdir(parents=True, exist_ok=True)
            tmp = json.dumps(self.state, ensure_ascii=False, indent=2, default=str)
            self.path.write_text(tmp, encoding="utf-8")
        except Exception:
            # Never crash on a persistence failure
            pass

    def _default_state(self) -> dict[str, Any]:
        return {"dossier": {}, "failures": [], "facts": {}, "summaries": []}

    def set_status(self, status: str) -> None:
        self.state["status"] = status
        self.save()
        
    def update_dossier(self, dossier: dict[str, Any]) -> None:
        self.state["dossier"] = dossier
        self.save()

    def add_failure(self, tool_name: str, reason: str, params: dict | None = None) -> None:
        self.state.setdefault("failures", []).append(
            {"tool_name": tool_name, "reason": reason, "params": params or {}, "timestamp": time.time()}
        )
        self.save()

    def remember_fact(self, key: str, value: Any) -> None:
        self.state.setdefault("facts", {})[key] = value
        self.save()

    def get_fact(self, key: str, default=None) -> Any:
        return self.state.get("facts", {}).get(key, default)

    def push_summary(self, text: str, max_items: int = 8) -> None:
        summaries = self.state.setdefault("summaries", [])
        summaries.append(text[:2000])
        self.state["summaries"] = summaries[-max_items:]
        self.save()
# ===== FILE: D:\EcodiaOS\systems\simula\nscs\evolution.py =====
# systems/simula/nscs/evolution.py
from __future__ import annotations

import logging
import re
from typing import Any
from uuid import uuid4

# Core EcodiaOS Services & Schemas
from core.services.synapse import SynapseClient
from systems.qora import api_client as qora_client
from systems.synapse.schemas import Candidate, PatchProposal, TaskContext

# Simula Core Subsystems
from systems.simula.agent.autoheal import auto_heal_after_static
from systems.simula.code_sim.evaluators import run_evaluator_suite, EvalResult
from systems.simula.code_sim.portfolio import generate_candidate_portfolio
from systems.simula.code_sim.repair.ddmin import isolate_and_attempt_heal
from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config
from systems.simula.policy.emit import patch_to_policygraph
from .context import ContextStore

logger = logging.getLogger(__name__)

# --- Helper Functions (ported from old orchestrator) ---

def _is_self_upgrade(changed_paths: list[str]) -> bool:
    """Determines if a patch modifies Simula's own source code."""
    return any(p.startswith("systems/simula/") for p in changed_paths)

_DIFF_FILE_RE = re.compile(r"^[+-]{3}\s+(?P<label>.+)$")
_STRIP_PREFIX_RE = re.compile(r"^(a/|b/)+")

def _extract_paths_from_unified_diff(diff_text: str) -> list[str]:
    """Extract unique repo-relative paths from a unified diff string."""
    if not isinstance(diff_text, str) or not diff_text: return []
    paths: list[str] = []
    seen = set()
    for line in diff_text.splitlines():
        m = _DIFF_FILE_RE.match(line)
        if not m: continue
        label = m.group("label").strip()
        if label == "/dev/null": continue
        p = _STRIP_PREFIX_RE.sub("", label)
        if not p or p == ".": continue
        if p not in seen:
            seen.add(p)
            paths.append(p)
    return paths

# --- Main Evolution Engine ---

async def execute_code_evolution(*, goal: str, objective: dict) -> dict[str, Any]:
    """
    Executes a single, complete, and rigorously verified code evolution step.
    This is the self-contained engine for the "propose_intelligent_patch" tool.
    """
    logger.info("Starting code evolution cycle for goal: %s", goal)
    
    evo_id = f"evo_{uuid4().hex[:8]}"
    ctx = ContextStore(run_dir=f"artifacts/runs/{evo_id}")
    syn_client = SynapseClient()

    ctx.set_status("evolving_code")

    # === STAGE 1: ASSEMBLE DOSSIER ===
    ctx.set_status("assembling_dossier")
    try:
        main_target = objective.get("target_fqname", ".")
        dossier_result = await qora_client.get_dossier(target_fqname=main_target, intent="implement")
        ctx.update_dossier(dossier_result)
        logger.info("Successfully assembled dossier for target: %s", main_target)
    except Exception as e:
        return {"status": "error", "reason": f"Dossier construction failed: {e!r}"}

    # === STAGE 2: GENERATE CANDIDATE PORTFOLIO ===
    ctx.set_status("generating_candidates")
    try:
       # The 'objective' dict now serves as the context, replacing the old 'step_dict'
        candidates_payload = await generate_candidate_portfolio(job_meta={}, step_dict=objective)
        if not candidates_payload:
            return {"status": "error", "reason": "Code generation produced no candidates."}
        candidates = [Candidate(id=f"cand_{i}", content=p) for i, p in enumerate(candidates_payload)]
    except Exception as e:
        return {"status": "error", "reason": f"Candidate generation failed: {e!r}"}

    # === STAGE 3: SELECT CHAMPION VIA SYNAPSE ===
    ctx.set_status("selecting_champion")
    task_ctx = TaskContext(task_key="simula.code_evolution.select", goal=goal, risk_level="high")
    selection = await syn_client.select_arm(task_ctx, candidates=candidates)
    champion_content = next((c.content for c in candidates if c.id == selection.champion_arm.arm_id), candidates[0].content)
    diff_text = champion_content.get("diff", "")
    if not diff_text.strip():
        return {"status": "error", "reason": "Champion candidate had an empty diff."}

    # === STAGE 4: VERIFICATION GAUNTLET ===
    ctx.set_status(f"validating_champion:{selection.champion_arm.arm_id}")
    changed_paths = _extract_paths_from_unified_diff(diff_text)
    
    # Auto-healing
    autoheal_result = await auto_heal_after_static(changed_paths)
    if autoheal_result.get("status") == "proposed":
        diff_text += "\n" + autoheal_result.get("diff", "")
        ctx.push_summary("Auto-healed formatting and lint issues.")

    # Self-Upgrade Governance Escalation
    if _is_self_upgrade(changed_paths):
        proposal = PatchProposal(summary=f"Simula self-upgrade: {goal}", diff=diff_text)
        return await syn_client.submit_upgrade_proposal(proposal)

    # Sandbox Execution & Self-Correction Loop
    for attempt in range(2):  # Allow one repair attempt
        ctx.set_status(f"sandbox_execution:attempt_{attempt + 1}")
        try:
            async with DockerSandbox(seed_config()).session() as sess:
                if not await sess.apply_unified_diff(diff_text):
                    raise RuntimeError("Failed to apply diff in sandbox.")

                eval_result: EvalResult = run_evaluator_suite(objective, sess)

                if eval_result.hard_gates_ok:
                    final_proposal = {"proposal_id": f"prop_{evo_id}", "diff": diff_text, "evidence": eval_result.summary()}
                    await syn_client.log_outcome(episode_id=selection.episode_id, task_key=task_ctx.task_key, arm_id=selection.champion_arm.arm_id, metrics={"utility": 1.0, **eval_result.summary()})
                    return {"status": "success", "proposal": final_proposal}

                # Gates failed, attempt repair
                failure_summary = f"Hard gates failed on attempt {attempt + 1}: {eval_result.summary()}"
                ctx.add_failure("sandbox_validation", failure_summary)
                if attempt > 0: break

                ctx.set_status("self_correction:ddmin")
                repair_result = await isolate_and_attempt_heal(diff_text)
                if repair_result.status == "healed" and repair_result.healed_diff:
                    diff_text = repair_result.healed_diff
                    continue  # Retry the loop with the healed diff
                else:
                    break # ddmin couldn't fix it

        except Exception as e:
            ctx.add_failure("sandbox_execution", f"Sandbox crashed: {e!r}")
            break

    # If loop finishes without success
    await syn_client.log_outcome(episode_id=selection.episode_id, task_key=task_ctx.task_key, arm_id=selection.champion_arm.arm_id, metrics={"utility": 0.0, "reason": "verification_failed"})
    return {"status": "error", "reason": "Champion failed verification and could not be repaired."}
# ===== FILE: D:\EcodiaOS\systems\simula\nscs\language_adapters.py =====
# systems/simula/nscs/language_adapters.py  (extend dispatch to Rust)
from __future__ import annotations

from pathlib import Path

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config

from .language_adapters_go import go_static, go_tests, is_go_repo
from .language_adapters_java import is_java_repo, java_static, java_tests
from .language_adapters_rust import is_rust_repo, rust_static, rust_tests


def _is_node_repo() -> bool:
    return Path("package.json").exists()


def _is_python_repo() -> bool:
    return any(Path(".").rglob("*.py"))


async def _python_static(paths: list[str]) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        ruff = await sess._run_tool(["bash", "-lc", "ruff check . || true"])
        mypy = await sess._run_tool(["bash", "-lc", "mypy --hide-error-context --pretty . || true"])
        ok = ruff.get("returncode", 1) == 0 and mypy.get("returncode", 1) == 0
        return {"status": "success" if ok else "failed", "ruff": ruff, "mypy": mypy}


async def _python_tests(paths: list[str], *, timeout_sec: int) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            [
                "bash",
                "-lc",
                "pytest -q --maxfail=1 --disable-warnings " + " ".join(paths) + " || true",
            ],
            timeout=timeout_sec,
        )
        ok = out.get("returncode", 0) == 0 and "failed" not in (out.get("stdout") or "")
        return {"status": "success" if ok else "failed", "logs": out}


async def _node_static(paths: list[str]) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(["bash", "-lc", "npx -y eslint . || true"])
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "eslint": out}


async def _node_tests(paths: list[str], *, timeout_sec: int) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        cmd = "npx jest -w 4 --ci --silent || npm test --silent || true"
        out = await sess._run_tool(["bash", "-lc", cmd], timeout=timeout_sec)
        ok = out.get("returncode", 0) == 0 and "failed" not in (out.get("stdout") or "")
        return {"status": "success" if ok else "failed", "logs": out}


async def static_check(paths: list[str]) -> dict[str, object]:
    if is_rust_repo():
        return await rust_static(paths)
    if is_go_repo():
        return await go_static(paths)
    if is_java_repo():
        return await java_static(paths)
    if _is_python_repo():
        return await _python_static(paths)
    if _is_node_repo():
        return await _node_static(paths)
    return {"status": "success", "note": "no static adapter matched"}


async def run_tests(paths: list[str], *, timeout_sec: int = 900) -> dict[str, object]:
    if is_rust_repo():
        return await rust_tests(paths, timeout_sec=timeout_sec)
    if is_go_repo():
        return await go_tests(paths, timeout_sec=timeout_sec)
    if is_java_repo():
        return await java_tests(paths, timeout_sec=timeout_sec)
    if _is_python_repo():
        return await _python_tests(paths, timeout_sec=timeout_sec)
    if _is_node_repo():
        return await _node_tests(paths, timeout_sec=timeout_sec)
    return {"status": "success", "note": "no test adapter matched"}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\language_adapters_go.py =====
# systems/simula/nscs/language_adapters_go.py
from __future__ import annotations

from pathlib import Path

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config


def is_go_repo() -> bool:
    return Path("go.mod").exists() or any(Path(".").rglob("*.go"))


async def go_static(paths: list[str]) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        # golangci-lint if available, else go vet
        out = await sess._run_tool(
            [
                "bash",
                "-lc",
                "command -v golangci-lint >/dev/null 2>&1 && golangci-lint run || go vet ./... || true",
            ],
        )
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "lint": out}


async def go_tests(paths: list[str], *, timeout_sec: int = 1200) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            ["bash", "-lc", "go test ./... -count=1 || true"],
            timeout=timeout_sec,
        )
        ok = out.get("returncode", 0) == 0 and "FAIL" not in (out.get("stdout") or "")
        return {"status": "success" if ok else "failed", "logs": out}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\language_adapters_java.py =====
# systems/simula/nscs/language_adapters_java.py
from __future__ import annotations

from pathlib import Path

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config


def is_java_repo() -> bool:
    return (
        Path("pom.xml").exists() or Path("build.gradle").exists() or any(Path(".").rglob("*.java"))
    )


async def java_static(paths: list[str]) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        # try spotbugs/checkstyle if present, else javac compilation check
        cmd = (
            "mvn -q -DskipTests spotbugs:check checkstyle:check || mvn -q -DskipTests compile || true"
            if Path("pom.xml").exists()
            else "gradle -q check || gradle -q compileJava || true"
            if Path("build.gradle").exists()
            else "find . -name '*.java' -print0 | xargs -0 -n1 javac -Xlint || true"
        )
        out = await sess._run_tool(["bash", "-lc", cmd])
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "lint": out}


async def java_tests(paths: list[str], *, timeout_sec: int = 2400) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        cmd = (
            "mvn -q -DskipITs test || true"
            if Path("pom.xml").exists()
            else "gradle -q test || true"
        )
        out = await sess._run_tool(["bash", "-lc", cmd], timeout=timeout_sec)
        ok = out.get("returncode", 0) == 0 and "FAIL" not in (out.get("stdout") or "")
        return {"status": "success" if ok else "failed", "logs": out}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\language_adapters_rust.py =====
# systems/simula/nscs/language_adapters_rust.py
from __future__ import annotations

from pathlib import Path

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config


def is_rust_repo() -> bool:
    return Path("Cargo.toml").exists() or any(Path(".").rglob("*.rs"))


async def rust_static(paths: list[str]) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            ["bash", "-lc", "cargo clippy --all-targets -- -D warnings || true"],
        )
        ok = out.get("returncode", 0) == 0
        return {"status": "success" if ok else "failed", "clippy": out}


async def rust_tests(paths: list[str], *, timeout_sec: int = 1800) -> dict[str, object]:
    async with DockerSandbox(seed_config()).session() as sess:
        out = await sess._run_tool(
            ["bash", "-lc", "cargo test --quiet || true"],
            timeout=timeout_sec,
        )
        ok = out.get("returncode", 0) == 0 and "FAILED" not in (out.get("stdout") or "")
        return {"status": "success" if ok else "failed", "logs": out}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\ir\model.py =====
from __future__ import annotations

from pydantic import BaseModel, ConfigDict, Field


class TypeDecl(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    fqname: str
    kind: str  # class | dataclass | alias | enum
    fields: dict[str, str] = Field(default_factory=dict)


class FuncDecl(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    fqname: str  # file::Class?::func
    params: dict[str, str] = Field(default_factory=dict)
    returns: str = "None"
    contracts: dict[str, str] = Field(default_factory=dict)  # pre/post expr


class ModuleIR(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    path: str
    types: list[TypeDecl] = Field(default_factory=list)
    funcs: list[FuncDecl] = Field(default_factory=list)
    imports: list[str] = Field(default_factory=list)


class SIMIR(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    modules: dict[str, ModuleIR] = Field(default_factory=dict)

    def ensure_module(self, path: str) -> ModuleIR:
        if path not in self.modules:
            self.modules[path] = ModuleIR(path=path)
        return self.modules[path]

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\ir\patch.py =====
from __future__ import annotations

from typing import Any

from .model import SIMIR, FuncDecl


async def plan_patch_from_constraints(constraints: dict[str, Any]) -> dict[str, Any]:
    # Placeholder planner; upgrade with LLM + dossier-guided planning.
    targets = constraints.get("targets") or ["app/core.py"]
    patch = {"modules": {}}
    for t in targets:
        patch["modules"][t] = {
            "funcs": [
                {
                    "fqname": f"{t}::main",
                    "params": {},
                    "returns": "int",
                    "contracts": {"post": "result >= 0"},
                },
            ],
        }
    return patch


def apply_ir_patch(ir: SIMIR, patch: dict[str, Any]) -> SIMIR:
    for path, m in (patch.get("modules") or {}).items():
        mod = ir.ensure_module(path)
        for f in m.get("funcs", []):
            fd = FuncDecl(**f)
            mod.funcs = [x for x in mod.funcs if x.fqname != fd.fqname] + [fd]
    return ir

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\ir\backend\python.py =====
from __future__ import annotations

import os

from ..model import SIMIR, FuncDecl, ModuleIR

HEADER = "# Auto-generated by NSCS Python backend\nfrom __future__ import annotations\n"


def render_function(fd: FuncDecl) -> str:
    params = ", ".join(fd.params.keys())
    name = fd.fqname.split("::")[-1]
    sig = f"def {name}({params}) -> {fd.returns}:"
    body = [
        '"""Generated function. Fill logic via Simula if needed."""',
        "result = 0",
    ]
    post = fd.contracts.get("post")
    if post:
        body.append(f"assert {post}")
    body.append("return result")
    return sig + "\n    " + "\n    ".join(body) + "\n\n"


def render_module(module_ir: ModuleIR) -> str:
    out = [HEADER]
    for imp in module_ir.imports:
        out.append(f"import {imp}\n")
    for fd in module_ir.funcs:
        out.append(render_function(fd))
    return "".join(out)


def emit_files_from_ir(ir: SIMIR, root_dir: str) -> dict[str, str]:
    out: dict[str, str] = {}
    for path, mod in ir.modules.items():
        content = render_module(mod)
        out[path] = content
        os.makedirs(os.path.dirname(os.path.join(root_dir, path)), exist_ok=True)
        with open(os.path.join(root_dir, path), "w", encoding="utf-8") as f:
            f.write(content)
    return out

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\specs\compiler.py =====
from __future__ import annotations

from typing import Any

from .dsl import SystemSpec


def compile_spec_to_constraints(spec: SystemSpec, target: str | None = None) -> dict[str, Any]:
    focus = [m for m in spec.modules if target is None or m.path == target]
    return {
        "targets": [m.path for m in focus],
        "apis": [{"path": m.path, "apis": [a.model_dump() for a in m.apis]} for m in focus],
        "invariants": [
            {"path": m.path, "invariants": [i.model_dump() for i in m.invariants]} for m in focus
        ],
        "perf": [{"path": m.path, "perf": m.perf.model_dump() if m.perf else None} for m in focus],
        "global_invariants": [i.model_dump() for i in spec.global_invariants],
    }

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\specs\dsl.py =====
from __future__ import annotations

from typing import Literal

from pydantic import BaseModel, ConfigDict, Field


class APISignature(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    name: str
    params: dict[str, str] = Field(default_factory=dict)
    returns: str = "None"


class Invariant(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    id: str
    language: Literal["python", "z3", "tla"] = "python"
    body: str


class PerfBudget(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    p95_ms: int = 1000
    memory_mb: int = 512


class ModuleSpec(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    path: str
    apis: list[APISignature] = Field(default_factory=list)
    invariants: list[Invariant] = Field(default_factory=list)
    perf: PerfBudget | None = None
    tests: list[str] = Field(default_factory=list)


class SystemSpec(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    name: str
    modules: list[ModuleSpec] = Field(default_factory=list)
    global_invariants: list[Invariant] = Field(default_factory=list)

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\specs\forge.py =====
from __future__ import annotations

from .dsl import APISignature, ModuleSpec, SystemSpec


def natural_language_to_spec(nl: str, *, name: str = "system") -> SystemSpec:
    # Seed spec; wire your LLM+Qora expansion later.
    mod = ModuleSpec(path="app/core.py", apis=[APISignature(name="main", params={}, returns="int")])
    return SystemSpec(name=name, modules=[mod])

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\strategy\arms.py =====
from __future__ import annotations

from systems.synapse.core.registry import arm_registry


def _register():
    arm_registry.register(
        id="strategy/spec_ir_cgrag_v1",
        desc="Spec→SIM-IR→Python with contract-aware context; tests-first; SMT-lite",
        params={"planner": "tree", "retrieval": "contract-graph", "repair": "self-edit-diff"},
    )
    arm_registry.register(
        id="strategy/ir_refactor_semantic_v2",
        desc="Graph-preserving refactors; coverage-diff; twin replay",
        params={"refactor": "graph-preserving", "verify": "coverage-diff"},
    )


try:
    _register()
except Exception:
    pass

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\twin\probes.py =====
def inject_runtime_contracts(py_src: str, contracts: dict) -> str:
    # TODO: transform source with assert wrappers around public APIs
    return py_src

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\twin\runner.py =====
from __future__ import annotations

from typing import Any


def run_scenarios(scenarios: list[dict]) -> dict[str, Any]:
    # Real impl: orchestrate DockerSandbox workloads + probes.
    return {"integration_ok": True, "scenarios": len(scenarios), "details": {}}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\twin\scenarios.py =====
EXAMPLE_SCENARIOS = [
    {"name": "smoke", "type": "http", "requests": 10},
]

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\verify\bundle.py =====
from __future__ import annotations

from typing import Any

from pydantic import BaseModel, ConfigDict


class ProofBundle(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    contracts_ok: bool = True
    types_ok: bool = True
    lint_ok: bool = True
    property_ok: bool = True
    smt_ok: bool = True
    perf_ok: bool = True
    coverage: float = 0.0
    artifacts: dict[str, Any] = {}


def summarize(bundle: ProofBundle) -> dict[str, Any]:
    ok = all(
        [
            bundle.contracts_ok,
            bundle.types_ok,
            bundle.lint_ok,
            bundle.property_ok,
            bundle.smt_ok,
            bundle.perf_ok,
        ],
    )
    return {"ok": ok, "coverage": bundle.coverage, "artifacts": bundle.artifacts}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\verify\perf.py =====
from __future__ import annotations


def run_perf_benchmarks(target: str, p95_ms: int | None = None):
    # Microbench stub
    return {"ok": True, "p95_ms": 1}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\verify\props.py =====
from __future__ import annotations


def run_property_tests(paths):
    # Hook Hypothesis later; stub OK.
    return {"ok": True, "failures": []}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\verify\smt.py =====
from __future__ import annotations

from typing import Any


def smt_check(contracts: dict[str, str]) -> dict[str, Any]:
    # Hook CrossHair/Z3 later; stub OK.
    return {"ok": True, "details": {}}

# ===== FILE: D:\EcodiaOS\systems\simula\nscs\verify\types_styles.py =====
from __future__ import annotations

from typing import Any


def run_types_and_style(paths: list[str]) -> dict[str, Any]:
    # Real impl: run ruff + mypy in DockerSandbox; stub returns clean.
    return {"mypy": {"ok": True, "errors": 0}, "ruff": {"ok": True, "errors": 0}}

# ===== FILE: D:\EcodiaOS\systems\simula\ops\glue.py =====
# systems/simula/ops/glue.py
from __future__ import annotations

from systems.simula.code_sim.evaluators.coverage_delta import compute_delta_coverage
from systems.simula.code_sim.evaluators.impact import compute_impact
from systems.simula.config.loader import load_config
from systems.simula.policy.eos_checker import check_diff_against_policies, load_policy_packs


def quick_policy_gate(diff_text: str) -> dict[str, object]:
    cfg = load_config()
    packs = load_policy_packs(cfg.eos_policy_paths) if cfg.eos_policy_paths else load_policy_packs()
    rep = check_diff_against_policies(diff_text, packs)
    return {"ok": rep.ok, "findings": rep.summary()}


def quick_impact_and_cov(diff_text: str) -> dict[str, object]:
    impact = compute_impact(diff_text)
    cov = compute_delta_coverage(diff_text).summary()
    return {"impact": {"changed": impact.changed, "k_expr": impact.k_expr}, "coverage_delta": cov}

# ===== FILE: D:\EcodiaOS\systems\simula\policy\effects.py =====
# systems/simula/policy/effects.py
# NEW FILE FOR PHASE III
from __future__ import annotations

import ast

DANGEROUS_CALLS = {"os.system", "subprocess.run", "eval", "exec"}
NETWORK_MODULES = {"requests", "httpx", "socket", "urllib"}


class EffectAnalyzer(ast.NodeVisitor):
    """Analyzes a Python AST to infer potential side-effects."""

    def __init__(self):
        self.effects: set[str] = set()
        self.net_access: bool = False
        self.execution: bool = False

    def visit_Import(self, node: ast.Import):
        for alias in node.names:
            if alias.name in NETWORK_MODULES:
                self.net_access = True
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom):
        if node.module in NETWORK_MODULES:
            self.net_access = True
        self.generic_visit(node)

    def visit_Call(self, node: ast.Call):
        func_name = ast.unparse(node.func)
        if func_name in DANGEROUS_CALLS:
            self.execution = True
        self.generic_visit(node)


def extract_effects_from_diff(diff_text: str) -> dict[str, bool]:
    """
    Performs static analysis on the Python code added in a diff to infer side-effects.
    """
    added_code_lines = [
        line[1:]
        for line in diff_text.splitlines()
        if line.startswith("+") and not line.startswith("+++")
    ]

    if not added_code_lines:
        return {}

    try:
        tree = ast.parse("\n".join(added_code_lines))
        analyzer = EffectAnalyzer()
        analyzer.visit(tree)
        return {
            "net_access": analyzer.net_access,
            "execution": analyzer.execution,
        }
    except SyntaxError:
        # If the diff is not valid Python, we can't analyze it.
        return {"execution": True}  # Fail safe: assume execution if unparseable

# ===== FILE: D:\EcodiaOS\systems\simula\policy\emit.py =====
# systems/simula/policy/emit.py
# FINAL VERSION FOR PHASE III
from __future__ import annotations

import hashlib
from typing import Any

from systems.simula.policy.effects import extract_effects_from_diff
from systems.synapse.policy.policy_dsl import PolicyGraph, PolicyNode


def patch_to_policygraph(candidate: dict[str, Any]) -> PolicyGraph:
    """
    Translates a Simula candidate diff into a rich PolicyGraph by performing
    static analysis to infer the true effects of the code change.
    """
    diff_text = candidate.get("diff", "")
    inferred_effects = extract_effects_from_diff(diff_text)

    # Base effects for any git operation
    effects = {"write"}
    if inferred_effects.get("net_access"):
        effects.add("net_access")
    if inferred_effects.get("execution"):
        effects.add("execute")

    # The policy graph now reflects the analyzed effects of the specific patch
    graph_data = {
        "version": 1,
        "nodes": [
            PolicyNode(
                id="simula.apply_patch",
                type="tool",
                effects=list(effects),
                params={"diff_hash": hashlib.sha256(diff_text.encode()).hexdigest()},
            ),
            PolicyNode(
                id="simula.run_tests",
                type="tool",
                effects=["execute"],
                params={"suite": "ci"},
            ),
        ],
        "edges": [{"source": "simula.apply_patch", "target": "simula.run_tests"}],
        "constraints": [{"class": "danger", "smt": "(not (and write net_access))"}],
    }
    return PolicyGraph.model_validate(graph_data)

# ===== FILE: D:\EcodiaOS\systems\simula\policy\eos_checker.py =====
# systems/simula/policy/eos_checker.py  (extended loader)
from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path


@dataclass
class PolicyFinding:
    ok: bool
    rule_id: str
    message: str


@dataclass
class PolicyReport:
    ok: bool
    findings: list[PolicyFinding]

    def summary(self) -> dict[str, object]:
        return {"ok": self.ok, "findings": [f.__dict__ for f in self.findings]}


def load_policy_packs(paths: list[str] | None = None) -> list[dict[str, object]]:
    packs: list[dict[str, object]] = []
    roots = paths or ["systems/simula/policy/packs", ".simula/policies"]
    for r in roots:
        pr = Path(r)
        if not pr.exists():
            continue
        for p in pr.glob("*.json"):
            try:
                packs.extend(json.loads(p.read_text(encoding="utf-8")))
            except Exception:
                continue
    return packs


def check_diff_against_policies(diff_text: str, policies: list[dict[str, object]]) -> PolicyReport:
    findings: list[PolicyFinding] = []
    blocks = diff_text.splitlines()
    for pol in policies or []:
        rid = str(pol.get("id") or "rule")
        patt = re.compile(str(pol.get("pattern") or r"$^"), re.I | re.M)
        when = str(pol.get("when") or "added").lower()
        msg = str(pol.get("message") or f"Policy violation: {rid}")
        matched = False
        if when == "added":
            for ln in blocks:
                if ln.startswith("+") and not ln.startswith("+++"):
                    if patt.search(ln[1:]):
                        matched = True
                        break
        else:
            if patt.search(diff_text):
                matched = True
        if matched:
            findings.append(PolicyFinding(ok=False, rule_id=rid, message=msg))
    return PolicyReport(ok=(len(findings) == 0), findings=findings)

# ===== FILE: D:\EcodiaOS\systems\simula\policy\packs.py =====
# systems/simula/policy/packs.py
from __future__ import annotations

import fnmatch
import json
import os
import re
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

try:
    import yaml  # type: ignore
except Exception:  # pragma: no cover
    yaml = None  # YAML optional; JSON works too.

_DIFF_PATH_RE = re.compile(r"^\+\+\+\s+b/(.+)$", re.M)


@dataclass
class PolicyFinding:
    rule: str
    severity: str
    message: str
    data: dict[str, Any]


@dataclass
class PolicyReport:
    ok: bool
    findings: list[PolicyFinding]

    def summary(self) -> dict[str, Any]:
        return {
            "ok": self.ok,
            "findings": [asdict(f) for f in self.findings],
        }


@dataclass
class PolicyPack:
    name: str
    block_paths: list[str]
    require_tests_modified_on_code_change: bool
    max_changed_files: int | None = None
    max_hunk_size: int | None = None  # per @@ block (approx via +/- lines)


def _repo_root() -> Path:
    try:
        from systems.simula.config import settings  # type: ignore

        root = getattr(settings, "repo_root", None)
        if root:
            return Path(root).resolve()
    except Exception:
        pass
    for env in ("SIMULA_WORKSPACE_ROOT", "SIMULA_REPO_ROOT", "PROJECT_ROOT"):
        p = os.getenv(env)
        if p:
            return Path(p).resolve()
    return Path(".").resolve()


def _policy_dir() -> Path:
    return _repo_root() / ".simula" / "policy"


def _load_one(path: Path) -> PolicyPack:
    data: dict[str, Any]
    text = path.read_text(encoding="utf-8")
    if path.suffix.lower() in (".yaml", ".yml") and yaml:
        data = yaml.safe_load(text) or {}
    else:
        data = json.loads(text)
    return PolicyPack(
        name=str(data.get("name") or path.stem),
        block_paths=list(data.get("block_paths") or []),
        require_tests_modified_on_code_change=bool(
            data.get("require_tests_modified_on_code_change", True),
        ),
        max_changed_files=data.get("max_changed_files"),
        max_hunk_size=data.get("max_hunk_size"),
    )


def load_policy_packs() -> list[PolicyPack]:
    d = _policy_dir()
    if not d.exists():
        return []
    packs: list[PolicyPack] = []
    for p in sorted(d.glob("**/*")):
        if p.is_file() and p.suffix.lower() in (".yaml", ".yml", ".json"):
            try:
                packs.append(_load_one(p))
            except Exception:
                # Skip malformed files
                continue
    return packs


def _paths_from_diff(diff_text: str) -> list[str]:
    return sorted(set(_DIFF_PATH_RE.findall(diff_text or "")))


def _hunks_from_diff(diff_text: str) -> list[list[str]]:
    hunks: list[list[str]] = []
    current: list[str] = []
    for ln in (diff_text or "").splitlines():
        if ln.startswith("@@ "):
            if current:
                hunks.append(current)
                current = []
        current.append(ln)
    if current:
        hunks.append(current)
    return hunks


def check_diff_against_policies(diff_text: str, packs: list[PolicyPack]) -> PolicyReport:
    paths = _paths_from_diff(diff_text)
    hunks = _hunks_from_diff(diff_text)

    findings: list[PolicyFinding] = []
    code_changed = any(
        p.endswith((".py", ".ts", ".js", ".java", ".go", ".rs", ".cpp", ".c", ".cs")) for p in paths
    )
    tests_changed = any(("tests/" in p) or p.endswith(("_test.py", "Test.java")) for p in paths)

    for pack in packs:
        # 1) Blocked paths
        for pat in pack.block_paths:
            banned = [p for p in paths if fnmatch.fnmatch(p, pat)]
            if banned:
                findings.append(
                    PolicyFinding(
                        rule=f"{pack.name}.block_paths",
                        severity="high",
                        message=f"Blocked paths matched pattern '{pat}'",
                        data={"paths": banned},
                    ),
                )

        # 2) Require tests modified if code changed
        if pack.require_tests_modified_on_code_change and code_changed and not tests_changed:
            findings.append(
                PolicyFinding(
                    rule=f"{pack.name}.require_tests_modified_on_code_change",
                    severity="medium",
                    message="Code changed but no tests were modified.",
                    data={"paths": paths},
                ),
            )

        # 3) Max changed files
        if isinstance(pack.max_changed_files, int) and pack.max_changed_files >= 0:
            if len(paths) > pack.max_changed_files:
                findings.append(
                    PolicyFinding(
                        rule=f"{pack.name}.max_changed_files",
                        severity="medium",
                        message=f"Changed files ({len(paths)}) exceed limit ({pack.max_changed_files}).",
                        data={"paths": paths, "limit": pack.max_changed_files},
                    ),
                )

        # 4) Max hunk size (approx: count +/- lines in each hunk)
        if isinstance(pack.max_hunk_size, int) and pack.max_hunk_size > 0:
            for idx, h in enumerate(hunks):
                changes = sum(1 for ln in h if ln.startswith("+") or ln.startswith("-"))
                if changes > pack.max_hunk_size:
                    findings.append(
                        PolicyFinding(
                            rule=f"{pack.name}.max_hunk_size",
                            severity="low",
                            message=f"Hunk {idx} has {changes} changed lines (limit {pack.max_hunk_size}).",
                            data={"hunk_index": idx, "changed_lines": changes},
                        ),
                    )

    return PolicyReport(ok=(len(findings) == 0), findings=findings)

# ===== FILE: D:\EcodiaOS\systems\simula\recipes\generator.py =====
# systems/simula/recipes/generator.py
from __future__ import annotations

import json
import time
from dataclasses import asdict, dataclass
from pathlib import Path


@dataclass
class Recipe:
    id: str
    goal: str
    context_fqname: str
    impact_hint: str
    steps: list[str]
    success: bool
    created_at: float


_CATALOG = Path(".simula/recipes.json")


def load_catalog() -> list[Recipe]:
    if not _CATALOG.exists():
        return []
    try:
        raw = json.loads(_CATALOG.read_text(encoding="utf-8"))
        return [Recipe(**r) for r in raw]
    except Exception:
        return []


def save_catalog(items: list[Recipe]) -> None:
    _CATALOG.parent.mkdir(parents=True, exist_ok=True)
    _CATALOG.write_text(json.dumps([asdict(r) for r in items], indent=2), encoding="utf-8")


def append_recipe(
    goal: str,
    context_fqname: str,
    steps: list[str],
    success: bool,
    impact_hint: str = "",
) -> Recipe:
    rs = load_catalog()
    r = Recipe(
        id=f"rx-{int(time.time())}",
        goal=goal,
        context_fqname=context_fqname,
        impact_hint=impact_hint,
        steps=steps,
        success=success,
        created_at=time.time(),
    )
    rs.append(r)
    save_catalog(rs)
    return r

# ===== FILE: D:\EcodiaOS\systems\simula\review\atune_summary.py =====
# systems/simula/review/atune_summary.py
from __future__ import annotations

from typing import Any


def summarize_atune(detail: dict[str, Any]) -> dict[str, Any]:
    """
    Normalize Atune/Unity review detail into a compact summary the LLM can observe.
    Expects one item's detail from Orchestrator's atune route response.
    """
    status = str(detail.get("status", "unknown"))
    escalated = status.startswith("escalated_")
    pvals = detail.get("pvals") or {}
    plan = detail.get("plan") or {}
    unity = detail.get("unity_result") or {}
    return {
        "status": status,
        "escalated": escalated,
        "salience_p": float(pvals.get("salience") or pvals.get("salient") or 0.0),
        "safety_p": float(pvals.get("safety") or 0.0),
        "plan_steps": len(plan.get("steps") or []),
        "unity_summary": {
            "actors": list((unity.get("actors") or {}).keys()),
            "decision": unity.get("decision"),
            "notes": unity.get("notes"),
        },
    }

# ===== FILE: D:\EcodiaOS\systems\simula\review\pr_templates.py =====
# systems/simula/review/pr_templates.py
from __future__ import annotations

import json
from typing import Any


def render_pr_body(*, title: str, evidence: dict[str, Any]) -> str:
    cov = evidence.get("coverage_delta") or {}
    hyg = evidence.get("hygiene") or {}
    policy = evidence.get("policy") or {}
    ddmin = evidence.get("ddmin") or {}
    auto = evidence.get("auto_repair") or {}
    lines = [
        f"# {title}",
        "",
        "## Summary",
        "- Proposed by **Simula**.",
        "",
        "## Hygiene",
        f"- static: `{hyg.get('static')}`",
        f"- tests: `{hyg.get('tests')}`",
        "",
        "## Coverage (changed lines)",
        f"- {cov.get('pct_changed_covered', 0)}%",
        "",
    ]
    if policy:
        lines += ["## Policy", "```json", json.dumps(policy, indent=2), "```", ""]
    if ddmin:
        lines += ["## ddmin", "```json", json.dumps(ddmin, indent=2), "```", ""]
    if auto:
        lines += ["## auto_repair", "```json", json.dumps(auto, indent=2), "```", ""]
    return "\n".join(lines)

# ===== FILE: D:\EcodiaOS\systems\simula\risk\estimator.py =====
# systems/simula/risk/estimator.py
from __future__ import annotations

import re
from typing import Any

# Very light-weight heuristics. 0 (low) → 1 (high).
# Inputs: diff text + optional booleans/results from quick checks.

_DIFF_FILE_RE = re.compile(r"^\+\+\+ b/(.+)$", re.M)


def _changed_files(diff_text: str) -> list[str]:
    return sorted(set(_DIFF_FILE_RE.findall(diff_text or "")))


def _diff_magnitude(diff_text: str) -> tuple[int, int]:
    adds = sum(
        1 for ln in diff_text.splitlines() if ln.startswith("+") and not ln.startswith("+++")
    )
    dels = sum(
        1 for ln in diff_text.splitlines() if ln.startswith("-") and not ln.startswith("---")
    )
    return adds, dels


def estimate_risk(
    *,
    diff_text: str,
    policy_ok: bool | None = None,
    static_ok: bool | None = None,
    tests_ok: bool | None = None,
    delta_cov_pct: float | None = None,
    simulate_p_success: float | None = None,
) -> dict[str, Any]:
    files = _changed_files(diff_text)
    adds, dels = _diff_magnitude(diff_text)
    size = adds + dels

    # Feature scalers
    f_size = min(size / 2000.0, 1.0)  # >2000 lines ~ max risk contribution
    f_files = min(len(files) / 50.0, 1.0)  # >50 files ~ max
    f_cov = 0.0 if (delta_cov_pct is None) else max(0.0, (50.0 - float(delta_cov_pct)) / 50.0)
    f_policy = 0.5 if policy_ok is False else 0.0
    f_static = 0.3 if static_ok is False else 0.0
    f_tests = 0.6 if tests_ok is False else 0.0
    f_sim = 0.0
    if simulate_p_success is not None:
        # If the simulator predicted low success, raise risk
        f_sim = max(0.0, (0.7 - float(simulate_p_success)) / 0.7)  # p<0.7 ramps up

    # Weighted sum (tuned conservatively)
    risk = (
        0.30 * f_size
        + 0.20 * f_files
        + 0.20 * f_cov
        + 0.15 * f_tests
        + 0.10 * f_static
        + 0.10 * f_policy
        + 0.15 * f_sim
    )
    risk = max(0.0, min(1.0, risk))

    grade = (
        "low"
        if risk < 0.25
        else "moderate"
        if risk < 0.5
        else "elevated"
        if risk < 0.75
        else "high"
    )

    return {
        "risk": risk,
        "grade": grade,
        "features": {
            "size_lines": size,
            "files_changed": len(files),
            "delta_cov_pct": delta_cov_pct,
            "policy_ok": policy_ok,
            "static_ok": static_ok,
            "tests_ok": tests_ok,
            "simulate_p_success": simulate_p_success,
        },
        "files_sample": files[:20],
    }

# ===== FILE: D:\EcodiaOS\systems\simula\runtime\ingestor.py =====
from __future__ import annotations

import asyncio, re
import hashlib
import inspect
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

# Ensure project root in path for direct execution
import sys
sys.path.append(str(Path(__file__).resolve().parents[3]))

from core.llm.embeddings_gemini import get_embedding
from core.utils.neo.cypher_query import cypher_query
from systems.simula.code_sim.telemetry import _TOOL_REGISTRY # Import the registry populated by the decorator

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

VECTOR_INDEX_NAME = "simulaToolIndex"
# Use a dimension size compatible with your embedding model, e.g., 768 for gemini-embedding-001
EMBED_DIM = 768

def _spec_to_json_schema(func: Any) -> Dict[str, Any]:
    """
    Introspects a function's signature and docstring to build a JSON schema for its parameters.
    """
    sig = inspect.signature(func)
    schema = {"type": "object", "properties": {}, "required": []}
    doc_params = {}
    if func.__doc__:
        for line in func.__doc__.split('\n'):
            match = re.match(r'\s*:\s*param\s+(\w+):\s*(.*)', line)
            if match:
                doc_params[match.group(1)] = match.group(2).strip()

    for name, param in sig.parameters.items():
        if name in ('cls', 'self'):
            continue
        
        prop = {}
        # Basic type mapping
        if param.annotation in (str, Optional[str]):
            prop['type'] = 'string'
        elif param.annotation in (int, Optional[int]):
            prop['type'] = 'integer'
        elif param.annotation in (float, Optional[float]):
            prop['type'] = 'number'
        elif param.annotation in (bool, Optional[bool]):
            prop['type'] = 'boolean'
        elif param.annotation in (list, List, Optional[list], Optional[List]):
            prop['type'] = 'array'
        elif param.annotation in (dict, Dict, Optional[dict], Optional[Dict]):
            prop['type'] = 'object'
        else:
            prop['type'] = 'string' # Default for unknown types
        
        if name in doc_params:
            prop['description'] = doc_params[name]

        schema["properties"][name] = prop
        if param.default is inspect.Parameter.empty:
            schema["required"].append(name)
            
    return schema

def _content_hash_for(node_props: Dict[str, Any]) -> str:
    """Creates a stable hash for the tool's definition."""
    s = json.dumps(
        {k: node_props[k] for k in sorted(node_props.keys()) if k != 'embedding'},
        sort_keys=True, separators=(",", ":"), ensure_ascii=False
    )
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

async def _ensure_vector_index() -> None:
    """Creates the Neo4j vector index if it doesn't already exist."""
    await cypher_query(
        f"""
        CREATE VECTOR INDEX {VECTOR_INDEX_NAME} IF NOT EXISTS
        FOR (t:SimulaTool) ON (t.embedding)
        OPTIONS {{
          indexConfig: {{
            `vector.dimensions`: {EMBED_DIM},
            `vector.similarity_function`: 'cosine'
          }}
        }}
        """
    )
    logger.info(f"Vector index '{VECTOR_INDEX_NAME}' ensured at {EMBED_DIM} dims.")

async def _load_existing_hashes() -> Dict[str, str]:
    """Fetches content hashes of existing tools to avoid redundant updates."""
    rows = await cypher_query("MATCH (t:SimulaTool) RETURN t.name AS name, t.content_hash AS hash")
    return {r["name"]: r["hash"] for r in (rows or []) if r.get("name") and r.get("hash")}

async def synchronize_simula_tool_catalog() -> None:
    """
    Extracts decorated Simula tools, embeds them, and upserts into Neo4j
    for semantic retrieval during agent planning.
    """
    print("➡️  Simula: Synchronizing Tool Catalog with Neo4j...")

    try:
        await _ensure_vector_index()
    except Exception as e:
        logger.error(f"🔥 CRITICAL: Failed to ensure vector index: {e}")
        return

    # Dynamically import to ensure decorators have run and populated the registry
    from systems.simula.nscs import agent_tools
    if not _TOOL_REGISTRY:
        logger.error("🔥 CRITICAL: Tool registry is empty. Ensure agent_tools.py is imported and decorators are running.")
        return

    logger.info(f"Discovered {len(_TOOL_REGISTRY)} Simula tools via decorator registry.")

    try:
        existing_hashes = await _load_existing_hashes()
    except Exception as e:
        logger.error(f"🔥 CRITICAL: Could not fetch existing SimulaTool hashes: {e}")
        return

    updated_count = 0
    for tool_name, func in _TOOL_REGISTRY.items():
        try:
            description = inspect.getdoc(func) or f"Simula tool: {tool_name}"
            parameters_schema = _spec_to_json_schema(func)

            node_props = {
                "name": tool_name,
                "description": description.strip(),
                "parameters": json.dumps(parameters_schema),
                "returns": json.dumps({"type": "object", "description": "The result of the tool execution."}),
                "safety": 1.0,
            }

            content_hash = _content_hash_for(node_props)
            if existing_hashes.get(tool_name) == content_hash:
                continue

            # Compose rich text for embedding
            embed_text = f"Tool Name: {tool_name}\nPurpose: {description}"
            embedding = await get_embedding(embed_text)

            params_to_upsert = {**node_props, "content_hash": content_hash, "embedding": embedding}

            await cypher_query(
                """
                MERGE (t:SimulaTool {name: $name})
                SET t += {
                  description: $description,
                  parameters: $parameters,
                  returns: $returns,
                  safety: $safety,
                  content_hash: $content_hash,
                  embedding: $embedding,
                  last_updated: datetime()
                }
                """,
                params_to_upsert,
            )
            updated_count += 1
            logger.info(f"✅ Upserted SimulaTool '{tool_name}'.")
        except Exception as e:
            logger.error(f"🔥 FAILED to upsert SimulaTool '{tool_name}': {e}", exc_info=True)

    print(f"✅ Simula Tool Catalog synchronized. Updated: {updated_count}, Total: {len(_TOOL_REGISTRY)}")

if __name__ == "__main__":
    # Allows running the script directly for manual syncs
    # python -m systems.simula.scripts.sync_simula_tools
    asyncio.run(synchronize_simula_tool_catalog())
# ===== FILE: D:\EcodiaOS\systems\simula\scoring\score.py =====
# systems/simula/scoring/score.py
from __future__ import annotations


def composite_score(evidence: dict[str, object]) -> float:
    """
    Combine hygiene, coverage Δ, security/policy, and (optional) mutation score into [0,1].
    """
    hyg = evidence.get("hygiene", {})
    static_ok = 1.0 if hyg.get("static") == "success" else 0.0
    tests_ok = 1.0 if hyg.get("tests") == "success" else 0.0
    cov = float(evidence.get("coverage_delta", {}).get("pct_changed_covered", 0.0)) / 100.0
    policy = evidence.get("policy", {"ok": True})
    policy_ok = 1.0 if policy.get("ok", True) else 0.0
    mut = float(evidence.get("mutation", {}).get("score", 1.0))
    # weights tuned for conservatism
    return 0.28 * static_ok + 0.32 * tests_ok + 0.20 * cov + 0.12 * policy_ok + 0.08 * mut

# ===== FILE: D:\EcodiaOS\systems\simula\search\portfolio_runner.py =====
# systems/simula/search/portfolio_runner.py
from __future__ import annotations

import copy

from systems.simula.code_sim.evaluators.coverage_delta import compute_delta_coverage
from systems.simula.code_sim.evaluators.impact import compute_impact
from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config
from systems.simula.policy.eos_checker import check_diff_against_policies, load_policy_packs
from systems.simula.scoring.score import composite_score


async def evaluate_candidate(diff_text: str) -> dict[str, object]:
    """
    Minimal local evaluation: apply → pytest -k impact or full → static → cov → policy → score.
    """
    ev = {"hygiene": {}, "coverage_delta": {}, "policy": {}, "mutation": {}}
    impact = compute_impact(diff_text)
    async with DockerSandbox(seed_config()).session() as sess:
        ok = await sess.apply_unified_diff(diff_text)
        if not ok:
            return {"status": "rejected", "reason": "git apply failed"}
        # tests
        ok1, logs1 = await sess.run_pytest_select(["tests"], impact.k_expr or "", timeout=900)
        if not ok1:
            ok2, logs2 = await sess.run_pytest(["tests"], timeout=1500)
            ok1, _logs1 = ok2, logs2
        ev["hygiene"]["tests"] = "success" if ok1 else "failed"
        # static (python assumed here; multi-lang flows routed by higher-level adapters)
        ruff = await sess.run_ruff(["."])
        mypy = await sess.run_mypy(["."])
        ev["hygiene"]["static"] = (
            "success"
            if ruff.get("returncode", 1) == 0 and mypy.get("returncode", 1) == 0
            else "failed"
        )
        # coverage delta (best effort)
        try:
            _ = await sess.run_pytest_coverage(
                ["tests"],
                include=impact.changed or None,
                timeout=900,
            )
            ev["coverage_delta"] = compute_delta_coverage(diff_text).summary()
        except Exception:
            ev["coverage_delta"] = {"pct_changed_covered": 0.0}
        # policy packs
        pols = load_policy_packs()
        rep = check_diff_against_policies(diff_text, pols)
        ev["policy"] = rep.summary()
    # score
    s = composite_score(ev)
    return {"status": "scored", "evidence": ev, "score": s}


async def rank_portfolio(
    candidates: list[dict[str, object]],
    top_k: int = 3,
) -> list[dict[str, object]]:
    scored: list[tuple[float, dict[str, object]]] = []
    for c in candidates:
        res = await evaluate_candidate(c.get("diff", ""))
        if res.get("status") != "scored":
            continue
        c2 = copy.deepcopy(c)
        c2["evidence"] = res["evidence"]
        c2["score"] = res["score"]
        scored.append((c2["score"], c2))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:top_k]]

# ===== FILE: D:\EcodiaOS\systems\simula\service\deps.py =====
from __future__ import annotations

from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="SIMULA_", env_file=None)

    # Core
    repo_root: str = "/app"

    # Tool timeouts (seconds)
    fmt_timeout: int = 600
    test_timeout: int = 1800

    # Health/limits
    max_apply_bytes: int = 5_000_000


settings = Settings()

# ===== FILE: D:\EcodiaOS\systems\simula\service\services\codegen.py =====
from __future__ import annotations
import json
import logging
import time
import traceback
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import Any
from uuid import uuid4

from systems.simula.config import settings


class JobContext:
    """Manages state, artifacts, and logging for a single codegen job.

    Improvements:
      - Context manager (`with JobContext(...) as ctx:`) for auto setup/teardown
      - Stable metadata fields (start/end time, status, durations)
      - Small helpers for artifacts and structured logging
    """

    def __init__(self, spec: str, targets: list[dict[str, Any]] | None):
        self.spec = spec
        self.targets = targets or []
        self.start_ts = time.time()
        self.job_id = f"job_{int(self.start_ts)}_{str(uuid4())[:8]}"

        runs_dir = Path(settings.artifacts_root) / "runs"
        self.workdir = runs_dir / self.job_id
        self.workdir.mkdir(parents=True, exist_ok=True)

        self.log_handler: logging.Handler | None = None
        self.meta: dict[str, Any] = {
            "job_id": self.job_id,
            "status": "init",
            "start_time_utc": self._utc_iso(self.start_ts),
            "artifacts_root": str(self.workdir),
            # keep spec/targets lightweight in meta
            "spec_preview": (self.spec[:400] + "…") if len(self.spec) > 400 else self.spec,
            "targets_count": len(self.targets),
        }

    # ---------- lifecycle ----------
    def __enter__(self) -> "JobContext":
        self.setup_logging()
        self.set_status("running")
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        result = {"status": "ok", "message": "completed"} if exc is None else {
            "status": "error",
            "reason": str(exc),
        }
        self.finalize(result, exc)
        self.teardown_logging()

    def _utc_iso(self, ts: float) -> str:
        return datetime.fromtimestamp(ts, tz=UTC).isoformat()

    def setup_logging(self) -> None:
        """Attach a file logger for this job."""
        handler = logging.FileHandler(self.workdir / "agent.log", encoding="utf-8")
        formatter = logging.Formatter(
            fmt="%(asctime)s.%(msecs)03dZ %(levelname)-8s [%(name)s] %(message)s",
            datefmt="%Y-%m-%dT%H:%M:%S",
        )
        handler.setFormatter(formatter)
        handler.setLevel(logging.DEBUG)
        logging.getLogger().addHandler(handler)
        self.log_handler = handler

    def teardown_logging(self) -> None:
        """Detach the job-specific file logger."""
        if self.log_handler:
            logging.getLogger().removeHandler(self.log_handler)
            self.log_handler.close()
            self.log_handler = None

    # ---------- helpers ----------
    def set_status(self, status: str, **extra: Any) -> None:
        self.meta["status"] = status
        if extra:
            self.meta.update(extra)
        (self.workdir / "meta.json").write_text(json.dumps(self.meta, indent=2), encoding="utf-8")

    def log(self, level: int, msg: str, **kwargs: Any) -> None:
        logging.log(level, msg, extra={"job_id": self.job_id, **kwargs})

    def write_artifact(self, relpath: str, content: bytes | str) -> Path:
        path = self.workdir / relpath
        path.parent.mkdir(parents=True, exist_ok=True)
        if isinstance(content, bytes):
            path.write_bytes(content)
        else:
            path.write_text(content, encoding="utf-8")
        return path

    def finalize(self, result: dict[str, Any], error: Exception | None = None) -> None:
        """Finalize job metadata and save results."""
        self.meta.update(
            {
                "status": result.get("status", "error"),
                "message": result.get("message") or result.get("reason"),
                "duration_s": round(time.time() - self.start_ts, 4),
                "end_time_utc": self._utc_iso(time.time()),
            }
        )
        if error:
            self.meta["error"] = str(error)
            self.meta["traceback"] = traceback.format_exc()
        (self.workdir / "result.json").write_text(json.dumps(self.meta, indent=2), encoding="utf-8")


def run_codegen_job(spec: str, targets: list[dict[str, Any]] | None = None) -> dict[str, Any]:
    """
    Initializes the environment and runs the autonomous agent to fulfill the spec.
    Returns a small, API-friendly result dict with pointers to artifacts.
    """
    try:
        with JobContext(spec=spec, targets=targets) as ctx:
            ctx.log(logging.INFO, "Starting codegen")
            # --- your codegen pipeline goes here ---
            # Example: persist the incoming spec to artifacts for traceability
            ctx.write_artifact("inputs/spec.txt", spec)
            if targets:
                ctx.write_artifact("inputs/targets.json", json.dumps(targets, indent=2))

            # TODO: call into agent/orchestrator here and stream logs to agent.log
            # Simulate a minimal output artifact
            ctx.write_artifact("outputs/summary.txt", "Codegen completed successfully.")

            ctx.set_status("success")
            return {
                "status": "ok",
                "message": "codegen completed",
                "job_id": ctx.job_id,
                "artifacts": {
                    "workdir": str(ctx.workdir),
                    "log": str(ctx.workdir / "agent.log"),
                    "result": str(ctx.workdir / "result.json"),
                },
            }
    except Exception as e:
        # In case __exit__ wasn't reached, ensure we at least return an error payload
        logging.exception("Codegen job failed")
        return {"status": "error", "reason": str(e)}
# ===== FILE: D:\EcodiaOS\systems\simula\service\services\equor_bridge.py =====
# systems/simula/code_sim/equor_bridge.py
# DEPRECATED BRIDGE — kept as a soft-compat shim (no hard Equor imports)

from __future__ import annotations

import os
from typing import Any


# Try modern identity surface first; fall back to env only
def _current_identity_id() -> str:
    # Prefer explicit runtime identity if your new API is available
    try:
        from systems.equor.client import get_current_identity  # type: ignore

        ident = get_current_identity()
        if isinstance(ident, dict) and ident.get("id"):
            return str(ident["id"])
    except Exception:
        pass
    # Fallbacks
    return os.getenv("IDENTITY_ID", "ecodia.system")


async def fetch_identity_context(spec: str) -> dict[str, Any]:
    """Lightweight identity context for planning prompts (kept for legacy callsites)."""
    return {
        "identity_id": _current_identity_id(),
        "spec_preview": (spec or "")[:4000],
    }


# Legacy names preserved for callers; no-ops if old modules are gone
def resolve_equor_for_agent(*_args, **_kwargs):
    return {"status": "deprecated", "reason": "equor_bridge is a shim; use new Equor client APIs."}


def log_call_result(*_args, **_kwargs):
    return None


__all__ = ["fetch_identity_context", "resolve_equor_for_agent", "log_call_result"]

# ===== FILE: D:\EcodiaOS\systems\simula\service\services\executor.py =====
import asyncio
import os
from collections.abc import Sequence
from typing import Any


async def run_cmd(
    cmd: Sequence[str],
    cwd: str | None = None,
    timeout: int | None = None,
) -> dict[str, Any]:
    # Ensure user-site bin dirs (pip --user) are on PATH even under asyncio subprocesses
    env = dict(os.environ)
    extra_bins = ["/home/ecodia/.local/bin", "/root/.local/bin"]
    path = env.get("PATH", "")
    for p in extra_bins:
        if p and p not in path:
            path = path + (":" if path else "") + p
    env["PATH"] = path

    proc = await asyncio.create_subprocess_exec(
        *cmd,
        cwd=cwd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.STDOUT,
        env=env,
    )
    try:
        out, _ = await asyncio.wait_for(proc.communicate(), timeout=timeout)
    except TimeoutError:
        try:
            proc.kill()
        finally:
            return {"returncode": 124, "stdout": "TIMEOUT"}
    return {"returncode": proc.returncode, "stdout": (out or b"").decode("utf-8", "replace")}

# ===== FILE: D:\EcodiaOS\systems\simula\service\services\prompts.py =====
# systems/simula/code_sim/prompts.py
"""
Prompt builders for Simula Godmode

REFACTORED:
- These functions now ONLY build the user-facing part of the prompt.
- They no longer fetch or inject identity; the central LLM Bus handles that.
- They return a single user prompt string, not a full message list.
"""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any

# ---- Cross-system deps for context gathering (Unchanged) --------------------
from systems.evo.core.EvoEngine.dao import get_recent_codegen_feedback
from systems.unity.core.logger.dao import get_recent_unity_reviews

# ---- Constants --------------------------------------------------------------
REPO_ROOT = Path(os.environ.get("SIMULA_REPO_ROOT", "/workspace")).resolve()


# ---- Helpers (Unchanged) ----------------------------------------------------


def _read_file_snippet(path: Path, max_lines: int = 60) -> str:
    """
    Read head/tail of a file for compact context. Gracefully handles missing files.
    """
    try:
        if not path.is_file():
            return "[[ FILE NOT FOUND ]]"
        lines = path.read_text(errors="ignore").splitlines()
        if len(lines) <= max_lines:
            return "\n".join(lines)
        half = max_lines // 2
        head = "\n".join(lines[:half])
        tail = "\n".join(lines[-half:])
        return f"{head}\n...\n{tail}"
    except Exception:
        return "[[ FILE UNREADABLE ]]"


def _gather_repo_context(targets: list[dict[str, Any]], max_lines: int = 60) -> str:
    """
    Lightweight repo context aggregator using file head/tail snippets.
    """
    blocks: list[str] = []
    for t in targets or []:
        rel = t.get("path")
        if not rel:
            continue
        abs_path = (REPO_ROOT / rel).resolve()
        snippet = _read_file_snippet(abs_path, max_lines=max_lines)
        blocks.append(f"### File: {rel}\n```\n{snippet}\n```")
    return "\n\n".join(blocks)


# ---- DEPRECATED HELPERS -----------------------------------------------------

# The _ensure_identity and fetch_identity_context logic is now fully obsolete.
# The LLM Bus is solely responsible for composing the agent's identity.

# ---- Public API (Refactored) -------------------------------------------------


async def build_plan_prompt(
    spec: str,
    targets: list[dict[str, Any]],
) -> str:
    """
    Builds the user content for the planning prompt.

    REFACTORED: Returns a single string for the user prompt. Does not include
    system messages or identity context.
    """
    # Side signals (best-effort; don't explode if stores are empty)
    evo_feedback = await get_recent_codegen_feedback(limit=10)
    unity_reviews = await get_recent_unity_reviews(limit=5)

    repo_ctx = _gather_repo_context(targets, max_lines=60)

    # This function now assembles only the user-facing content.
    # The LLM Bus will prepend the full system prompt and identity from Equor.
    return (
        f"## SPEC\n{spec}\n\n"
        f"## RECENT EVO FEEDBACK (last 10)\n```json\n{json.dumps(evo_feedback, indent=2)}\n```\n\n"
        f"## RECENT UNITY REVIEWS (last 5)\n```json\n{json.dumps(unity_reviews, indent=2)}\n```\n\n"
        f"## TARGET FILE CONTEXT\n{repo_ctx}\n\n"
        "## INSTRUCTIONS\n"
        "Only output VALID JSON with exactly this schema:\n"
        '{ "plan": { "files": [ { "path": "<rel>", '
        '"mode": "<patch|full|scaffold|imports|typing|error_paths>", '
        '"signature": "<optional>", "notes": "<why>" } ] }, '
        '"notes": "<strategy>" }\n'
        "Prefer the smallest atomic plan that satisfies the spec. "
        "Avoid risky rewrites; use patches where possible."
    )


async def build_file_prompt(
    spec: str,
    file_plan: dict[str, Any],
) -> str:
    """
    Builds the user content for the single-file generation/patch prompt.

    REFACTORED: Returns a single string. Does not include system messages
    or identity context.
    """
    rel = file_plan.get("path", "")
    abs_path = (REPO_ROOT / rel).resolve() if rel else REPO_ROOT
    snippet = _read_file_snippet(abs_path, max_lines=240)

    include_current = str(file_plan.get("mode", "")).lower() in {
        "patch",
        "imports",
        "typing",
        "error_paths",
    }

    # Assemble all necessary context into a single string.
    parts: list[str] = [
        f"## SPEC\n{spec}",
        f"## FILE PLAN\n```json\n{json.dumps(file_plan, indent=2)}\n```",
    ]
    if include_current:
        parts.append(f"## CURRENT CONTENT OF {rel}\n```\n{snippet}\n```")

    parts.append(
        "## INSTRUCTIONS\n"
        "Output ONLY the FINAL, complete file content (not a diff). "
        "Follow PEP8 and established project style. "
        "If unsure about small details, choose the safest reasonable default.",
    )

    return "\n\n".join(parts)

# ===== FILE: D:\EcodiaOS\systems\simula\service\services\validator.py =====
from __future__ import annotations

import os
from pathlib import Path

# Default to container mount. If you centralize settings, import from deps.
REPO_ROOT = Path(os.environ.get("SIMULA_REPO_ROOT", "/workspace")).resolve()

# Anything outside repo or touching host/daemon sockets is blocked.
BLOCKLIST_ABS = {
    "/etc",
    "/proc",
    "/sys",
    "/dev",
    "/var/run/docker.sock",
}
BLOCKED_SUFFIXES = {".sock"}


def _is_subpath(child: Path, parent: Path) -> bool:
    try:
        child.relative_to(parent)
        return True
    except Exception:
        return False


def safe_patch_paths(paths: list[Path]) -> bool:
    """
    Returns True iff all paths resolve under REPO_ROOT and avoid blocklisted locations.
    """
    for p in paths:
        rp = p.resolve()
        # Must stay inside repo
        if not _is_subpath(rp, REPO_ROOT):
            return False
        # No sockets / weird devices
        if any(str(rp).startswith(b) for b in BLOCKLIST_ABS):
            return False
        if any(str(rp).endswith(suf) for suf in BLOCKED_SUFFIXES):
            return False
    return True

# ===== FILE: D:\EcodiaOS\systems\simula\service\services\vcs.py =====
from __future__ import annotations

import asyncio
import subprocess


def _git_sync(args: list[str], repo_path: str) -> dict:
    p = subprocess.run(
        ["git", *args],
        cwd=repo_path,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        timeout=600,
    )
    return {"rc": p.returncode, "out": p.stdout}


async def _git(args: list[str], repo_path: str) -> dict:
    return await asyncio.to_thread(_git_sync, args, repo_path)


async def ensure_branch(branch: str, repo_path: str):
    """
    If branch exists -> checkout. Else create from current HEAD.
    """
    # does it exist?
    exists = await _git(["rev-parse", "--verify", branch], repo_path)
    if exists["rc"] == 0:
        await _git(["checkout", branch], repo_path)
    else:
        await _git(["checkout", "-b", branch], repo_path)


async def commit_all(repo_path: str, message: str):
    """
    Stage everything and commit if there are changes.
    """
    await _git(["add", "-A"], repo_path)
    status = await _git(["status", "--porcelain"], repo_path)
    if status["rc"] == 0 and status["out"].strip():
        await _git(["commit", "-m", message], repo_path)

# ===== FILE: D:\EcodiaOS\systems\simula\spec_eval\scoreboard.py =====
# systems/simula/spec_eval/scoreboard.py
from __future__ import annotations

import json
import statistics as stats
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from systems.simula.config import settings

SPEC_EVAL_DIRNAME = "spec_eval"  # under artifacts_root


@dataclass
class RunSummary:
    run_id: str
    path: str
    num_candidates: int
    best_score: float
    avg_score: float
    median_score: float
    delta_cov_pct: float | None = None
    created_at: str | None = None
    meta: dict[str, Any] = None  # loose bag for anything else


def _iter_json(dirpath: Path):
    if not dirpath.exists():
        return
    for p in dirpath.rglob("*.json"):
        # ignore huge blobs; scoreboard is metadata-focused
        if p.stat().st_size > 8 * 1024 * 1024:
            continue
        yield p


def _safe_float(x: Any, default: float = 0.0) -> float:
    try:
        return float(x)
    except Exception:
        return default


def _extract_scores(payload: dict[str, Any]) -> list[float]:
    # Flexible: accept several shapes
    scores: list[float] = []
    # common shapes:
    # - {"candidates":[{"score": 0.91}, ...]}
    # - {"summary":{"scores":[...]}}
    # - {"results":[{"metrics":{"score":...}}, ...]}
    if isinstance(payload.get("candidates"), list):
        for c in payload["candidates"]:
            if isinstance(c, dict):
                if "score" in c:
                    scores.append(_safe_float(c["score"]))
                elif isinstance(c.get("metrics"), dict) and "score" in c["metrics"]:
                    scores.append(_safe_float(c["metrics"]["score"]))
    if not scores and isinstance(payload.get("summary"), dict):
        s = payload["summary"]
        if isinstance(s.get("scores"), list):
            scores = [_safe_float(v) for v in s["scores"]]
    if not scores and isinstance(payload.get("results"), list):
        for r in payload["results"]:
            m = r.get("metrics") if isinstance(r, dict) else None
            if isinstance(m, dict) and "score" in m:
                scores.append(_safe_float(m["score"]))
    return scores


def _extract_cov(payload: dict[str, Any]) -> float | None:
    # Try to find an evidence-like delta coverage fig
    # e.g. {"coverage_delta":{"pct_changed_covered": 72.5}}
    ev = payload.get("coverage_delta") or (payload.get("evidence") or {}).get("coverage_delta")
    if isinstance(ev, dict) and "pct_changed_covered" in ev:
        try:
            return float(ev["pct_changed_covered"])
        except Exception:
            return None
    return None


def _extract_created(payload: dict[str, Any]) -> str | None:
    for k in ("created_at", "timestamp", "ts"):
        if k in payload:
            v = payload.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    return None


def load_scoreboard() -> dict[str, Any]:
    root = Path(settings.artifacts_root or (settings.repo_root or ".")) / SPEC_EVAL_DIRNAME
    runs: list[RunSummary] = []
    for fp in _iter_json(root):
        try:
            data = json.loads(fp.read_text(encoding="utf-8"))
        except Exception:
            continue
        scores = _extract_scores(data)
        if not scores:
            continue
        run_id = data.get("run_id") or data.get("id") or fp.stem
        cov = _extract_cov(data)
        created = _extract_created(data)
        rs = RunSummary(
            run_id=str(run_id),
            path=str(fp.relative_to(Path(settings.repo_root or ".").resolve())),
            num_candidates=len(scores),
            best_score=max(scores),
            avg_score=sum(scores) / len(scores),
            median_score=stats.median(scores),
            delta_cov_pct=cov,
            created_at=created,
            meta={"title": data.get("title"), "notes": data.get("notes")},
        )
        runs.append(rs)

    runs.sort(key=lambda r: (r.best_score, r.avg_score), reverse=True)
    return {
        "count": len(runs),
        "runs": [asdict(r) for r in runs[:200]],
        "artifacts_root": str(Path(settings.artifacts_root or ".").resolve()),
        "dir": SPEC_EVAL_DIRNAME,
    }

# ===== FILE: D:\EcodiaOS\systems\simula\vcs\commit_msg.py =====
# systems/simula/vcs/commit_msg.py
from __future__ import annotations


def render_conventional_commit(
    *,
    type_: str,
    scope: str | None,
    subject: str,
    body: str | None = None,
) -> str:
    head = f"{type_}{f'({scope})' if scope else ''}: {subject}".strip()
    if body:
        return head + "\n\n" + body.strip() + "\n"
    return head + "\n"


def title_from_evidence(evidence: dict[str, object]) -> str:
    hyg = (evidence or {}).get("hygiene") or {}
    tests = hyg.get("tests", "unknown")
    static = hyg.get("static", "unknown")
    cov = (evidence or {}).get("coverage_delta", {}).get("pct_changed_covered", 0)
    return f"simula: patch (tests={tests}, static={static}, Δcov={cov}%)"

# ===== FILE: D:\EcodiaOS\systems\simula\vcs\pr_manager.py =====
# systems/simula/vcs/pr_manager.py
from __future__ import annotations

import json
import uuid
from dataclasses import dataclass

from systems.simula.code_sim.sandbox.sandbox import DockerSandbox
from systems.simula.code_sim.sandbox.seeds import seed_config
from systems.simula.review.pr_templates import render_pr_body


@dataclass
class PROpenResult:
    status: str
    branch: str
    title: str
    body: str
    web_url: str | None


async def open_pr(
    diff_text: str,
    *,
    title: str,
    evidence: dict[str, object] | None = None,
    base: str = "main",
) -> PROpenResult:
    branch = f"simula/{uuid.uuid4().hex[:8]}"
    async with DockerSandbox(seed_config()).session() as sess:
        await sess._run_tool(
            ["bash", "-lc", f"git checkout -B {branch} {base} || git checkout -B {branch} || true"],
        )
        ok = await sess.apply_unified_diff(diff_text)
        if not ok:
            return PROpenResult(status="failed", branch=branch, title=title, body="", web_url=None)
        await sess._run_tool(
            ["bash", "-lc", f"git add -A && git commit -m {json.dumps(title)} || true"],
        )
        # push best-effort (might be a dry-run sandbox)
        await sess._run_tool(["bash", "-lc", "git push -u origin HEAD || true"])
    body = render_pr_body(title=title, evidence=evidence or {})
    # return a dry-run result; actual URL may be created by CI bot
    return PROpenResult(status="created", branch=branch, title=title, body=body, web_url=None)
